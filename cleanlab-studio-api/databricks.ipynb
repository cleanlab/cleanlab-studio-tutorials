{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Use Cleanlab Studio With Your Data Stored in Databricks\n",
    "\n",
    "This tutorial (and accompanying\u00a0[video tutorial](https://www.youtube.com/watch?v=HnC6DwdV4EE)) demonstrates how\u00a0to run Cleanlab Studio on data stored in the Databricks platform.\u00a0As an example, we consider an application of text classification with LLMs, improving the models by improving the data they are fine-tuned on.\n",
    "We\u2019ll see how Cleanlab Studio systematically improves the training data to boost LLM performance by 37%, without requiring you spending any time or resources to change the model architecture, hyperparameters, or the training process.\n",
    "\n",
    "LLMs acquire powerful generative and discriminative capabilities after being pre-trained on a large corpus of text (usually scraped from the internet), but producing reliable outputs for a particular business use case often requires additional training (*fine-tuning*) on a labeled data set from the application domain.\n",
    "\n",
    "Labeled data powers AI/ML\u00a0in the enterprise, but real-world datasets have been found to\u00a0[contain between 7-50% annotation errors](https://go.cloudfactory.com/hubfs/02-Contents/3-Reports/Crowd-vs-Managed-Team-Hivemind-Study.pdf). Imperfectly-labeled text data hampers the training (and evaluation of) ML models across tasks like intent recognition, entity recognition, and sequence generation. Although pretrained LLMs are equipped with a lot of world knowledge, their performance is adversely affected by noisy training data (as\u00a0[noted by OpenAI](https://openai.com/research/dall-e-2-pre-training-mitigations)). This tutorial illustrates how using Cleanlab Studio to improve the training data can mitigate the negative effects of bad data (such as erroneous labels) without writing code or spending time to change the model architecture, hyperparameters, or training process.\n",
    "Because Cleanlab Studio improves the data itself (regardless of which model is used) it remains applicable for LLMs that are yet to be invented, like GPT-10.\n",
    "\n",
    "This tutorial applies LLMs to a politeness classification task, beginning by fine-tuning OpenAI's Davinci model on the original dataset stored in Databricks. The baseline model achieves moderate performance, but by automatically finding and fixing errors in the data using the\u00a0Databricks connector\u00a0for\u00a0Cleanlab Studio, we can achieve significantly better performance\u00a0*using the same LLM model and fine-tuning process*, just by improving the data (and spending minimal human time manually reviewing data). We see a 37% reduction in LLM prediction error after using Cleanlab Studio to improve the dataset:\n",
    "\n",
    "![Improvement using Cleanlab Studio](../assets/databricks/barchart.png)\n",
    "\n",
    "You can achieve analogous results whether you are using popular APIs for fine-tuning LLMs or training open-source LLMs like\u00a0[Dolly](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)\u00a0directly\u00a0[on Databricks](https://www.databricks.com/product/machine-learning/large-language-models).\n",
    "See the accompanying [blog post](https://cleanlab.ai/blog/fine-tune-LLM/) for additional context on LLMs and fine-tuning, why data quality matters for LLMs and ML tasks in general, and how\u00a0Cleanlab Studio\u00a0can help you easily improve ML model robustness and performance by systematically improving data quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and configure dependencies\n",
    "\n",
    "This tutorial uses the fine-tuning APIs\u00a0[offered by OpenAI](https://platform.openai.com/docs/guides/fine-tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure OpenAI API key\n",
    "\n",
    "Note that invoking the OpenAI API will use credits or bill you. The estimated cost to run this tutorial is 15 dollars with the Davinci model, which is the most powerful and most expensive. You can also scale down to the Curie or Ada model to reduce costs, by setting\u00a0`openai_model`\u00a0in the cell below, replacing \"davinci\" with \"curie\" or \"ada\". Fine-tuning the Ada model costs 1 dollar per run with the given dataset.\n",
    "\n",
    "Put your OpenAI API key in the cell below. You can find your API key at\u00a0https://platform.openai.com/api-keys. Here we have saved the key in a secret scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    " \n",
    "# we set the environment variable because it is used by the OpenAI command-line tool\n",
    "os.environ['OPENAI_API_KEY'] = dbutils.secrets.get(\"solution-accelerator-cicd\",\"openai_api\")\n",
    "# we also set the .api_key property below for the Python API\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "# set openai model name\n",
    "openai_model = 'davinci'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Cleanlab Studio\n",
    "1. If you don't have an account already,\u00a0[sign up for an account](https://app.cleanlab.ai/). It may take up to one day to get access.\n",
    "2. Get your\u00a0[API key](https://app.cleanlab.ai/account?tab=General)\u00a0and enter it below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANLAB_STUDIO_API_KEY = dbutils.secrets.get(\"solution-accelerator-cicd\",\"cleanlab_api\") \n",
    "studio = cleanlab_studio.Studio(CLEANLAB_STUDIO_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and prepare data\n",
    "\n",
    "Here we consider a 3-class variant of the Stanford Politeness Dataset, which has text phrases labeled as: impolite, neutral, or polite. Annotated by human raters, some of these labels are naturally low-quality.\n",
    "\n",
    "The training dataset has 1916 examples each labeled by a single human annotator, and thus some may be unreliable.\n",
    "\n",
    "The test dataset has 480 examples each labeled by 5 annotators, and we use their consensus label as a high-quality approximation of the true politeness (measuring test accuracy against these consensus labels). To ensure a fair comparison, this test dataset remains fixed throughout the experiments in this tutorial (all data quality improvement is done only for the training set).\n",
    "\n",
    "To prepare the data, we download raw data into\u00a0[DBFS](https://docs.databricks.com/dbfs/index.html), load it into PySpark DataFrames, and do some processing to prepare the dataset for the downstream task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh\n",
    " \n",
    "rm -rf /tmp/stanford-politeness\n",
    "mkdir -p /tmp/stanford-politeness\n",
    "cd /tmp/stanford-politeness\n",
    "curl --silent -L https://raw.githubusercontent.com/databricks-industry-solutions/cleanlab-improving-llms/main/data/train.csv -o train.csv\n",
    "curl --silent -L https://raw.githubusercontent.com/databricks-industry-solutions/cleanlab-improving-llms/main/data/test.csv -o test.csv\n",
    " \n",
    "# move the dataset to our main bucket\n",
    "rm -rf /dbfs/solacc/product/llm/stanford-politeness/raw\n",
    "mkdir -p /dbfs/solacc/product/llm/stanford-politeness/raw\n",
    "cp train.csv test.csv /dbfs/solacc/product/llm/stanford-politeness/raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the\u00a0`%fs`\u00a0command to see that our raw data is indeed saved in DBFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%fs ls /solacc/product/llm/stanford-politeness/raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the raw data into a PySpark DataFrame to enable further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/solacc/product/llm/stanford-politeness'\n",
    "raw_path = f'{data_path}/raw'\n",
    "politeness_train_raw = spark.read.options(header='true', inferSchema='true', escape='\"', multiLine=True).csv(f'{raw_path}/train.csv')\n",
    "politeness_test_raw = spark.read.options(header='true', inferSchema='true', escape='\"', multiLine=True).csv(f'{raw_path}/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is missing an index column, but downstream processing that we plan to do requires a unique ID per row. Here, we add monotonically-increasing integer IDs to the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def with_id_column(df):\n",
    "    df = df.select(F.monotonically_increasing_id().alias(\"id\"), \"*\")\n",
    "    return df\n",
    "\n",
    "politeness_train = with_id_column(politeness_train_raw)\n",
    "politeness_test = with_id_column(politeness_test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect this prepared data, looking at some specific rows to highlight data errors that are present. For example, the data point with ID\u00a0`1426`\u00a0is erroneously labeled \"impolite\" when \"polite\" is a more appropriate label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(politeness_train.where((politeness_train.id == 1426) | (politeness_train.id == 299) | (politeness_train.id == 134)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting data for fine-tuning\n",
    "\n",
    "We are using the OpenAI APIs for fine-tuning, which require data in a specific format (JSONL, newline-delimited JSON objects). We also need to do some pre-processing of the label column, adding whitespace before the completion, as the API recommends.\n",
    "\n",
    "We save the prepared results into DBFS, so that the result files can be used by the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, path):\n",
    "    '''\n",
    "    Write a dataframe into a single JSONL file located at path, in a format appropriate for fine tuning.\n",
    "\n",
    "    This makes a small tweak to the data, namely, adding whitespace before the completion.\n",
    "\n",
    "    We don't need the full power of Spark's parallel and distributed processing for this small demo dataset, but you would want to leverage it for any large real-world datasets. Our small dataset lives in a single partition, but larger datasets would have multiple partitions. By default, Spark writes the dataset into multiple files for efficiency (each partition corresponds to a separate file), but we need a single file to pass to the OpenAI command-line tool. This function ensures that a single JSONL file containing all of the data is produced as the final result.\n",
    "    '''\n",
    "    # add whitespace to the completion, as OpenAI requires\n",
    "    df = df.withColumn('completion', F.format_string(' %s', 'completion'))\n",
    "    # we don't need the index column here\n",
    "    df = df.drop('id')\n",
    "    temp_dir = f'{path}_tmp'\n",
    "    # write using a single partition, so we have a single JSONL file\n",
    "    df.coalesce(1).write.mode('overwrite').json(temp_dir)\n",
    "    # Spark saves the JSON file in a directory, along with some other files we don't need anymore\n",
    "    all_files = dbutils.fs.ls(temp_dir)\n",
    "    # move the .json file to the output destination\n",
    "    for f in all_files:\n",
    "        if f.path.endswith('.json'):\n",
    "            dbutils.fs.mv(f.path, path)\n",
    "    # remove all the other files, which we don't need\n",
    "    dbutils.fs.rm(temp_dir, recurse=True)\n",
    "\n",
    "prepare_data(politeness_train, f'{data_path}/processed/train.jsonl')\n",
    "prepare_data(politeness_test, f'{data_path}/processed/test.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tune and evaluate OpenAI model without Cleanlab Studio (accuracy ~65%)\n",
    "\n",
    "We use the\u00a0[OpenAI fine-tuning API](https://platform.openai.com/docs/guides/fine-tuning)\u00a0to first establish a baseline by:\n",
    "\n",
    "- Fine-tuning the OpenAI model on our (original, with some errors) training set\n",
    "- Evaluating the model on our test set\n",
    "\n",
    "First, we upload our training set and test set to OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = openai.File.create(file=open(f'/dbfs/{data_path}/processed/train.jsonl', 'rb'), purpose='fine-tune')\n",
    "test_file = openai.File.create(file=open(f'/dbfs/{data_path}/processed/test.jsonl', 'rb'), purpose='fine-tune')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we invoke the OpenAI API to fine tune the\u00a0`openai_model`\u00a0(\"davinci\" by default, unless you changed it above). Note that this incurs some\u00a0cost (roughly $7.50 for the Davinci model or $0.50 for the Ada model, at the time this was written).\n",
    "\n",
    "We also invoke the fine-tuning API with the\u00a0`validation_file`\u00a0keyword argument, so that the API will automatically compute statistics including model accuracy on the test set after the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.FineTune.create(\n",
    "    training_file=train_file.id,\n",
    "    validation_file=test_file.id,\n",
    "    compute_classification_metrics=True,\n",
    "    classification_n_classes=3,\n",
    "    model=openai_model,\n",
    "    suffix='baseline'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can follow the progress of fine-tuning with the following command. Once it's done, it'll print \"Job complete!\". You might need to re-run the cell if it times out. Training time varies based on queue length and other factors;\u00a0**it can take up to 1 hour to fine-tune the LLM**. The block below would check the status of the finetune and block execution until the finetune is complete. The block is based on this\u00a0[openai-cookbook example](https://github.com/openai/openai-cookbook/blob/594fc6c952425810e9ea5bd1a275c8ca5f32e8f9/examples/azure/finetuning.ipynb#L278)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def wait_for_finetune(job_id):\n",
    "  status = openai.FineTune.retrieve(id=job_id)[\"status\"]\n",
    "  if status not in [\"succeeded\", \"failed\"]:\n",
    "      print(f'Job not in terminal status: {status}. Waiting.')\n",
    "      while status not in [\"succeeded\", \"failed\"]:\n",
    "          time.sleep(60)\n",
    "          status = openai.FineTune.retrieve(id=job_id)[\"status\"]\n",
    "          print(f'Status: {status}')\n",
    "  else:\n",
    "      print(f'Finetune job {job_id} finished with status: {status}')\n",
    "      \n",
    "wait_for_finetune(response.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the job completes, we see the test accuracy achieved when fine-tuning this LLM on the original training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "!openai api fine_tunes.results -i {response.id} > baseline.csv\n",
    "\n",
    "base_df = pd.read_csv('baseline.csv')\n",
    "baseline_acc = base_df.iloc[-1]['classification/accuracy']\n",
    "print(f\"Fine-tuning Accuracy: {baseline_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline results: ~65% accuracy\n",
    "\n",
    "Our baseline Davinci LLM achieves a\u00a0**test accuracy of 65%**\u00a0when fine-tuned on the original training data (Curie achieved 64% accuracy, Ada achieved 60% accuracy). Model training is nondeterministic, so your results might vary slightly, even with the exact same dataset and initial model checkpoint. OpenAI's models might also be changed/updated over time.\n",
    "\n",
    "Even a state-of-the-art LLM like the Davinci\u00a0model produces lackluster results for this classification task; is it because of low data quality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve the data using Cleanlab Studio and re-train the LLM (accuracy ~78%)\n",
    "\n",
    "Next, we use the\u00a0[Databricks connector](https://github.com/cleanlab/cleanlab-studio)\u00a0for\u00a0[Cleanlab Studio](https://app.cleanlab.ai/)\u00a0to automatically improve the data quality, and then re-train our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade cleanlab-studio\n",
    "import cleanlab_studio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload dataset to Cleanlab Studio\n",
    "\n",
    "Next, we can directly upload a Spark DataFrame to Cleanlab Studio by passing it to\u00a0`studio.upload_dataset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = studio.upload_dataset(politeness_train, dataset_name='Stanford Politeness')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create a project**\n",
    "\n",
    "To analyze the data, use the\u00a0[Cleanlab Studio web UI](https://app.cleanlab.ai/)\u00a0to create a project, configuring it according to the ML task. For this demo, you should select:\n",
    "\n",
    "- ML task: text classification\n",
    "- Type of classification: multi-class\n",
    "- Text column: prompt (will be auto-detected)\n",
    "- Label column: completion (will be auto-detected)\n",
    "\n",
    "Select fast mode or regular mode depending on the speed/quality tradeoff you desire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Creating a project.](../assets/databricks/project.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make corrections to your dataset with Cleanlab Studio\n",
    "\n",
    "Cleanlab Studio not only finds data points with potential issues, but it also makes suggestions for how to address the issues (e.g., changing the label of a data point). Deciding how to make use of the analysis results is up to you. For example, you could discard all potentially erroneous data points, or you could review the data points most likely to have issues and make corrections. This human-in-the-loop data correction usually gives the best results.\n",
    "\n",
    "If you want to save time, you could briefly review some flagged issues, and then auto-fix the top issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Creating a project.](../assets/databricks/resolver.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selected row in the screenshot above is an example of a poorly-labeled datapoint. The phrase:\n",
    "\n",
    "> I\u2019ll take a look at getLogEntries when I have time. Would you mind adding me as a committer?\n",
    "> \n",
    "\n",
    "is labeled \"impolite\". Cleanlab Studio flags this as a label error, and it suggests that the label be switched to \"polite\". In the screenshot above, we pressed \"W\" to accept Cleanlab Studio's suggestion to automatically fix the label.\n",
    "\n",
    "Label issues like this cause the accuracy of the fine-tuned LLM to be degraded. Correcting these issues allows us to train an improved LLM, as we'll see below.\n",
    "\n",
    "### Export your improved dataset back into Databricks\n",
    "\n",
    "Once you're done correcting issues found in your dataset with Cleanlab Studio, export the improved dataset back into Databricks:\n",
    "\n",
    "1. Click on the \"Export Cleanset\" button in your Cleanlab Studio project\n",
    "2. Select the \"Export using API\" tab\n",
    "3. Copy the \"cleanset ID\" and paste it into the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanset_id = '7b27d51ba79b4087b32b3f064f87a47b'  # paste your own Cleanset ID here\n",
    "politeness_train_fixed = studio.apply_corrections(cleanset_id, politeness_train)\n",
    "display(politeness_train_fixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune the LLM on your improved dataset and evaluate the results\n",
    "\n",
    "Let's see how Cleanlab Studio improves the performance of the LLM. We follow the same process as earlier, except we use the\u00a0`politeness_train_fixed`\u00a0DataFrame as our training data.\n",
    "\n",
    "When we ran the experiment below, we used Cleanlab Studio's web interface to review the data issues that it flagged. Machine-augmented human-in-the-loop data improvement often gives the best results. If you want to use the dataset that we exported from Cleanlab Studio, uncomment the line below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default for reproducibility, we use the dataset that we exported from Cleanlab Studio as csv\n",
    "# But if you want to use your dataset that you improved, downloaded as politeness_train_fixed above\n",
    "# set the flag below to 'False'\n",
    "use_provided_training_set_improved_using_cleanlab_studio = True\n",
    "if use_provided_training_set_improved_using_cleanlab_studio:\n",
    "    politeness_train_fixed = pd.read_csv('https://raw.githubusercontent.com/databricks-industry-solutions/cleanlab-improving-llms/main/data/train_fixed.csv')\n",
    "    politeness_train_fixed = with_id_column(spark.createDataFrame(politeness_train_fixed))\n",
    "\n",
    "prepare_data(politeness_train_fixed, f'{data_path}/processed/train_fixed.jsonl')\n",
    "\n",
    "train_file_fixed = openai.File.create(file=open(f'/dbfs/{data_path}/processed/train_fixed.jsonl', 'rb'), purpose='fine-tune')\n",
    "\n",
    "response_fixed = openai.FineTune.create(\n",
    "    training_file=train_file_fixed.id,\n",
    "    validation_file=test_file.id,\n",
    "    compute_classification_metrics=True,\n",
    "    classification_n_classes=3,\n",
    "    model=openai_model,\n",
    "    suffix='fixed'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can follow the progress of fine-tuning with the following command. Once it's done, it'll print \"Job complete!\". You might need to re-run the cell if it times out. Training time varies based on queue length and other factors; it can take up to 1 hour to fine-tune the LLM. We use the\u00a0`wait_for_finetune`\u00a0function defined before to block this step until the finetuning is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_for_finetune(response_fixed.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the job completes, we see the test accuracy achieved when fine-tuning this LLM on the improved dataset. If you simply auto-fixed some of the labels (spending zero human time on data improvement), you'll still see improvement; if you reviewed some of Cleanlab Studio's suggestions following a human-in-the-loop data cleaning process, you'll see larger improvements here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!openai api fine_tunes.results -i {response_fixed.id} > fixed.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact of data improvement using Cleanlab Studio: ~78% accuracy (compared to ~65% baseline accuracy)\n",
    "\n",
    "Training on the improved dataset, we see a\u00a0**test accuracy of 78%**\u00a0for the Davinci model (Curie achieved ~76% accuracy, Ada achieved ~75% accuracy). These results are from our\u00a0`train_fixed.csv`\u00a0(provided above); results on your dataset will vary depending on how you improved the dataset using Cleanlab Studio (e.g., whether you used auto-fix or manually reviewed the top issues, how you corrected labels, how you removed outliers, etc.). Even the results of fine-tuning on the provided dataset might vary slightly, because model training is nondeterministic, and OpenAI's initial model checkpoints may be updated over time.\n",
    "\n",
    "In this evaluation, we see that data quality has a huge impact on LLM performance.\u00a0**By simply improving the data quality**\u00a0(and leaving the original LLM checkpoint, training parameters, fine-tuning process, etc. as-is), we have\u00a0**reduced prediction error by ~37%**.\n",
    "Because Cleanlab Studio improves models by improving the underlying data, the strategy outlined here works for any model or LLM that exists today or may exist in the future!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}