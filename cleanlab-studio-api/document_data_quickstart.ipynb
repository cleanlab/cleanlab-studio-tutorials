{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7c0df36-a1e0-4e6b-bef4-6442fa1bc8af",
   "metadata": {},
   "source": [
    "# Curating Heterogenous Document Datasets\n",
    "\n",
    "<head>\n",
    "  <meta name=\"title\" content=\"Curating Heterogeneous Document Datasets with Data-Centric AI\"/>\n",
    "  <meta property=\"og:title\" content=\"Curating Heterogeneous Document Datasets with Data-Centric AI\"/>\n",
    "  <meta name=\"twitter:title\" content=\"Curating Heterogeneous Document Datasets with Data-Centric AI\" />\n",
    "  <meta name=\"description\" content=\"A quick tutorial on analyzing document collections via Cleanlab Studio's Python API.\"  />\n",
    "  <meta property=\"og:description\" content=\"A quick tutorial on analyzing document collections via Cleanlab Studio's Python API.\" />\n",
    "  <meta name=\"twitter:description\" content=\"A quick tutorial on analyzing document collections via Cleanlab Studio's Python API.\" />\n",
    "</head>\n",
    "\n",
    "This is the recommended quickstart tutorial for analyzing *document* datasets/collections via Cleanlab Studio's [Python API](/guide/quickstart/api/).\n",
    "\n",
    "This tutorial demonstrates how to run Cleanlab Studio on diverse document file types like: `csv`, `doc`, `docx`, `pdf`, `ppt`, `pptx`, `xls`, `xlsx`.\n",
    "We first recommend completing our [Text Data Quickstart Tutorial](/tutorials/text_data_quickstart/) to understand how Cleanlab Studio works with text datasets. Here we only demonstrate how to process your document collection into a text dataset within Cleanlab Studio, refer to the [Text Data Quickstart Tutorial](/tutorials/text_data_quickstart/) for next steps after that. You can alternatively extract text from the documents yourself and run Cleanlab Studio on the resulting text dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0dd704-a8c5-4a75-9a50-27d0317ea51a",
   "metadata": {},
   "source": [
    "## Install and import required dependencies\n",
    "\n",
    "Cleanlab's Python client can be installed using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c96bc8-9a7c-4a74-ab6d-afd33358a3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade cleanlab-studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eedb82e-8654-4485-a723-7ef517d66324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab_studio import Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906e2814-750f-4de8-bfd7-6d9c10ca3bfa",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "This tutorial considers an example dataset of around 500 business documents of several file types (`ppt`, `pdf`, `docx`). We want to categorize each document into one of these 5 *topics*: **HR**, **IT**, **Finance**, **Sales**, **Product** (this is a *multi-class classification* task).\n",
    "\n",
    "Let's download the dataset in an [appropriate format](guide/concepts/datasets/#metadata-zip) with the following terminal command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f0bf623-6f5d-406c-8e8f-01447b2587ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc 'https://cleanlab-public.s3.amazonaws.com/Datasets/documents_cl_labeled.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8e26b4-4e4b-4b63-be93-dc84c80a0a94",
   "metadata": {},
   "source": [
    "### Format Your Own Dataset\n",
    "\n",
    "The same ideas from this tutorial apply to document datasets with other file types and class labels or document tags.\n",
    "You can follow along with your own dataset as long as it is formatted similarly or in other acceptable formats listed in the [Document Datasets guide](/guide/concepts/datasets/#document-datasets). Below are some more details on how to format a document dataset like the one used in this tutorial. \n",
    "\n",
    "#### Dataset Structure\n",
    "\n",
    "You can directly run Cleanlab Studio on a document dataset, where each document lives in its own file, and the files are organized in a particular directory shown below. In addition to the documents, your top-level directory should contain a file `metadata.csv`, containing metadata about each document, including any annotated class labels or tags.\n",
    "\n",
    "Here's how the directory structure and `metadata.csv` file should look:\n",
    "\n",
    "```\n",
    "documents_cl\n",
    "\u251c\u2500\u2500 metadata.csv\n",
    "\u251c\u2500\u2500 file_0.pdf\n",
    "\u2514\u2500\u2500 documents\n",
    "    \u251c\u2500\u2500 optional_subdirectory\n",
    "    \u2502   \u251c\u2500\u2500 file_1.ppt\n",
    "    \u2502   \u2514\u2500\u2500 file_2.pdf\n",
    "    \u251c\u2500\u2500file_3.docx  \u22ee\n",
    "    \u2514\u2500\u2500file_4.pdf\n",
    "        \u22ee\n",
    "\n",
    "\n",
    "metadata.csv\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502filename                                     \u2502label   \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502file_0.ppf                                   \u2502hr      \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502documents/optional_subdirectory/file_1.ppt   \u2502        \u2502  <\u2500\u2500 unlabeled example\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502documents/optional_subdirectory/file_2.pdf   \u2502sales   \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502documents/file_3.docx                        \u2502it      \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502documents/file_4.pdf                         \u2502        \u2502  <\u2500\u2500 unlabeled example\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502...                                          \u2502...     \u2502\n",
    "\u2514\u2500 \u2500\u2500 \u2500\u2500 \u2500\u2500 \u2500\u2500 \u2500\u2500 \u2500\u2500 \u2500\u2500 \u2500\u2500 \u2500\u2500 \u2500\u2500 \u2500\u2500 \u2500\u2500 \u2500\u2500 \u2500\u2500 \u2500\u2500 \u2500\u2500 \u2500\u2500 \u2500\u2518\n",
    "\n",
    "```\n",
    "\n",
    "- **Parent Directory**: In our tutorial, `documents_cl/` serves as the top-level directory. It holds all document files and the `metadata.csv` file.\n",
    "\n",
    "- **metadata.csv**: This manifest must be named `metadata.csv` and placed at the top-level directory. It contains mappings between relative filepaths to the documents and metadata about each document in the dataset (such as labels). While the document files in the directory can be of mixed media formats with an arbitrary layout, the metadata must be formatted as a standard CSV file (e.g. use `, ` as the delimiter and `\"` as the quote character).\n",
    "\n",
    "- **Data Directories (Optional)**: Optional divisions between document files, such as the `optional_subdirectory/` and `documents/` sub-directories shown above, may be included for organizational purposes. As long as the metadata file correctly points to the location of the documents (relative paths within these subdirectories), it does not matter how these sub-directories are organized within the top-level directory.\n",
    "\n",
    "- **Unlabeled Data**: If there are unlabeled documents you would like labeled: add these files into the folder, and specify their filenames in `metadata.csv` with their corresponding `label` column taking a value Cleanlab will interpret as unlabeled. For multi-class classification datasets, such values include: empty string (`\"\"`) or None (`None`, `np.nan`)). For tagging rather than classifying documents, follow the [multi-label classification guide]((/guide/concepts/datasets/#multi-label) on how to represent unlabeled data.  We recommend at least 5 documents labeled for each possible class, in order for Cleanlab's AutoML system to effectively learn about your data.\n",
    "\n",
    "#### Create ZIP file\n",
    "\n",
    "After it is properly structured, simply zip the your top-level directory. For example, we produced the ZIP file used in this tutorial using the following terminal command:\n",
    "```bash\n",
    "zip -r documents_cl.zip documents_cl/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8e26b4-4e4b-4b63-be93-dc84c80a0a94",
   "metadata": {},
   "source": [
    "## Load Dataset into Cleanlab Studio\n",
    "\n",
    "Now that the data is in an appropriately formatted zip file, let's load it into Cleanlab Studio. First use your API key to instantiate a `studio` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8f42075-93fb-40eb-b179-ff06b9312775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get API key from here: https://app.cleanlab.ai/account after creating an account\n",
    "API_KEY = \"<API_KEY>\"\n",
    "\n",
    "studio = Studio(API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65a5900-e88c-441e-8367-06dce4d6d9f0",
   "metadata": {},
   "source": [
    "Next let's load the dataset into Cleanlab Studio, which will require that it has been properly formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d3fa54f-710b-4f89-b8ce-d9ddf552833b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading dataset...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|\n",
      "Ingesting Dataset...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ID: b658befd357f4d438056829cf1d2e936\n"
     ]
    }
   ],
   "source": [
    "dataset_id = studio.upload_dataset(dataset='documents_cl_labeled.zip', dataset_name='document_quickstart_dataset')\n",
    "print(f\"Dataset ID: {dataset_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3a8790-a86f-43d8-b7ab-5d8f4386720b",
   "metadata": {},
   "source": [
    "## Begin Document Curation\n",
    "\n",
    "After your dataset is successfully loaded, it will appear under the **Datasets** tab in the [Cleanlab Studio Web Inferface](https://app.cleanlab.ai/). The document dataset appears as a text dataset with the text extracted from each document, such that it can be handled like any other text dataset in Cleanlab Studio. Each document is represented as a row in this text dataset, with the document file name listed in a **Document** column, the **Topic** column containing the annotations specified in `metadata.csv`, and a **Text** column containing the extracted text from this document.\n",
    "\n",
    "\n",
    "![Loaded document dataset as text dataset](../assets/document-data-quickstart-tutorial/dataset-upload-img.png)\n",
    "\n",
    "At this point, you can treat your document collection as a text dataset within Cleanlab Studio and run Projects like you would for text datasets. Refer to the [Text Data Quickstart Tutorial](/tutorials/text_data_quickstart/) to understand next steps regarding how Cleanlab Studio works with text datasets and how to obtain/understand the results. You can map each Cleanlab result back to the corresponding document via the **Document** column.\n",
    "\n",
    "\n",
    "## Applications\n",
    "\n",
    "One common application of Cleanlab Studio to documents is: curating a document collection to prepare for Retrieval-Augmented Generation -- learn about this via our [instructional video](https://www.youtube.com/watch?v=-VXKmMKBjhU).\n",
    "\n",
    "Other applications include:\n",
    "- automated labeling/categorization/tagging of documents\n",
    "- catching mis-categorized / mis-tagged documents / other bad document annotations\n",
    "- catching near duplicate documents, outliers, as well as documents containing low-quality (poorly-written) text, foreign languages, or unsafe content (toxicity, PII, etc).\n",
    "\n",
    "To learn how to accomplish these tasks, follow the analogous text data tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd22fc87-e4d3-4903-a6c4-215cc307e218",
   "metadata": {},
   "source": [
    "## Other Document Types\n",
    "\n",
    "Have documents of a file type not covered in the above list of supported file types? \n",
    "\n",
    "You can either: convert your documents to a supported file type, extract the text from them yourself, or [contact us](https://cleanlab.ai/sales/) if you want to run Cleanlab Studio directly on other types of documents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}