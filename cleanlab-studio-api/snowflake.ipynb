{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_OES2hbn2AK"
   },
   "source": [
    "# How to Use Cleanlab Studio With Your Data Stored in Snowflake\n",
    "\n",
    "This tutorial demonstrates how\u00a0to run Cleanlab Studio on data stored in the Snowflake platform.\u00a0As an example, we consider an application of text classification with LLMs, improving the models by improving the data they are fine-tuned on.\n",
    "We\u2019ll see how Cleanlab Studio systematically improves the training data to boost LLM performance by 37%, without requiring you spending any time or resources to change the model architecture, hyperparameters, or the training process.\n",
    "\n",
    "LLMs acquire powerful generative and discriminative capabilities after being pre-trained on a large corpus of text (usually scraped from the internet), but producing reliable outputs for a particular business use case often requires additional training (*fine-tuning*) on a labeled data set from the application domain.\n",
    "\n",
    "Labeled data powers AI/ML\u00a0in the enterprise, but real-world datasets have been found to\u00a0[contain between 7-50% annotation errors](https://go.cloudfactory.com/hubfs/02-Contents/3-Reports/Crowd-vs-Managed-Team-Hivemind-Study.pdf). Imperfectly-labeled text data hampers the training (and evaluation of) ML models across tasks like intent recognition, entity recognition, and sequence generation. Although pretrained LLMs are equipped with a lot of world knowledge, their performance is adversely affected by noisy training data (as\u00a0[noted by OpenAI](https://openai.com/research/dall-e-2-pre-training-mitigations)). This tutorial illustrates how using Cleanlab Studio to improve the training data can mitigate the negative effects of bad data (such as erroneous labels) without writing code or spending time to change the model architecture, hyperparameters, or training process.\n",
    "Because Cleanlab Studio improves the data itself (regardless of which model is used) it remains applicable for LLMs that are yet to be invented, like GPT-10.\n",
    "\n",
    "This tutorial applies LLMs to a politeness classification task, beginning by fine-tuning OpenAI's Davinci model on the original dataset. The baseline model achieves moderate performance, but by automatically finding and fixing errors in the data using the\u00a0Snowflake connector\u00a0for\u00a0Cleanlab Studio, we can achieve significantly better performance\u00a0*using the same LLM model and fine-tuning process*, just by improving the data (and spending minimal human time manually reviewing data). We see a 37% reduction in LLM prediction error after using Cleanlab Studio to improve the dataset:\n",
    "\n",
    "![Improvement using Cleanlab Studio](https://github.com/cleanlab/cleanlab-studio-tutorials/blob/main/assets/databricks/barchart.png?raw=1)\n",
    "\n",
    "See the accompanying [blog post](https://cleanlab.ai/blog/fine-tune-LLM/) for additional context on LLMs and fine-tuning, why data quality matters for LLMs and ML tasks in general, and how\u00a0Cleanlab Studio\u00a0can help you easily improve ML model robustness and performance by systematically improving data quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6k4FKU3n2AM"
   },
   "source": [
    "## Install and configure dependencies\n",
    "\n",
    "This tutorial uses the fine-tuning APIs\u00a0[offered by OpenAI](https://platform.openai.com/docs/guides/fine-tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-I-mz_xkn2AM"
   },
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "!pip install snowflake-snowpark-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5A29lBOHn2AN"
   },
   "source": [
    "### Configure OpenAI API key\n",
    "\n",
    "Note that invoking the OpenAI API will use credits or bill you. The estimated cost to run this tutorial is 15 dollars with the Davinci model, which is the most powerful and most expensive. You can also scale down to the Curie or Ada model to reduce costs, by setting\u00a0`openai_model`\u00a0in the cell below, replacing \"davinci\" with \"curie\" or \"ada\". Fine-tuning the Ada model costs 1 dollar per run with the given dataset.\n",
    "\n",
    "Put your OpenAI API key in the cell below. You can find your API key at\u00a0https://platform.openai.com/api-keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uUD5Zhnnn2AN"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# we set the environment variable because it is used by the OpenAI command-line tool\n",
    "os.environ['OPENAI_API_KEY'] = \"<YOUR_OPENAI_API_KEY>\"\n",
    "# we also set the .api_key property below for the Python API\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "# set openai model name\n",
    "openai_model = 'davinci'\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AArcLN1-n2AO"
   },
   "source": [
    "### Set up a Snowflake connector\n",
    "Connect to Snowflake using [Snowpark](https://docs.snowflake.com/en/developer-guide/snowpark/python/creating-session). Select the database and schema you want to store this tutorial's datasets under."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CwIKwKtRf9Or"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark import Session\n",
    "\n",
    "# Create a Snowflake session\n",
    "config_dict = {\n",
    "    \"account\": \"<YOUR_SNOWFLAKE_ACCOUNT_NAME>\",\n",
    "    \"user\": \"<YOUR_SNOWFLAKE_USER_NAME>\",\n",
    "    \"password\": \"<YOUR_SNOWFLAKE_PASSWORD>\",\n",
    "    \"database\": \"<YOUR_SNOWFLAKE_DATABASE>\", # database to store datasets in\n",
    "    \"schema\": \"<YOUR_SNOWFLAKE_SCHEMA>\", # schema to store datasets in under database\n",
    "}\n",
    "\n",
    "session = Session.builder.configs(config_dict).create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdDU4192n2AO"
   },
   "source": [
    "## Download and prepare data\n",
    "\n",
    "Here we consider a 3-class variant of the Stanford Politeness Dataset, which has text phrases labeled as: impolite, neutral, or polite. Annotated by human raters, some of these labels are naturally low-quality.\n",
    "\n",
    "The training dataset has 1916 examples each labeled by a single human annotator, and thus some may be unreliable.\n",
    "\n",
    "The test dataset has 480 examples each labeled by 5 annotators, and we use their consensus label as a high-quality approximation of the true politeness (measuring test accuracy against these consensus labels). To ensure a fair comparison, this test dataset remains fixed throughout the experiments in this tutorial (all data quality improvement is done only for the training set).\n",
    "\n",
    "To prepare the data, we download raw data into\u00a0snowflake DataFrames, do some processing to prepare the dataset for downstream tasks, store intermediate files in Snowflake stages, and save the processed data in Snowflake tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rM-dnxo_n2AP",
    "outputId": "71c0d372-b27b-4932-dc07-5ca9e255685d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "politeness_train = session.create_dataframe(pd.read_csv(\"https://cleanlab-public.s3.amazonaws.com/StudioDemoDatasets/improving_llms/train.csv\"))\n",
    "politeness_test = session.create_dataframe(pd.read_csv(\"https://cleanlab-public.s3.amazonaws.com/StudioDemoDatasets/improving_llms/test.csv\"))\n",
    "politeness_train.write.mode(\"overwrite\").save_as_table(\"politeness_train\")\n",
    "politeness_test.write.mode(\"overwrite\").save_as_table(\"politeness_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nZNjuJCn2AP"
   },
   "source": [
    "We can inspect this prepared data, looking at some specific rows to highlight data errors that are present. For example, the data point on row `1426`\u00a0is erroneously labeled \"impolite\" when \"polite\" is a more appropriate label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "wAhSV_gyn2AP",
    "outputId": "125cd196-a469-4ba0-e961-b064d8c0e7f3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>Thanks for your response! How should we proceed ?</td>\n",
       "      <td>impolite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>Alright then!?</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>That image is CC and has a link to its source....</td>\n",
       "      <td>polite</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 prompt completion\n",
       "1426  Thanks for your response! How should we proceed ?   impolite\n",
       "299                                      Alright then!?    neutral\n",
       "134   That image is CC and has a link to its source....     polite"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(politeness_train.to_pandas().iloc[[1426, 299, 134]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKVdTHn9n2AP"
   },
   "source": [
    "### Formatting data for fine-tuning\n",
    "\n",
    "We are using the OpenAI APIs for fine-tuning, which require data in a specific format (JSONL, newline-delimited JSON objects). We also need to do some pre-processing of the label column, adding whitespace before the completion, as the API recommends.\n",
    "\n",
    "We save the prepared results on a Snowflake stage, which can be used by the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "id": "b2tBa2HUn2AQ",
    "outputId": "5dd06fee-bad0-4b07-f48e-0304f3251b45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_train.jsonl UPLOADED\n",
      "processed_test.jsonl UPLOADED\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import io\n",
    "import snowflake.snowpark.functions as F\n",
    "\n",
    "_ = session.sql(\"create or replace stage politeness_prepared_data\").collect()\n",
    "\n",
    "def prepare_data(df, path, stage=\"politeness_prepared_data\"):\n",
    "    '''\n",
    "    Write a dataframe into a single JSONL file located at path under a specified stage, in a format appropriate for fine tuning.\n",
    "\n",
    "    This makes a small tweak to the data, namely, adding whitespace before the completion.\n",
    "    '''\n",
    "    # add whitespace to the completion, as OpenAI requires\n",
    "    df = df.withColumn('\"completion\"', F.concat(F.lit(' '), '\"completion\"'))\n",
    "    \n",
    "    fileobj = io.BytesIO()\n",
    "    for row in df.to_local_iterator():\n",
    "        fileobj.write(str.encode(f'{json.dumps(row.as_dict())} \\n'))\n",
    "    fileobj.seek(0)\n",
    "    \n",
    "    put_result = session.file.put_stream(fileobj, f\"@{stage}/{path}\", auto_compress=False)\n",
    "    print(f\"{path} {put_result.status}\")\n",
    "\n",
    "\n",
    "prepare_data(politeness_train, 'processed_train.jsonl')\n",
    "prepare_data(politeness_test, 'processed_test.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N59ssTW9n2AQ"
   },
   "source": [
    "## Fine-Tune and evaluate OpenAI model without Cleanlab Studio (accuracy ~65%)\n",
    "\n",
    "We use the\u00a0[OpenAI fine-tuning API](https://platform.openai.com/docs/guides/fine-tuning)\u00a0to first establish a baseline by:\n",
    "\n",
    "- Fine-tuning the OpenAI model on our (original, with some errors) training set\n",
    "- Evaluating the model on our test set\n",
    "\n",
    "First, we upload our training set and test set to OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7MuWCFpfn2AQ"
   },
   "outputs": [],
   "source": [
    "processed_train_jsonl = session.file.get_stream(\"@politeness_prepared_data/processed_train.jsonl\")\n",
    "processed_test_jsonl = session.file.get_stream(\"@politeness_prepared_data/processed_test.jsonl\")\n",
    "\n",
    "train_file = client.files.create(file=processed_train_jsonl, purpose='fine-tune')\n",
    "test_file = client.files.create(file=processed_test_jsonl, purpose='fine-tune')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZ8dlaQVn2AQ"
   },
   "source": [
    "Then, we invoke the fine-tuning API to fine tune the\u00a0`openai_model`\u00a0(\"davinci\" by default, unless you changed it above). Note that this incurs some\u00a0[cost](https://openai.com/pricing), roughly $7.50 for the Davinci model or $0.50 for the Ada model, at the time this was written.\n",
    "\n",
    "We also invoke the fine-tuning API with the\u00a0`validation_file`\u00a0keyword argument, so that the API will automatically compute statistics including model accuracy on the test set after the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "RzkjJ7r8n2AQ"
   },
   "outputs": [],
   "source": [
    "response = client.fine_tunes.create(\n",
    "    training_file=train_file.id,\n",
    "    validation_file=test_file.id,\n",
    "    compute_classification_metrics=True,\n",
    "    classification_n_classes=3,\n",
    "    model=openai_model,\n",
    "    suffix='baseline'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88aqRveon2AQ"
   },
   "source": [
    "You can follow the progress of fine-tuning with the following command. Once it's done, it'll print \"Job complete!\". You might need to re-run the cell if it times out. Training time varies based on queue length and other factors;\u00a0**it can take up to 1 hour to fine-tune the LLM**. The block below would check the status of the finetune and block execution until the finetune is complete. The block is based on this\u00a0[openai-cookbook example](https://github.com/openai/openai-cookbook/blob/594fc6c952425810e9ea5bd1a275c8ca5f32e8f9/examples/azure/finetuning.ipynb#L278)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BhST6CQyn2AQ"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def wait_for_finetune(job_id):\n",
    "  status = client.fine_tunes.retrieve(fine_tune_id=job_id).status\n",
    "  if status not in [\"succeeded\", \"failed\"]:\n",
    "      print(f'Job not in terminal status: {status}. Waiting.')\n",
    "      while status not in [\"succeeded\", \"failed\"]:\n",
    "          time.sleep(60)\n",
    "          status = client.fine_tunes.retrieve(fine_tune_id=job_id).status\n",
    "          print(f'Status: {status}')\n",
    "  else:\n",
    "      print(f'Finetune job {job_id} finished with status: {status}')\n",
    "\n",
    "wait_for_finetune(response.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQNyNo2bn2AQ"
   },
   "source": [
    "Once the job completes, we see the test accuracy achieved when fine-tuning this LLM on the original training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Accuracy: 65.2%\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "def get_accuracy(fine_tune_id):\n",
    "    file_id = client.fine_tunes.retrieve(fine_tune_id=fine_tune_id).result_files[0].id\n",
    "    file_content = client.files.content(file_id=file_id).text\n",
    "    base_df = pd.read_csv(io.StringIO(file_content))\n",
    "    baseline_acc = base_df.iloc[-1]['classification/accuracy']\n",
    "    print(f\"Fine-tuning Accuracy: {baseline_acc:.1%}\")\n",
    "\n",
    "get_accuracy(response.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYJ-kiLgn2AQ"
   },
   "source": [
    "### Baseline results: ~65% accuracy\n",
    "\n",
    "Our baseline Davinci LLM achieves a\u00a0**test accuracy of 65%**\u00a0when fine-tuned on the original training data (Curie achieved 64% accuracy, Ada achieved 60% accuracy). Model training is nondeterministic, so your results might vary slightly, even with the exact same dataset and initial model checkpoint. OpenAI's models might also be changed/updated over time.\n",
    "\n",
    "Even a state-of-the-art LLM like the Davinci\u00a0model produces lackluster results for this classification task; is it because of low data quality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_M-D17TFn2AQ"
   },
   "source": [
    "## Improve the data using Cleanlab Studio and re-train the LLM (accuracy ~78%)\n",
    "\n",
    "Next, we use the\u00a0[Snowflake connector](https://github.com/cleanlab/cleanlab-studio)\u00a0for\u00a0[Cleanlab Studio](https://app.cleanlab.ai/)\u00a0to automatically improve the data quality, and then re-train our LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Cleanlab Studio\n",
    "1. If you don't have an account already,\u00a0[sign up for an account](https://app.cleanlab.ai/). It may take up to one day to get access.\n",
    "2. Get your\u00a0[API key](https://app.cleanlab.ai/account?tab=General)\u00a0and enter it below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade cleanlab-studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "HUP7gm6An2AQ"
   },
   "outputs": [],
   "source": [
    "import cleanlab_studio\n",
    "\n",
    "CLEANLAB_STUDIO_API_KEY = \"<YOUR_CLEANLAB_STUDIO_API_KEY>\"\n",
    "studio = cleanlab_studio.Studio(CLEANLAB_STUDIO_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8Sa4DF5n2AQ"
   },
   "source": [
    "### Upload dataset to Cleanlab Studio\n",
    "\n",
    "Next, we can directly upload a Spark DataFrame to Cleanlab Studio by passing it to\u00a0`studio.upload_dataset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "n64TwUZQn2AQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading dataset...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|\n",
      "Generating schema...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|\n",
      "Ingesting Dataset...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|\n"
     ]
    }
   ],
   "source": [
    "dataset_id = studio.upload_dataset(politeness_train, dataset_name='Stanford Politeness')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M96yZnaOn2AQ"
   },
   "source": [
    "### **Create a project**\n",
    "\n",
    "To analyze the data, use the\u00a0[Cleanlab Studio web UI](https://app.cleanlab.ai/)\u00a0to create a project, configuring it according to the ML task. For this demo, you should select:\n",
    "\n",
    "- ML task: text classification\n",
    "- Type of classification: multi-class\n",
    "- Text column: prompt (will be auto-detected)\n",
    "- Label column: completion (will be auto-detected)\n",
    "\n",
    "Select fast mode or regular mode depending on the speed/quality tradeoff you desire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFdo8CxPn2AQ"
   },
   "source": [
    "![Creating a project.](../assets/databricks/project_new.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51zsENv_n2AQ"
   },
   "source": [
    "### Make corrections to your dataset with Cleanlab Studio\n",
    "\n",
    "Cleanlab Studio not only finds data points with potential issues, but it also makes suggestions for how to address the issues (e.g., changing the label of a data point). Deciding how to make use of the analysis results is up to you. For example, you could discard all potentially erroneous data points, or you could review the data points most likely to have issues and make corrections. This human-in-the-loop data correction usually gives the best results.\n",
    "\n",
    "If you want to save time, you could briefly review some flagged issues, and then auto-fix the top issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UuM0-Uion2AQ"
   },
   "source": [
    "![Creating a project.](../assets/databricks/resolver_new.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpbA9MhHn2AQ"
   },
   "source": [
    "The selected row in the screenshot above is an example of a poorly-labeled datapoint.\n",
    "\n",
    "The phrase:\n",
    "> I went through your comments. How does it look now?\n",
    ">\n",
    "is labeled \"impolite\". Cleanlab Studio flags this as a label error, and it suggests that the label be switched to \"neutral\". In the screenshot above, we pressed \"W\" to accept Cleanlab Studio's suggestion to automatically fix the label.\n",
    "\n",
    "Label issues like this cause the accuracy of the fine-tuned LLM to be degraded. Correcting these issues allows us to train an improved LLM, as we'll see below.\n",
    "\n",
    "### Export your improved dataset back into Snowflake\n",
    "\n",
    "Once you're done correcting issues found in your dataset with Cleanlab Studio, export the improved dataset back into Snowflake as a table:\n",
    "\n",
    "1. Click on the \"Export Cleanset\" button in your Cleanlab Studio project\n",
    "2. Select the \"Export using API\" tab\n",
    "3. Copy the \"cleanset ID\" and paste it into the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gKz0xAj_n2AR"
   },
   "outputs": [],
   "source": [
    "cleanset_id = \"<PASTE YOUR CLEANSET ID HERE>\"\n",
    "politeness_train_fixed = studio.apply_corrections(cleanset_id, politeness_train)\n",
    "politeness_train_fixed.write.mode(\"overwrite\").save_as_table(\"politeness_train_fixed\")\n",
    "display(politeness_train_fixed.to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTQOP263n2AU"
   },
   "source": [
    "### Fine-tune the LLM on your improved dataset and evaluate the results\n",
    "\n",
    "Let's see how Cleanlab Studio improves the performance of the LLM. We follow the same process as earlier, except we use the\u00a0`politeness_train_fixed`\u00a0DataFrame as our training data.\n",
    "\n",
    "When we ran the experiment below, we used Cleanlab Studio's web interface to review the data issues that it flagged. Machine-augmented human-in-the-loop data improvement often gives the best results. If you want to use the dataset that we exported from Cleanlab Studio, uncomment the line below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Asn2omun2AU"
   },
   "outputs": [],
   "source": [
    "# By default for reproducibility, we use the dataset that we exported from Cleanlab Studio as csv\n",
    "# But if you want to use your dataset that you improved, downloaded as politeness_train_fixed above\n",
    "# set the flag below to 'False'\n",
    "use_provided_training_set_improved_using_cleanlab_studio = True\n",
    "if use_provided_training_set_improved_using_cleanlab_studio:\n",
    "    politeness_train_fixed = pd.read_csv('https://cleanlab-public.s3.amazonaws.com/StudioDemoDatasets/improving_llms/train_fixed.csv')\n",
    "\n",
    "prepare_data(politeness_train_fixed, 'train_fixed.jsonl')\n",
    "\n",
    "train_fixed_jsonl = session.file.get_stream(\"@politeness_prepared_data/train_fixed.jsonl\")\n",
    "train_file_fixed = openai.files.create(file=train_fixed_jsonl, purpose='fine-tune')\n",
    "\n",
    "response_fixed = openai.fine_tunes.create(\n",
    "    training_file=train_file_fixed.id,\n",
    "    validation_file=test_file.id,\n",
    "    compute_classification_metrics=True,\n",
    "    classification_n_classes=3,\n",
    "    model=openai_model,\n",
    "    suffix='fixed'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqxXUQt_n2AU"
   },
   "source": [
    "You can follow the progress of fine-tuning with the following command. Once it's done, it'll print \"Job complete!\". You might need to re-run the cell if it times out. Training time varies based on queue length and other factors; it can take up to 1 hour to fine-tune the LLM. We use the\u00a0`wait_for_finetune`\u00a0function defined before to block this step until the finetuning is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uTnuGP_Sn2AU"
   },
   "outputs": [],
   "source": [
    "wait_for_finetune(response_fixed.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7eEKEaRrn2AU"
   },
   "source": [
    "Once the job completes, we see the test accuracy achieved when fine-tuning this LLM on the improved dataset. If you simply auto-fixed some of the labels (spending zero human time on data improvement), you'll still see improvement; if you reviewed some of Cleanlab Studio's suggestions following a human-in-the-loop data cleaning process, you'll see larger improvements here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A22LGJZmn2AU"
   },
   "outputs": [],
   "source": [
    "get_accuracy(response_fixed.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtLyHi92n2AU"
   },
   "source": [
    "### Impact of data improvement using Cleanlab Studio: ~78% accuracy (compared to ~65% baseline accuracy)\n",
    "\n",
    "Training on the improved dataset, we see a\u00a0**test accuracy of 78%**\u00a0for the Davinci model (Curie achieved ~76% accuracy, Ada achieved ~75% accuracy). These results are from our\u00a0`train_fixed.csv`\u00a0(provided above); results on your dataset will vary depending on how you improved the dataset using Cleanlab Studio (e.g., whether you used auto-fix or manually reviewed the top issues, how you corrected labels, how you removed outliers, etc.). Even the results of fine-tuning on the provided dataset might vary slightly, because model training is nondeterministic, and OpenAI's initial model checkpoints may be updated over time.\n",
    "\n",
    "In this evaluation, we see that data quality has a huge impact on LLM performance.\u00a0**By simply improving the data quality**\u00a0(and leaving the original LLM checkpoint, training parameters, fine-tuning process, etc. as-is), we have\u00a0**reduced prediction error by ~37%**.\n",
    "Because Cleanlab Studio improves models by improving the underlying data, the strategy outlined here works for any model or LLM that exists today or may exist in the future!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}