{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trustworthy Retrieval-Augmented Generation with the Trustworthy Language Model\n",
    "\n",
    "This tutorial demonstrates how to replace the Generator LLM in any RAG system with Cleanlab's Trustworthy Language Model (TLM), to score the trustworthiness of answers and improve overall reliability.\n",
    "We recommend first completing the [TLM quickstart tutorial](/tutorials/tlm/).\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) has become popular for building LLM-based Question-Answer systems in domains where LLMs alone suffer from: hallucination, knowledge gaps, and factual inaccuracies. However, RAG systems often still produce unreliable responses, because they depend on LLMs that are fundamentally unreliable. Cleanlab's Trustworthy Language Model (TLM) offers a solution by providing trustworthiness scores to assess and improve response quality, **independent of your RAG architecture or retrieval and indexing processes**. To diagnose when RAG answers cannot be trusted, simply swap your existing LLM that is generating answers based on the retrieved context with TLM. This tutorial showcases this for a standard RAG system, based off a tutorial in the popular [LlamaIndex](https://docs.llamaindex.ai/) framework. Here we merely replace the LLM used in the LlamaIndex tutorial with TLM, and showcase some of the benefits. TLM can be similarly inserted into *any* other RAG framework.\n",
    "\n",
    "![TLM RAG system correctly identifying high/low confidence responses](../assets/tlm-rag-tutorial/thumbnail.png)\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "RAG is all about connecting LLMs to data, to better inform their answers. This tutorial uses Nvidia's Q1 FY2024 earnings report as an example dataset.\n",
    "Use the following commands to download the data (earnings report) and store it in a directory named `data/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc 'https://cleanlab-public.s3.amazonaws.com/Datasets/NVIDIA_Financial_Results_Q1_FY2024.md'\n",
    "!mkdir -p ./data\n",
    "!mv NVIDIA_Financial_Results_Q1_FY2024.md data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's next install required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U cleanlab-studio llama-index llama-index-embeddings-huggingface\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then initialize our Cleanlab Studio client.\n",
    "You can get your API key for Cleanlab Studio client from here: https://app.cleanlab.ai/account after creating an account. For detailed instructions, refer to [this guide](https://help.cleanlab.ai/guide/quickstart/api/#api-key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab_studio import Studio\n",
    "\n",
    "studio = Studio(\"<insert your API key>\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate TLM with LlamaIndex\n",
    "\n",
    "\n",
    "TLM not only provides a response but also includes a **trustworthiness score** indicating the confidence that this response is good/accurate.\n",
    "Here we initialize a TLM object with default settings. You can achieve better results by playing with the TLM configurations outlined in the Advanced section of the [TLM quickstart tutorial](/tutorials/tlm/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlm = studio.TLM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our RAG pipeline closely follows the LlamaIndex guide on [Using a custom LLM Model](https://docs.llamaindex.ai/en/stable/module_guides/models/llms/usage_custom/#example-using-a-custom-llm-model-advanced). LLamaIndex's `CustomLLM` class exposes two methods, `complete()` and `stream_complete()`, for returning the LLM response. Additionally, it provides a `metadata` property to specify LLM details such as context window, number of output tokens, and name of your LLM.\n",
    "\n",
    "Here we create a `TLMWrapper` subclass of `CustomLLM` that uses our TLM object instantiated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "import json\n",
    "\n",
    "# Import LlamaIndex dependencies\n",
    "from llama_index.core.base.llms.types import (\n",
    "    CompletionResponse,\n",
    "    CompletionResponseGen,\n",
    "    LLMMetadata,\n",
    ")\n",
    "from llama_index.core.llms.callbacks import llm_completion_callback\n",
    "from llama_index.core.llms.custom import CustomLLM\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "\n",
    "class TLMWrapper(CustomLLM):\n",
    "    context_window: int = 16000\n",
    "    num_output: int = 256\n",
    "    model_name: str = \"TLM\"\n",
    "\n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        \"\"\"Get LLM metadata.\"\"\"\n",
    "        return LLMMetadata(\n",
    "            context_window=self.context_window,\n",
    "            num_output=self.num_output,\n",
    "            model_name=self.model_name,\n",
    "        )\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n",
    "        # Prompt tlm for a response and trustworthiness score\n",
    "        response: Dict[str, str] = tlm.prompt(prompt)\n",
    "        output = json.dumps(response)\n",
    "        return CompletionResponse(text=output)\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:\n",
    "        # Prompt tlm for a response and trustworthiness score\n",
    "        response = tlm.prompt(prompt)\n",
    "        output = json.dumps(response)\n",
    "\n",
    "        # Stream the output\n",
    "        output_str = \"\"\n",
    "        for token in output:\n",
    "            output_str += token\n",
    "            yield CompletionResponse(text=output_str, delta=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a RAG pipeline with TLM\n",
    "\n",
    "Now let's integrate our TLM-based `CustomLLM` into a RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = TLMWrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Embedding Model\n",
    "\n",
    "RAG uses an embedding model to match queries against document chunks to retrieve the most relevant data. Here we opt for a no-cost, local embedding model from Hugging Face. You can use any other embedding model by referring to this [LlamaIndex guide](https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings/#embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Create Index + Query Engine\n",
    "\n",
    "Let's create an index from the documents stored in the data directory. The system can index multiple files within the same folder, although for this tutorial, we'll use just one document.\n",
    "We stick with the default index from LlamaIndex for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated index is used to power a query engine over the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that TLM is agnostic to the index and the query engine used for RAG, and is compatible with any choices you make for these components of your system."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering queries with our RAG system\n",
    "\n",
    "Let's try out our RAG pipeline based on TLM. Here we pose questions with differing levels of complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": 0,
   "metadata": {},
   "source": [
    "**Optional: Define `display_response` helper function**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This method presents formatted responses from our TLM-based RAG pipeline. It parses the output to display both the response itself and the corresponding trustworthiness score.\n",
    "def display_response(response):\n",
    "    response_str = response.response\n",
    "    output_dict = json.loads(response_str)\n",
    "    print(f\"Response: {output_dict['response']}\")\n",
    "    print(f\"Trustworthiness score: {round(output_dict['trustworthiness_score'], 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy Questions\n",
    "\n",
    "We first pose straightforward questions that can be directly answered by the provided data and can be easily located within a few lines of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: NVIDIA's total revenue in the first quarter of fiscal 2024 was $7.19 billion.\n",
      "Trustworthiness score: 0.97\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"What was NVIDIA's total revenue in the first quarter of fiscal 2024?\"\n",
    ")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The percentage increase in NVIDIA's GAAP net income from Q4 FY23 to Q1 FY24 was 44%.\n",
      "Trustworthiness score: 0.93\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"What was the percentage increase in NVIDIA's GAAP net income from Q4 FY23 to Q1 FY24?\"\n",
    ")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Jensen Huang, NVIDIA's CEO, commented on the significant transitions the computer industry is undergoing, particularly accelerated computing and generative AI.\n",
      "Trustworthiness score: 0.97\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"What significant transitions did Jensen Huang, NVIDIA's CEO, comment on?\"\n",
    ")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TLM returns high trustworthiness scores for these responses, indicating high confidence they are accurate. After doing a quick fact-check (reviewing the original earnings report), we can confirm that TLM indeed accurately answered these questions. In case you're curious, here are relevant excerpts from the data context for these questions:\n",
    "\n",
    "> NVIDIA (NASDAQ: NVDA) today reported revenue for the first quarter ended April 30, 2023, of $7.19 billion, ...\n",
    "\n",
    "> GAAP earnings per diluted share for the quarter were $0.82, up 28% from a year ago and up 44% from the previous quarter.\n",
    "\n",
    "> Jensen Huang, founder and CEO of NVIDIA, commented on the significant transitions the computer industry is undergoing, particularly accelerated computing and generative AI, ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions without Available Context \n",
    "\n",
    "Now let's see how TLM responds to queries that *cannot* be answered using the provided data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The report explains that NVIDIA's Gaming revenue decreased year over year due to a 38% decline in first-quarter revenue compared to the previous year. However, it does not provide specific reasons for this decline.\n",
      "Trustworthiness score: 0.62\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"How does the report explain why NVIDIA's Gaming revenue decreased year over year?\"\n",
    ")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The given context information does not provide any information about the industry average dividend payout. Therefore, it is not possible to compare NVIDIA's dividend payout for this quarter to the industry average based on the given information.\n",
      "Trustworthiness score: 0.89\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"How does NVIDIA's dividend payout for this quarter compare to the industry average?\",\n",
    ")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that TLM demonstrates the ability to recognize the limitations of the available information. It refrains from generating speculative responses or hallucinations, thereby maintaining the reliability of the question-answering system. This behavior showcases an understanding of the boundaries of the context and prioritizes accuracy over conjecture. The lower TLM trustworthiness score indicate a bit more uncertainty about the response, which aligns with the lack of information available.\n",
    "\n",
    "### Challenging Questions\n",
    "\n",
    "Let's see how our RAG system responds to harder questions, some of which may be misleading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Nvidia's revenue decreased by $1.6 billion this quarter compared to last quarter.\n",
      "Trustworthiness score: 0.44\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"How much did Nvidia's revenue decrease this quarter compared to last quarter, in dollars?\"\n",
    ")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The report mentions the following companies:\n",
      "\n",
      "1. NVIDIA\n",
      "2. Google Cloud\n",
      "3. Amazon Web Services\n",
      "4. Microsoft Azure\n",
      "5. Oracle Cloud Infrastructure\n",
      "6. ServiceNow\n",
      "7. Dell Technologies\n",
      "8. Medtronic\n",
      "9. BYD\n",
      "10. CD PROJEKT RED\n",
      "11. Microsoft\n",
      "12. NVIDIA Corporation (mentioned in the copyright statement)\n",
      "13. Ada\n",
      "14. NVIDIA Hopper\n",
      "15. NVIDIA Grace Hopper\n",
      "16. NVIDIA cuLitho\n",
      "17. NVIDIA AI Foundations\n",
      "18. NVIDIA H100 Tensor Core GPU\n",
      "19. NVIDIA AI Enterprise\n",
      "20. NVIDIA Omniverse\n",
      "Trustworthiness score: 0.59\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"There were 20 companies mentioned in the report. List all of them.\",\n",
    ")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TLM automatically alerts us that these answers are unreliable, by the low trustworthiness score. RAG systems with TLM help you properly exercise caution when you see low trustworthiness scores. Here are the correct answers to the aforementioned questions:\n",
    "\n",
    "> NVIDIA's revenue increased by $1.14 billion this quarter compared to last quarter.\n",
    "\n",
    "> There are only 10 companies mentioned in total.\n",
    "\n",
    "## Comparing TLM Trustworthiness Scores vs. OpenAI GPT-4 Logprobs\n",
    "\n",
    "One approach that OpenAI recommends to rate confidence in a response is via the average [log probabilities](https://cookbook.openai.com/examples/using_logprobs) of the tokens output by the LLM neural network model.\n",
    "\n",
    "We built the same LlamaIndex RAG system using GPT-4 in place of TLM via this [code](https://github.com/cleanlab/cleanlab-tools/blob/rag-tutorial-resources/gpt4-rag-logprobs/gpt4-rag-logprobs.ipynb). We then asked the GPT-4 RAG system the following query and report its response and the associated average token-probability from this model. We also repeated this with our TLM RAG system (both systems have the same context).\n",
    "\n",
    "| **Query**                                                     | **GPT-4 Response** | **GPT-4 Average Token Probability** | **TLM Response** | **TLM Trustworthiness Score** |\n",
    "| :---------------------------------------------------------------| :-------------------- | :-------------------------------------| :------------------| :-------------------------------|\n",
    "| True or False: Nvidia's Professional Visualization division is performing better than their Gaming division in terms of percent change in revenue compared to the previous quarter. |     False | 99.19% | False | 0.65 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of a difficult question. Gaming increased by 22% compared to the previous quarter whereas Visualization increased by 31%, so the correct answer is **True**. Both TLM and GPT-4 arrived at the same incorrect answer. While the GPT-4 probabilities are misleading, suggesting a confident answer, the TLM trustworthiness score is much lower, correcting suggesting that this answer is untrustworthy.\n",
    "\n",
    "Relying solely on token probabilities only captures *aleatoric uncertainty* in the ML model, whereas TLM trustworthiness scores capture all forms of uncertainty to better flag unreliable answers. Average token probabilities also tend to be highly influenced by the specific syntax and words used to express the answer, whereas TLM quantifies our confidence that the answer is good overall.\n",
    "\n",
    "With TLM, you can easily increase trust in any RAG system!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}