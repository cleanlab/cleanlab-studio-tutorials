{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-label Model Deployment\n",
    "\n",
    "<head>\n",
    "  <meta name=\"title\" content=\"Instant ML Model Training + Deployment for Multi-Label Data\"/>\n",
    "  <meta property=\"og:title\" content=\"Instant ML Model Training + Deployment for Multi-Label Data\"/>\n",
    "  <meta name=\"twitter:title\" content=\"Instant ML Model Training + Deployment for Multi-Label Data\"/>\n",
    "  <meta name=\"image\" content=\"/img/multilabel_accuracy_comparison.png\" />\n",
    "  <meta property=\"og:image\" content=\"/img/multilabel_accuracy_comparison.png\" />\n",
    "  <meta name=\"twitter:image\" content=\"/img/multilabel-performance-twitter.png\" />\n",
    "  <meta name=\"description\" content=\"A no-code solution to automate data prep, model training, and serving predictions for any image/document tagging dataset.\"  />\n",
    "  <meta property=\"og:description\" content=\"A no-code solution to automate data prep, model training, and serving predictions for any image/document tagging dataset.\" />\n",
    "  <meta name=\"twitter:description\" content=\"A no-code solution to automate data prep, model training, and serving predictions for any image/document tagging dataset.\" />\n",
    "</head>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates how to use Cleanlab Studio to train a model on a multi-label dataset and deploy it in a single click. Using the deployed model, you can make predictions on new data points in the Cleanlab Studio web app. The same steps can also be used to deploy models for multi-class datasets. Model inference is also supported through the [Python API](/tutorials/inference_api).\n",
    "\n",
    "![Model deployment using Cleanlab Studio](https://github.com/cleanlab/cleanlab-studio-tutorials/blob/main/assets/inference-tutorial/mldeployment.png?raw=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the the GoEmotions dataset ([link](https://cleanlab-public.s3.amazonaws.com/StudioDemoDatasets/goemotions/goemotions_train.csv), which contains Reddit comments tagged with one or more emotions like \"love\" or \"gratitude\".\n",
    "\n",
    "Below is what the dataset looks like. The important columns are the `text` column, which has the comment texts, and the `label` column, which has comma-separated strings containing the labels. The labels indicate the sentiments associated with the comment. Some data points can have more than one associated sentiment, e.g. \"admiration,gratitude\".\n",
    "\n",
    "![goemotions data](../assets/multilabel-deployment-web/initial_df.png)\n",
    "\n",
    "We will train a model that takes the `text` column as input and predicts the associated sentiment labels. We will use a train-test split to demo model inference. You can download the test data [here.](https://cleanlab-public.s3.amazonaws.com/StudioDemoDatasets/goemotions/goemotions_test.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We renamed the downloaded training dataset created above to `goemotions_train.csv`, and now we upload the dataset to Cleanlab Studio and create a project. This step can also be accomplished through the [Python API](/tutorials/text_data_quickstart).\n",
    "\n",
    "The project creation step is shown in the video below. Since the main predictive column is the column `text` only, we choose to train a text model with `text` as the text column. If you have a tabular dataset (with multiple predictive columns), you should train a tabular model instead.\n",
    "\n",
    "<Video\n",
    "  width=\"1906\"\n",
    "  height=\"922\"\n",
    "  src=\"../assets/multilabel-deployment-web/create_project.mp4\"\n",
    "  autoPlay={true}\n",
    "  loop={true}\n",
    "  muted={true}\n",
    "/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the project completes, you can use the web interface to fix any issues you see (see this [guide](/guide/quickstart/web/#review-issues-detected-in-your-dataset-and-correct-them) for how to fix issues), and deploy a model by clicking the `Deploy Model` button at the bottom of your [project](/guide/concepts/projects).\n",
    "\n",
    "Since fixing issues is not the focus of this tutorial, we will simply use the `Clean Top K` button to auto-fix the issues, and then deploy a model on the cleaned data. These steps are shown in the video below.\n",
    "\n",
    "**Warning:** in general, we do not recommend blindly auto-fixing the entire dataset; generally, a human-in-the-loop approach gives superior results.\n",
    "\n",
    "<Video\n",
    "  width=\"1906\"\n",
    "  height=\"922\"\n",
    "  src=\"../assets/multilabel-deployment-web/clean_and_deploy.mp4\"\n",
    "  autoPlay={true}\n",
    "  loop={true}\n",
    "  muted={true}\n",
    "/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the deployed model finishes training, we are ready to create the test dataset to be used for inference. As for the test dataset `goemotions_test.csv`, so we will create a new file for these rows with their labels removed. These steps are shown in the video below.\n",
    "\n",
    "<Video\n",
    "  width=\"1910\"\n",
    "  height=\"926\"\n",
    "  src=\"../assets/multilabel-deployment-web/split_test.mp4\"\n",
    "  autoPlay={true}\n",
    "  loop={true}\n",
    "  muted={true}\n",
    "/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the test dataset created, we can use the deployed model to run inference by clicking the `Predict New Labels` button, as shown below.\n",
    "\n",
    "**Note**: If you run model inference on a large test dataset and experience errors, split the test data into multiple batches and separately run inference on each batch.\n",
    "\n",
    "<Video\n",
    "  width=\"1906\"\n",
    "  height=\"922\"\n",
    "  src=\"../assets/multilabel-deployment-web/inference.mp4\"\n",
    "  autoPlay={true}\n",
    "  loop={true}\n",
    "  muted={true}\n",
    "/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, when inference finishes, we can click export and get the predicted probabilities and suggested labels for our test dataset, as shown below.\n",
    "\n",
    "<Video\n",
    "  width=\"1906\"\n",
    "  height=\"922\"\n",
    "  src=\"../assets/multilabel-deployment-web/results.mp4\"\n",
    "  autoPlay={true}\n",
    "  loop={true}\n",
    "  muted={true}\n",
    "/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final exported results can be seen in the screenshot below.\n",
    "\n",
    "![Result table](../assets/multilabel-deployment-web/final_results.png)\n",
    "\n",
    "As shown, the result contains a column for each label, showing the predicted probability of that label being present. There is also a `Suggested Label` column, which contains the likely labels for a data point, gathered in a comma-separated string.\n",
    "\n",
    "Here is a screenshot that shows the first few of the test examples, along with their `Suggested Label`.\n",
    "\n",
    "![Labels Comparison](../assets/multilabel-deployment-web/label_comparison.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have predictions, it is natural to assess how accurate they are for test data with available labels (ideally clean labels). Labels are available for the test data used here, so we evaluated the F1 score of the model predictions for each class, macro-averaging these scores over classes into an overall evaluation performance measure. This is a standard performance measure for multi-label classification tasks.  We also separately trained a scikit-learn model on featurized text (same dataset as used for Cleanlab training) and evaluated its performance on the (same) test set, as well as evaluating the performance of a OpenAI GPT Large Language model used as a multi-label classifier. Here are the Macro F1 scores on the same test set achieved by three ML models \u2013 OpenAI, sklearn, and Cleanlab Studio.\n",
    "\n",
    "![Models Comparision](../assets/multilabel-deployment-web/accuracy_comparison_plot.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}