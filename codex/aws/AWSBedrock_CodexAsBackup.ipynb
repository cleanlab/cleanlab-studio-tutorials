{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "381b2e85-3eee-442d-a089-d4e31124a4e8",
   "metadata": {},
   "source": [
    "# Integrate Codex as-a-Backup with AWS Bedrock Knowledge Bases\n",
    "\n",
    "This notebook demonstrates how to integrate Codex as a backup to an existing RAG application using AWS Knowledge Bases via the [AWS Bedrock Knowledge Bases](https://aws.amazon.com/bedrock/knowledge-bases/) and [Converse](https://docs.aws.amazon.com/bedrock/latest/userguide/tool-use-inference-call.html) APIs.\n",
    "\n",
    "This is our recommended integration strategy for developers using AWS Knowledge Bases, and will allow you to detect and remediate incorrect and unhelpful responses from your RAG app. The integration is **only a few lines of code**.\n",
    "\n",
    "\n",
    "![RAG Workflow](../assets/codexasbackup.png)\n",
    "\n",
    "## Setup\n",
    "\n",
    "This tutorial requires a TLM API key. Get one [here](https://tlm.cleanlab.ai/). Then let's install necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bdf684",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install cleanlab_codex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95d0a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install boto3  # we used package-version 1.36.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59a3b424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CLEANLAB_TLM_API_KEY\"] = \"<API key>\"  # Get your free API key from: https://tlm.cleanlab.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2472f2-a561-4397-9432-b44773ffc67f",
   "metadata": {},
   "source": [
    "## Example RAG App: Customer Service for a New Product\n",
    "\n",
    "Consider a customer support use-case, where the RAG application is built on a Knowledge Base with product pages such as the following:\n",
    "\n",
    "![Image of a beautiful simple water bottle that is definitely worth more than the asking price](../assets/simple_water_bottle.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88eb5fb-bb34-420e-b6e4-a4ec5b595383",
   "metadata": {},
   "source": [
    "### RAG with AWS Knowledge Bases\n",
    "\n",
    "Let's set up our RAG application! To keep this example simple, the Knowledge Base contains only a single document containing the describtion of the product listed above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0940519-d732-4db8-b6ca-21eb4d141be6",
   "metadata": {},
   "source": [
    "**Optional: Helper functions for AWS Knowledge Bases**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f44b67-de6b-4987-8182-c15703d6c8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']= \"<YOUR_AWS_ACCESS_KEY_ID>\" # Your permament access key (not session access key)\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']='<YOUR_AWS_SECRET_ACCESS_KEY>'   # Your permament secret access key (not session secret access key)\n",
    "os.environ[\"MFA_DEVICE_ARN\"] = \"<YOUR_MFA_DEVICE_ARN>\"  # If your organization requires MFA, find this in AWS Console undeer: settings -> security credentials -> your mfa device\n",
    "os.environ[\"AWS_REGION\"] = \"us-east-1\"  # Specify your AWS region\n",
    "\n",
    "# Load environment variables\n",
    "aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "region_name = os.getenv(\"AWS_REGION\", \"us-east-1\")  # Default to 'us-east-1' if not set\n",
    "mfa_serial_number = os.getenv(\"MFA_DEVICE_ARN\")\n",
    "\n",
    "# Ensure required environment variables are set\n",
    "if not all([aws_access_key_id, aws_secret_access_key, mfa_serial_number]):\n",
    "    raise EnvironmentError(\n",
    "        \"Missing required environment variables. Ensure AWS_ACCESS_KEY_ID, \"\n",
    "        \"AWS_SECRET_ACCESS_KEY, and MFA_DEVICE_ARN are set.\"\n",
    "    )\n",
    "\n",
    "# Prompt user for MFA code\n",
    "mfa_token_code = input(\"Enter your MFA code: \")\n",
    "\n",
    "# Create an STS client\n",
    "sts_client = boto3.client(\n",
    "    \"sts\",\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    region_name=region_name,\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Request temporary credentials\n",
    "    response = sts_client.get_session_token(\n",
    "        DurationSeconds=3600 * 24,  # Valid for 24 hours\n",
    "        SerialNumber=mfa_serial_number,\n",
    "        TokenCode=mfa_token_code,\n",
    "    )\n",
    "\n",
    "    # Extract temporary credentials\n",
    "    temp_credentials = response[\"Credentials\"]\n",
    "    temp_access_key = temp_credentials[\"AccessKeyId\"]\n",
    "    temp_secret_key = temp_credentials[\"SecretAccessKey\"]\n",
    "    temp_session_token = temp_credentials[\"SessionToken\"]\n",
    "\n",
    "    # Create the Bedrock Agent Runtime client\n",
    "    client = boto3.client(\n",
    "        \"bedrock-agent-runtime\",\n",
    "        aws_access_key_id=temp_access_key,\n",
    "        aws_secret_access_key=temp_secret_key,\n",
    "        aws_session_token=temp_session_token,\n",
    "        region_name=region_name,\n",
    "    )\n",
    "    print(\"Bedrock client successfully created.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating Bedrock client: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629a30bb-1026-4362-b2ae-c417411d65e0",
   "metadata": {},
   "source": [
    "**Optional: Define helper methods for RAG Retrieval and generation and client creation(following AWS Knowledge Base format)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5b4f878-7797-43f9-a6d3-3bab03b38894",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORE_THRESHOLD = (\n",
    "    0.3  #  Similarity score threshold for retrieving context to use in our RAG app\n",
    ")\n",
    "\n",
    "\n",
    "def retrieve(query, knowledgebase_id, numberOfResults=3):\n",
    "    return BEDROCK_RETRIEVE_CLIENT.retrieve(\n",
    "        retrievalQuery={\"text\": query},\n",
    "        knowledgeBaseId=knowledgebase_id,\n",
    "        retrievalConfiguration={\n",
    "            \"vectorSearchConfiguration\": {\n",
    "                \"numberOfResults\": numberOfResults,\n",
    "                \"overrideSearchType\": \"HYBRID\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def form_prompt(query, context):\n",
    "    return f\"{context}\\n\\nQUESTION:\\n{query}\"\n",
    "\n",
    "def retrieve_and_get_contexts(query, kbId, numberOfResults=3, threshold=0.0):\n",
    "    retrieval_results = retrieve(query, kbId, numberOfResults)\n",
    "    contexts = []\n",
    "\n",
    "    for retrievedResult in retrieval_results[\"retrievalResults\"]:\n",
    "        if retrievedResult[\"score\"] >= threshold:\n",
    "            text = retrievedResult[\"content\"][\"text\"]\n",
    "            if text.startswith(\"Document 1: \"):\n",
    "                text = text[len(\"Document 1: \") :]  # Remove prefix if present\n",
    "            contexts.append(text)\n",
    "\n",
    "    return contexts\n",
    "\n",
    "\n",
    "def generate_text(\n",
    "    messages: list[dict],\n",
    "    model_id: str,\n",
    "    system_messages=[],\n",
    "    bedrock_client=None,\n",
    ") -> list[dict]:\n",
    "    \"\"\"Generates text dynamically handling tool use within Amazon Bedrock.\n",
    "    Params:\n",
    "        messages: List of message history in the desired format.\n",
    "        model_id: Identifier for the Amazon Bedrock model.\n",
    "        tools: List of tools the model can call.\n",
    "        bedrock_client: Client to interact with Bedrock API.\n",
    "    Returns:\n",
    "        messages: Final updated list of messages including tool interactions and responses.\n",
    "    \"\"\"\n",
    "\n",
    "    # Make the initial call to the model\n",
    "    response = bedrock_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        system=system_messages,\n",
    "    )\n",
    "\n",
    "    output_message = response[\"output\"][\"message\"]\n",
    "    stop_reason = response[\"stopReason\"]\n",
    "    messages.append(output_message)\n",
    "    return messages\n",
    "\n",
    "\n",
    "bedrock_config = Config(\n",
    "    connect_timeout=120, read_timeout=120, retries={\"max_attempts\": 0}\n",
    ")\n",
    "\n",
    "BEDROCK_RETRIEVE_CLIENT = boto3.client(\n",
    "    \"bedrock-agent-runtime\",\n",
    "    config=bedrock_config,\n",
    "    aws_access_key_id=temp_access_key,\n",
    "    aws_secret_access_key=temp_secret_key,\n",
    "    aws_session_token=temp_session_token,\n",
    "    region_name=region_name,\n",
    ")\n",
    "\n",
    "BEDROCK_GENERATION_CLIENT = boto3.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    aws_access_key_id=temp_access_key,\n",
    "    aws_secret_access_key=temp_secret_key,\n",
    "    aws_session_token=temp_session_token,\n",
    "    region_name=region_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfa9bb1-0c98-4f1e-bf15-ab4f5474a36f",
   "metadata": {},
   "source": [
    "Once we have defined the basic functionality to set up our RAG components, let's implement a standard RAG app using the AWS Knowledge Bases retrieve and Converse API.\n",
    "Our application will be conversational, supporting multi-turn dialogues. A new dialogue  is instantiated as a `RAGChat` object defined below.\n",
    "To generate a response to each user message in the dialogue, simply call this object's `chat()` method. The `RAGChat` class properly manages conversation history, retrieval, and LLM response generation via the AWS Converse API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17e12f2-93ba-4fe5-8ba9-f7e872e2284f",
   "metadata": {},
   "source": [
    "**Optional: Defining the RAG application, `RAGChat`**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8992b1f-e38e-478d-9224-8eb104d975aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class RAGChat:\n",
    "    def __init__(self, model: str, kbId: str, client, system_message: str):\n",
    "        self.model = model\n",
    "        self.kbId = kbId\n",
    "        self.client = client\n",
    "        self.system_messages = [{\"text\": system_message}]  # Store the system message\n",
    "        self.messages = []  # Initialize messages as an empty list\n",
    "\n",
    "    def chat(self, user_query: str) -> str:\n",
    "        \"\"\"Performs RAG (Retrieval-Augmented Generation) using the provided model and tools.\n",
    "        Params:\n",
    "            user_query: The user's question or query.\n",
    "        Returns:\n",
    "            Final response text generated by the model.\n",
    "        \"\"\"\n",
    "\n",
    "        # Retrieve contexts based on the user query and knowledge base ID\n",
    "        contexts = retrieve_and_get_contexts(\n",
    "            user_query, self.kbId, threshold=SCORE_THRESHOLD\n",
    "        )\n",
    "        context_strings = \"\\n\\n\".join(\n",
    "            [f\"Context {i + 1}: {context}\" for i, context in enumerate(contexts)]\n",
    "        )\n",
    "\n",
    "        # Construct the user message with the retrieved contexts\n",
    "        user_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": form_prompt(user_query, context_strings)}],\n",
    "        }\n",
    "        self.messages.append(user_message)\n",
    "\n",
    "        # Call generate_text with the updated messages\n",
    "        final_messages = generate_text(\n",
    "            messages=self.messages,\n",
    "            model_id=self.model,\n",
    "            system_messages=self.system_messages,\n",
    "            bedrock_client=self.client,\n",
    "        )\n",
    "\n",
    "        # Extract and return the final response text\n",
    "        return final_messages[-1][\"content\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cadc008-b938-4414-9dcc-e3ad3d6eaae0",
   "metadata": {},
   "source": [
    "### Creating a Knowledge Base\n",
    "\n",
    "To keep our example simple, we upload the product description to AWS S3 as a single file: `simple_water_bottle.txt`. This is the sole file our Knowledge Base will contain, but you can populate your actual Knowledge Base with many heterogeneous documents.\n",
    "\n",
    "To create a Knowledge Base using Amazon Bedrock, refer to the [official documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-create.html).\n",
    "\n",
    "After you've created it, add your `KNOWLEDGE_BASE_ID` below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c322f5b2-b559-4c98-9101-921e88de05f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNOWLEDGE_BASE_ID = \"<YOUR-KNOWLEDGE_BASE_ID-HERE>\"  # replace with your own Knowledge Base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed0945a-39e6-4303-a345-2a3763fa76fa",
   "metadata": {},
   "source": [
    "Setting up the application goes as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47e2c77c-a736-4237-bbce-571e7f4d7176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model ARN for aws\n",
    "model_id = \"arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "\n",
    "# Define instructions for AI system\n",
    "fallback_answer = \"Based on the available information, I cannot provide a complete answer to this question.\"\n",
    "system_message = f\"\"\"Do not make up answers to questions if you cannot find the necessary information.\n",
    "If you remain unsure how to accurately respond to the user after considering the available information and tools, then only respond with: \"{fallback_answer}\".\n",
    "\"\"\"\n",
    "\n",
    "# Create RAG application\n",
    "rag = RAGChat(\n",
    "    model=model_id,\n",
    "    kbId=KNOWLEDGE_BASE_ID,\n",
    "    client=BEDROCK_GENERATION_CLIENT,\n",
    "    system_message=system_message,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6a9f62-7e12-4521-9d1a-f3010a3f783b",
   "metadata": {},
   "source": [
    "At this point, you can chat with your RAG application via: `rag.chat(your_query)` as shown below. Before we demonstrate that, let's first see how easy it is to integrate Codex.\n",
    "\n",
    "## Create Codex Project\n",
    "\n",
    "To use Codex, first [create a Project](/codex/web_tutorials/create_project/).\n",
    "Here we assume some common (question, answer) pairs about the *Simple Water Bottle* have already been added to a Codex Project.\n",
    "\n",
    "Our existing Codex Project contains the following entries:\n",
    "\n",
    "![Codex Knowledge Base Example](../assets/codex_kb.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dd64741-133f-46dd-9125-0fcdb9f411ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Programmatically populate the Codex Project with above (question, answer) pairs. Note: The recommended flow is to do this manually in the Web App.\n",
    "from cleanlab_codex.client import Client\n",
    "\n",
    "os.environ[\"CODEX_API_KEY\"] = \"<YOUR-KEY-HERE>\"  # Replace with your Codex API key\n",
    "codex_client = Client()\n",
    "\n",
    "# Create a project\n",
    "project = codex_client.create_project(\n",
    "    name=\"Product FAQs\",\n",
    "    description=\"Questions about product pages\",\n",
    ")\n",
    "\n",
    "# Add entries to the project\n",
    "project.add_entries(\n",
    "    entries=[\n",
    "        {\n",
    "            \"question\": \"How much water can the Simple Water Bottle hold?\",\n",
    "            \"answer\": \"32oz\",\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Can I return my Simple Water Bottle?\",\n",
    "            \"answer\": \"Return it within 30 days for a full refund-- no questions asked. Contact our support team to initiate your return!\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "os.environ[\"CODEX_ACCESS_KEY\"] = project.create_access_key(\"test access key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1fb96b-c11f-4a73-8847-3e10fd512383",
   "metadata": {},
   "source": [
    "## Integrate Codex as-a-Backup\n",
    "\n",
    "RAG apps unfortunately sometimes produce bad/unhelpful responses.\n",
    "Instead of providing these to your users, add Codex as a backup system that can automatically detect these cases and provide better answers.\n",
    "\n",
    "Integrating [Codex as-a-Backup](/codex/concepts/integrations/) just requires two steps:\n",
    "1. Configure the Codex backup system with your Codex Project credentials and settings that control what sort of responses are detected to be bad.\n",
    "2. Enhance your RAG app to:\n",
    "   - Use Codex to monitor whether each RAG response is bad.\n",
    "   - Query Codex for a better answer when the RAG responses is detected to be bad.\n",
    "   - Update the conversation with Codex's answer when it is available.\n",
    "\n",
    "After that, call your enhanced RAG app just like the original app - Codex works automatically in the background.\n",
    "Below is all the code needed to integrate Codex.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc45411d-6ea6-4761-8ec4-f9d81fde1b7a",
   "metadata": {},
   "source": [
    "**Optional: Define the RAG application with Codex as a Backup, `RAGChatWithCodexBackup`**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54ecb141-5e6a-455b-97c6-7c69287e63e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab_codex import Project\n",
    "from typing import Any, Optional\n",
    "from cleanlab_codex import Validator\n",
    "\n",
    "\n",
    "class RAGChatWithCodexBackup(RAGChat):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str,\n",
    "        kbId: str,\n",
    "        client,\n",
    "        system_message: str,\n",
    "        codex_access_key: str,\n",
    "        trustworthy_rag_config: Optional[dict[str, Any]] = None,\n",
    "        bad_response_thresholds: Optional[dict[str, float]] = None,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.kbId = kbId\n",
    "        self.client = client\n",
    "        self.system_messages = [{\"text\": system_message}]  # Store the system message\n",
    "        self.messages = []  # Initialize messages as an empty list\n",
    "        self._validator = Validator(  # Codex class for validating whether RAG responses are bad\n",
    "            codex_access_key=codex_access_key,\n",
    "            trustworthy_rag_config=trustworthy_rag_config,\n",
    "            bad_response_thresholds=bad_response_thresholds,\n",
    "        )\n",
    "        self._codex_project = Project.from_access_key(codex_access_key)\n",
    "\n",
    "    def _replace_latest_message(self, new_message: str) -> None:\n",
    "        \"\"\"\n",
    "        Replaces the latest assistant message in the messages list with the provided content.\n",
    "        It is assumed that the latest assistant message is the last one with role 'assistant'.\n",
    "        \"\"\"\n",
    "        latest_message = self.messages[-1]\n",
    "        latest_message[\"content\"] = [{\"text\": new_message}]\n",
    "\n",
    "    def chat(self, user_message: str) -> str:\n",
    "\n",
    "        # Retrieve contexts based on the user query and knowledge base ID\n",
    "        contexts = retrieve_and_get_contexts(\n",
    "            user_message, self.kbId, threshold=SCORE_THRESHOLD\n",
    "        )\n",
    "        context_strings = \"\\n\\n\".join(\n",
    "            [f\"Context {i + 1}: {context}\" for i, context in enumerate(contexts)]\n",
    "        )\n",
    "\n",
    "        # Construct the user message with the retrieved contexts\n",
    "        user_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": form_prompt(user_message, context_strings)}],\n",
    "        }\n",
    "        self.messages.append(user_message)\n",
    "\n",
    "        # Call generate_text with the updated messages\n",
    "        final_messages = generate_text(\n",
    "            messages=self.messages,\n",
    "            model_id=self.model,\n",
    "            system_messages=self.system_messages,\n",
    "            bedrock_client=self.client,\n",
    "        )\n",
    "\n",
    "        # Extract and return the final response text\n",
    "        response = final_messages[-1][\"content\"][0][\"text\"]\n",
    "\n",
    "        results = self._validator.validate(query=user_message[\"content\"][0][\"text\"], context=context_strings, response=response, form_prompt=form_prompt)\n",
    "        \n",
    "\n",
    "        if results[\"is_bad_response\"] and results[\"expert_answer\"]:\n",
    "            self._replace_latest_message(results[\"expert_answer\"])\n",
    "            response = results[\"expert_answer\"]\n",
    "\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b77e50-a6b1-4209-b2b2-d6ce12e001f7",
   "metadata": {},
   "source": [
    "Codex will be consulted as-a-Backup whenever the response from your RAG app is automatically determined to be untrustworthy or unhelpful.\n",
    "You can optionally configure Codex detection for better latency/accuracy (via `trustworthy_rag_config`) and adjust the score thresholds for flagging bad responses (via `bad_response_thresholds`).  Refer to the Advanced section of our [Validator API tutorial](/codex/tutorials/other_rag_frameworks/validator/#advanced-usage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77d3d667-72aa-4d74-91de-da4365ee8d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CODEX_ACCESS_KEY\"] = \"<YOUR_PROJECT_ACCESS_KEY>\"  # Available from your Project's settings page at: https://codex.cleanlab.ai/\n",
    "\n",
    "# Instantiate RAG app enhanced with Codex as-a-Backup\n",
    "rag_with_codex = RAGChatWithCodexBackup(\n",
    "    model=model_id,\n",
    "    kbId=KNOWLEDGE_BASE_ID,\n",
    "    client=BEDROCK_GENERATION_CLIENT,\n",
    "    system_message=system_message,\n",
    "    codex_access_key=os.environ[\"CODEX_ACCESS_KEY\"],\n",
    "    # Optional: Using standard default configurations\n",
    "    # trustworthy_rag_config=None,   # Default TrustworthyRAG settings\n",
    "    # bad_response_thresholds=None,  # Default detection thresholds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc3f9c6-3c1e-4c81-80c3-c1687b588a42",
   "metadata": {},
   "source": [
    "## RAG with Codex in action\n",
    "\n",
    "We can now ask user queries to our original RAG app (`rag`), as well as another version of this RAG app enhanced with Codex (`rag_with_codex`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f87b86-e391-497f-b6db-d292255d8a1f",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n",
    "Let's ask a question to our **original** RAG app (before Codex was integrated).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a212f27c-24d0-483b-9f6f-8d3a8b4c2dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the available information, I cannot provide a complete answer to this question. The given context does not include any information about return policies for the Simple Water Bottle - Amber. To accurately answer this question, we would need specific details about the company's return policy for this product.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_question = \"Can I return my simple water bottle?\"\n",
    "rag.chat(user_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fab9686-8cd8-4776-9a6d-9a155ca0dba5",
   "metadata": {},
   "source": [
    "The **original RAG app is unable to answer**, in this case because the required information is not in its Knowledge Base.\n",
    "\n",
    "Let's ask the same question to the RAG app enhanced with Codex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e5f84d1-dc29-4f8a-b9b4-9d9556fda7fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return it within 30 days for a full refund-- no questions asked. Contact our support team to initiate your return!'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_with_codex.chat(user_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26577faa-4597-4f5a-97e8-da02c9354d91",
   "metadata": {},
   "source": [
    "As you see, **integrating Codex enables your RAG app to answer questions it originally strugged with**, as long as a similar question was already answered in the corresponding Codex Project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6d91cb-9b75-4096-a40f-7f06bd9f3e22",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "Let's ask another question to our RAG app with Codex integrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68c3bab0-ebfd-4f8d-882b-e2fcf1a916e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the available information, I cannot provide a complete answer to this question. The given context does not include any information about bulk ordering options for the Simple Water Bottle - Amber. To accurately answer this question, we would need specific details about the company's bulk ordering policies or processes for this product.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_question = \"How can I order the Simple Water Bottle in bulk?\"\n",
    "rag.chat(user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e244e3a-a1f2-46a4-84f7-f0de4ea87e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the available information, I cannot provide a complete answer to this question.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_with_codex.chat(user_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff41bb79-1ad6-44f8-af77-464a9c259bf2",
   "metadata": {},
   "source": [
    "Our RAG app is unable to answer this question because there is no relevant information in its Knowledge Base, nor has a similar question been answered in the Codex Project (see the contents of the Codex Project above).\n",
    "\n",
    "**Codex automatically detects that this question could not be properly answered and logs it into the Project where it awaits an answer from a SME.**\n",
    "Navigate to your Codex Project in the [Web App](https://codex.cleanlab.ai/) where you (or a SME at your company) can enter the desired answer for this query.\n",
    "\n",
    "As soon as an answer is provided in Codex, our RAG app will be able to answer all similar questions going forward (as seen for the previous query)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febd317a-cbbc-4126-bab1-6c28e7822ee7",
   "metadata": {},
   "source": [
    "### Example 3\n",
    "\n",
    "Let's ask another query to our two RAG apps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "709de6b1-7628-48c4-8f68-80d91d5d10a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the provided information, the dimensions of the Simple Water Bottle - Amber are:\\n\\n10 inches in height\\n4 inches in width\\n\\nThese measurements give you an idea of the size of the water bottle.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_question = \"How big is the water bottle?\"\n",
    "rag.chat(user_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb316c1-0461-4f62-a0b3-cc6ffe32f2ac",
   "metadata": {},
   "source": [
    "The original RAG app was able to correctly answer without Codex (since the relevant information exists in the Knowledge Base)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74d817fa-9fba-4f6c-a4fd-2a84846df84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the provided information, the dimensions of the Simple Water Bottle - Amber are:\\n\\n10 inches in height\\n4 inches in width\\n\\nThese measurements give you an idea of the size of the water bottle.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_with_codex.chat(user_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5112ea44-322b-485c-9266-19418a8f73f1",
   "metadata": {},
   "source": [
    "We see that the RAG app with Codex integrated is still able to correctly answer this query. **Integrating Codex has no negative effect on questions your original RAG app could properly answer.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d738e720-7b17-4687-a801-dffdde75554d",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that Codex is integrated with your RAG app, you and SMEs can [open the Codex Project and answer questions](/codex/web_tutorials/codex_as_sme/) logged there to continuously improve your AI.\n",
    "\n",
    "This tutorial demonstrated how to easily integrate Codex as a backup system into any AWS Knowledge Bases application. **Unlike tool calls which are harder to control**, you can choose when to call Codex as a backup. For instance, you can use Codex to automatically detect whenever the RAG system produces hallucinations or unhelpful responses such as \"I don't know\".\n",
    "\n",
    "**Adding Codex only improves your RAG app.** Once integrated, Codex automatically logs all user queries that your original RAG app handles poorly. Using a [simple web interface](/codex/web_tutorials/codex_as_sme/), SMEs at your company can answer the highest priority questions in the Codex Project. As soon as an answer is entered in Codex, your RAG app will be able to properly handle all similar questions encountered in the future.\n",
    "\n",
    "Codex is **the fastest way for nontechnical SMEs to directly improve your AI Application**. As the Developer, you simply integrate Codex once, and from then on, SMEs can continuously improve how your RAG system handles common user queries without needing your help.\n",
    "\n",
    "Need help, more capabilities, or other deployment options?  \n",
    "Check the [FAQ](/codex/FAQ/) or email us at: support@cleanlab.ai"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}