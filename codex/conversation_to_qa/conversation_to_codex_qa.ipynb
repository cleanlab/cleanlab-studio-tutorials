{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c710a0a2",
   "metadata": {},
   "source": [
    "# Leverage Historical Support Tickets to Improve your AI\n",
    "\n",
    " This tutorial walks you through the process of transforming historical customer support conversations into high-quality question-answer (QA) pairs that can be ingested into Codex and served as expert answers for similar future queries. After adding your historical support tickets into Codex, your AI assistant will be able to accurately/helpfully answer significantly more queries from your customers.\n",
    "\n",
    "We assume you have a database of historical customer support tickets, in which your human support employees previously provided high-quality assistance to customers via a recorded chat.\n",
    "\n",
    "Alternatively if your documentation has a FAQ section, you can also follow this tutorial to ingest it into Codex expert answers. This enables FAQs to be served to your users more precisely/accurately (in particular, overcoming issues in your current AI system perhaps stemming from data formatting, document chunking, suboptimal retrieval, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefd8438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from cleanlab_codex.client import Client\n",
    "import shutil\n",
    "\n",
    "os.environ[\"CODEX_API_KEY\"] = \"<CODEX_API_KEY>\" # Get your free API key from: https://codex.cleanlab.ai/\n",
    "os.environ[\"CLEANLAB_TLM_API_KEY\"] = \"<TLM_API_KEY>\" # Get your free API key from: https://tlm.cleanlab.ai/\n",
    "\n",
    "\n",
    "codex_client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6143120",
   "metadata": {},
   "source": [
    "# Form Question-Answer Pairs from Message Histories\n",
    "\n",
    "We'll first transform raw customer support conversations into structured question-answer (QA) pairs. Later we'll filter those QA pairs to remove answers that are low-quality or likely outdated.\n",
    "\n",
    "**Steps to follow:**\n",
    "\n",
    "1. **Pre-process your data:** Ensuring it is organized as conversations, where each conversation is a sequence of User and Assistant messages.\n",
    "2. **Deduplicate conversations:** To save subsequent processing time.\n",
    "3. **Extract QA pairs:** From each conversation, extract the user's primary initial question and the corresponding assistant answer.\n",
    "4. **Deduplicate queries:** To save subsequent processing time.\n",
    "\n",
    "In this tutorial, we'll demonstrate this process using a subset of the [ABCD dataset](https://github.com/asappresearch/abcd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c9fb361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>messages</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Assistant: Thanks for contacting AcmeBrands, h...</td>\n",
       "      <td>Background Information:\\n The conversation sta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            messages  \\\n",
       "0  Assistant: Thanks for contacting AcmeBrands, h...   \n",
       "\n",
       "                                            metadata  \n",
       "0  Background Information:\\n The conversation sta...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"https://cleanlab-public.s3.us-east-1.amazonaws.com/Datasets/abcd_subsampled.csv\")\n",
    "\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9846b873",
   "metadata": {},
   "source": [
    "Let's look at the formatted data. Below we have a conversation history between a User (aka *customer*) and an Assistant (aka your *human customer service representative*, whose answers we can trust -- not an AI model!). We expect the history to be in `messages` list format with each conversation turn being indicated by `role`:`content` and being on a new line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8729f1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Hello, how can i help you?\n",
      "User: hi I have a general question about returns, What is your policy if I buy something and want to return it. I'm just using a guest Membership now.\n",
      "User: I'm curious about what different membership levels might afford me regarding returns, what's best?\n",
      "Assistant: Of course, id be happy to tell you.\n",
      "Assistant: The membership that allow the most for returns is the gold membership. With it you are allowed unlimited returns.\n",
      "User: Ok, what about with just my guest membership now?\n",
      "Assistant: With guest you are only allowed to return within the last 30 days of purchase.\n",
      "User: ok, Great, that's a lot to consider and I appreciate the information. Goodbye.\n",
      "Assistant: Do you need anything else?\n",
      "User: no, I just wondered if I got the wrong item what happens, you answered it. thanks.\n",
      "Assistant: Have a great day.\n"
     ]
    }
   ],
   "source": [
    "idx = 72\n",
    "print(df.iloc[idx]['messages'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacd783b",
   "metadata": {},
   "source": [
    "### Deduplicate message histories to ensure uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "263dd238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial size (100, 2)\n",
      "final size after deduplication: (100, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"initial size\", df.shape)\n",
    "df = df.drop_duplicates(subset=[\"messages\"])\n",
    "print(\"final size after deduplication:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ef36d3",
   "metadata": {},
   "source": [
    "### (Optional) Add Background Information into Messages\n",
    "\n",
    "Optionally, you can prepend any background information (metadata) that is relevant to each conversation (date, user info, etc).\n",
    "This helps ensure that Cleanlab's AI receives all relevant context about this conversation, when it is processing it to extract expert answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1addddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Background Information:\n",
      " The conversation started on 2025-06-17 and the user\u2019s account type is basic.\n",
      "\n",
      "Assistant: Hello, how can i help you?\n",
      "User: hi I have a general question about returns, What is your policy if I buy something and want to return it. I'm just using a guest Membership now.\n",
      "User: I'm curious about what different membership levels might afford me regarding returns, what's best?\n",
      "Assistant: Of course, id be happy to tell you.\n",
      "Assistant: The membership that allow the most for returns is the gold membership. With it you are allowed unlimited returns.\n",
      "User: Ok, what about with just my guest membership now?\n",
      "Assistant: With guest you are only allowed to return within the last 30 days of purchase.\n",
      "User: ok, Great, that's a lot to consider and I appreciate the information. Goodbye.\n",
      "Assistant: Do you need anything else?\n",
      "User: no, I just wondered if I got the wrong item what happens, you answered it. thanks.\n",
      "Assistant: Have a great day.\n"
     ]
    }
   ],
   "source": [
    "df['messages'] = df['metadata']+'\\n' + df['messages']\n",
    "print(df['messages'][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1785f013",
   "metadata": {},
   "source": [
    "### Extract initial Question and Answer from message history\n",
    "\n",
    "We'll use Cleanlab's TLM to extract one structured question and answer pair from each conversation.\n",
    "Cleanlab will analyze the conversation (including any metadata), and identify the user's primary initial question and the assistant's corresponding answer to this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c2c47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab_tlm import TLM\n",
    "\n",
    "# Optional configurations to control runtimes/costs:\n",
    "TLM_QUALITY_PRESET = \"base\"\n",
    "tlm = TLM(quality_preset=TLM_QUALITY_PRESET, options={\"model\": \"gpt-4.1-nano\"})\n",
    "\n",
    "SAVE_DIR = \"scores_run\" # Temp directory to save scores\n",
    "\n",
    "if os.path.exists(SAVE_DIR):\n",
    "    print(f\"Warning: Directory '{SAVE_DIR}' already exists. Existing filenames can cause issues if not removed.\")\n",
    "    response = input(f\"Do you want to remove the directory '{SAVE_DIR}'? (y/n): \").strip().lower()\n",
    "    if response == 'y':\n",
    "        print(f\"Removing existing directory: {SAVE_DIR}\")\n",
    "        shutil.rmtree(SAVE_DIR)\n",
    "    else:\n",
    "        print(f\"Keeping existing directory: {SAVE_DIR}. Be aware that existing files may cause issues.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e06066-78d2-4490-b16d-e408668cdab7",
   "metadata": {},
   "source": [
    "**Optional: Helper functions for extracting query-answer\u00a0pair from conversation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5cbd2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from typing import Tuple, Optional\n",
    "from typing import Tuple, Optional, Dict, List, Any\n",
    "import json\n",
    "\n",
    "\n",
    "TLM prompt use for QA pair extraction:\n",
    "PARSE_QUERY =  \"\"\"Analyze the following Conversation, and try to identify: overall what is the primary initial Query asked by the User (based on messages from the User) and what was a complete Answer to this Query provided by the assistant (based on messages from the assistant). \n",
    "\n",
    "<conversation> \n",
    "{message_history}\n",
    "</conversation>\n",
    "\n",
    "\n",
    "# Instructions\n",
    "\n",
    "First, determine what overall is the primary initial Query asked by the User in this Conversation.\n",
    "\n",
    "Next, determine a complete and accurate Answer to this self-contained Query, based on the information provided by the assistant in the Conversation.\n",
    "\n",
    "If a Topic is specified in the Conversation, make sure it is mentioned/obvious in the Query you write.\n",
    "\n",
    "When writing the Query:\n",
    "- Ensure that it self-contained and answerable without seeing the full Conversation.\n",
    "- If the User raised multiple concerns in their messages, focus the Query on summarizing their initial concern into a self-contained question.\n",
    "\n",
    "When writing the Answer:\n",
    "- Ensure that it is accurate, and would be helpful to other Users who had the same Query.\n",
    "- Only rely on information from the assistant's responses that directly address the Query!\n",
    "- Do not include any non-core information that is not necessary to answer the Query.\n",
    "- Omit any conversational/phatic statements and apologies from the assistant.\n",
    "- Anticipate follow-up questions that the User raised and the assistant answered and try to include them in your Answer if other Users will likely have the same follow-ups.\n",
    "- If the assistant did not provide a complete and helpful answer to the Query, write the Answer as an empty string (\"\").\n",
    "- Do not output an empty string Answer if the assistant's primary helpful response was a URL, instead your Answer should include the URL.\n",
    "- If the person is asking to speak with somebody, but the assistant said nobody is available, then your Answer should be the next alternative suggested by the assistant.\n",
    "\n",
    "Your output must strictly follow this plain text format:\n",
    "\n",
    "query: [self-contained Query that summarizes the User's main initial request]  \n",
    "answer: [complete self-contained Answer to this Query]\"\"\"\n",
    "\n",
    "\n",
    "PARSE_QUERY_TRUSTWORTHINESS_THRESHOLD = 0.8  # higher values will extract less Question-Answer pairs from conversations, we only keep those whose trust score exceeds this threshold.\n",
    "\n",
    "def batch_prompt(\n",
    "    tlm: TLM,\n",
    "    input_path: str,\n",
    "    output_path: str,\n",
    "    prompt_col_name: str,\n",
    "    answer_col_name: Optional[str] = None,\n",
    "    batch_size: int = 1000,\n",
    "    constrain_outputs: Optional[List[str]] = None,\n",
    "):\n",
    "    if os.path.exists(output_path):\n",
    "        start_idx = len(pd.read_csv(output_path))\n",
    "    else:\n",
    "        start_idx = 0\n",
    "\n",
    "    df_batched = pd.read_csv(input_path, chunksize=batch_size)\n",
    "    curr_idx = 0\n",
    "\n",
    "    for curr_batch in df_batched:\n",
    "        if curr_idx + len(curr_batch) <= start_idx:\n",
    "            curr_idx += len(curr_batch)\n",
    "            continue\n",
    "        elif curr_idx < start_idx:\n",
    "            curr_batch = curr_batch[start_idx - curr_idx :]\n",
    "            curr_idx = start_idx\n",
    "\n",
    "        if answer_col_name:\n",
    "            prompts = curr_batch[prompt_col_name].to_list()\n",
    "            answers = curr_batch[answer_col_name].to_list()\n",
    "            results = tlm.get_trustworthiness_score(prompts, answers)\n",
    "        else:\n",
    "            prompts = curr_batch[prompt_col_name].to_list()\n",
    "            results = tlm.prompt(prompts, constrain_outputs=constrain_outputs)\n",
    "\n",
    "        results_df = pd.DataFrame(results)\n",
    "        if \"log\" in results_df.columns:\n",
    "            results_df[\"log\"] = results_df[\"log\"].apply(json.dumps)\n",
    "        results_df.to_csv(\n",
    "            output_path, mode=\"a\", index=False, header=not os.path.exists(output_path)\n",
    "        )\n",
    "        curr_idx += len(curr_batch)\n",
    "\n",
    "        \n",
    "def extract_tags(text: str | None) -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Extracts 'query:' and 'answer:' values from plain text format.\n",
    "    Returns a tuple of (query, answer) or (None, None) if not found.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return None, None\n",
    "    \n",
    "    query_match = re.search(r\"query:\\s*(.*)\", text)\n",
    "    answer_match = re.search(r\"answer:\\s*(.*)\", text)\n",
    "\n",
    "    query = query_match.group(1).strip() if query_match else None\n",
    "    answer = answer_match.group(1).strip() if answer_match else None\n",
    "\n",
    "    return query, answer\n",
    "\n",
    "\n",
    "def get_parse_prompts(messages: list[list[str]]) -> list[str]:\n",
    "    \"\"\"Formats message histories into prompts for parsing.\"\"\"\n",
    "    return [\n",
    "        PARSE_QUERY.format(message_history=history)\n",
    "        for history in messages\n",
    "    ]\n",
    "\n",
    "def prompt_tlm_to_parse_message_history(\n",
    "    tlm, \n",
    "    prompts: list[str], \n",
    "    save_dir: str\n",
    ") -> list[str | None]:\n",
    "    \"\"\"Uses batch_prompt and returns parsed responses if they meet the trustworthiness threshold.\n",
    "    If no response meets the threshold, returns None.\"\"\"\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    input_path = os.path.join(save_dir, \"input_extract_qa.csv\")\n",
    "    output_path = os.path.join(save_dir, \"results.csv\")\n",
    "\n",
    "    pd.DataFrame({\"prompt\": prompts}).to_csv(input_path, index=False)\n",
    "\n",
    "    batch_prompt(\n",
    "        tlm=tlm,\n",
    "        input_path=input_path,\n",
    "        output_path=output_path,\n",
    "        prompt_col_name=\"prompt\",\n",
    "        batch_size=1000,\n",
    "    )\n",
    "\n",
    "    df = pd.read_csv(output_path)\n",
    "    parsed_messages_list = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        score = row.get(\"trustworthiness_score\")\n",
    "        if pd.notnull(score) and score >= PARSE_QUERY_TRUSTWORTHINESS_THRESHOLD:\n",
    "            parsed_messages_list.append(row.get(\"response\", None))\n",
    "        else:\n",
    "            parsed_messages_list.append(None)\n",
    "\n",
    "    return parsed_messages_list\n",
    "\n",
    "\n",
    "def parse_all_messages(tlm, messages: list[list[str]], save_dir: str = '/tmp/') -> list[str | None]:\n",
    "    \"\"\"Parses message history returning a list of (initial) queries and answers or None if parsing failed.\"\"\"\n",
    "    prompts = get_parse_prompts(messages)\n",
    "    parsed_messages_list = prompt_tlm_to_parse_message_history(tlm, prompts, save_dir)\n",
    "    parsed_queries_and_answers = [extract_tags(r) for r in parsed_messages_list]\n",
    "    return parsed_queries_and_answers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96067d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If this progress bar appears frozen, TLM is still processing your dataset so just continue waiting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Querying TLM... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|\n"
     ]
    }
   ],
   "source": [
    "df[[\"query\", \"answer\"]] = parse_all_messages(tlm, df['messages'].tolist(), save_dir=SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e49884d",
   "metadata": {},
   "source": [
    "### Deduplicate QA pairs so all queries are unique and have answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01fa80da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial size (100, 4)\n",
      "size after removing rows with NaN queries or answers: (100, 4)\n",
      "final size after deduplication: (99, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"initial size\", df.shape)\n",
    "df = df[\n",
    "    df[\"query\"].notna() & df[\"answer\"].notna() &  # remove NaN\n",
    "    df[\"query\"].astype(str).str.strip().ne(\"\") &  # remove empty/blank strings\n",
    "    df[\"answer\"].astype(str).str.strip().ne(\"\")\n",
    "].reset_index(drop=True)\n",
    "print(\"size after removing rows with NaN queries or answers:\", df.shape)\n",
    "df = df.drop_duplicates(subset=[\"query\"])\n",
    "print(\"final size after deduplication:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f884962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.to_csv(\"question_answer_pairs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb60d4a",
   "metadata": {},
   "source": [
    "## Filter out bad Question-Answer pairs\n",
    "\n",
    "Let's load the Question-Answer pairs extracted above. The data file should include the following components:\n",
    "\n",
    "- **Query**: The query made by the user.\n",
    "- **Answer**: The expert answer provided in response to the user's query.\n",
    "- **Message History (Optional)**: A record of the conversation history, which may provide additional context.\n",
    "- **Custom Metadata (Optional)**: Any additional information that may be relevant to the data, such as timestamps or user info.\n",
    "\n",
    "**Note:** If your original customer support tickets are single-turn Q&A rather than multi-turn chats, or you are instead ingesting a documentation FAQ, then you can ignore the conversation-processing code above and start at this part of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "241d5e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>messages</th>\n",
       "      <th>metadata</th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Background Information:\\n The conversation sta...</td>\n",
       "      <td>Background Information:\\n The conversation sta...</td>\n",
       "      <td>How can I regain access to my account after lo...</td>\n",
       "      <td>To regain access to your account after losing ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            messages  \\\n",
       "0  Background Information:\\n The conversation sta...   \n",
       "\n",
       "                                            metadata  \\\n",
       "0  Background Information:\\n The conversation sta...   \n",
       "\n",
       "                                               query  \\\n",
       "0  How can I regain access to my account after lo...   \n",
       "\n",
       "                                              answer  \n",
       "0  To regain access to your account after losing ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"question_answer_pairs.csv\")\n",
    "\n",
    "\n",
    "print(len(df))\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a600adb5",
   "metadata": {},
   "source": [
    "### Define Filters\n",
    "\n",
    "We apply a sequence of filters to remove low-quality or outdated question-answer pairs before we ingest this data into Codex. Different filters target specific issues like PII exposure, non-questions, outdated or non-informative content.\n",
    "You can easily add your own filters here too!\n",
    "\n",
    "**Note:** All TLM-based filters have a configurable `trustworthiness_threshold` which determine how many examples meet the filter criteria (how stringent the filter is). All examples where the filter's TLM trustworthiness score falls below the `trustworthiness_threshold` are removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06113208-919c-4e54-ab30-2c9857a68b3a",
   "metadata": {},
   "source": [
    "**Optional: Helper methods for defining and running filters**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf4a43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import string\n",
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def print_idx(row, columns):\n",
    "    \"\"\"\n",
    "    Print the index and values of the specified columns from a pandas Series or namedtuple row.\n",
    "\n",
    "    Args:\n",
    "        row: A pandas Series or namedtuple representing a row from a DataFrame.\n",
    "        columns: List of column names (for Series) or attribute names (for namedtuple) to print.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        value = (\n",
    "            row[col]\n",
    "            if isinstance(row, dict) or isinstance(row, pd.Series)\n",
    "            else getattr(row, col, None)\n",
    "        )\n",
    "        print(f\"{col}: {value}\")\n",
    "\n",
    "\n",
    "def display_filter_results(df, concern, n=20, columns=None, ascending=True):\n",
    "    \"\"\"\n",
    "    Print the top n rows sorted by the given concern column, showing specified columns and the concern.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame to print from.\n",
    "        concern: The column name to sort by and display.\n",
    "        n: Number of rows to print (default 20).\n",
    "        columns: List of columns to display (default ['query', 'answer']).\n",
    "        ascending: Whether to sort in ascending order (default True).\n",
    "    \"\"\"\n",
    "    if columns is None:\n",
    "        columns = [\"query\", \"answer\"]\n",
    "    display_columns = columns + [concern]  # Make a new list every call\n",
    "\n",
    "    for i, row in df.sort_values(by=concern, ascending=ascending).head(n).iterrows():\n",
    "        print_idx(row, display_columns)\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "\n",
    "class BaseFilter:\n",
    "    def __init__(\n",
    "        self, name: str, cost: str = \"low\", hyperparameters: Optional[Dict] = None\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.cost = cost  # 'low', 'med', or 'high'\n",
    "        self.hyperparameters = hyperparameters or {}\n",
    "\n",
    "    def apply(self, df: pd.DataFrame) -> Tuple[Optional[np.ndarray], np.ndarray]:\n",
    "        \"\"\"\n",
    "        Apply filter to dataframe.\n",
    "        Returns:\n",
    "            scores (np.ndarray or None): A float score per row (or None if not applicable)\n",
    "            keep_mask (np.ndarray): A boolean array indicating which rows to keep\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class KeywordFilter(BaseFilter):\n",
    "    def __init__(self, keywords: list[str], strip_punctuation: bool = False, **kwargs):\n",
    "        super().__init__(name=\"KeywordFilter\", cost=\"low\", **kwargs)\n",
    "        self.keywords = set(k.lower() for k in keywords)\n",
    "        self.strip_punctuation = strip_punctuation\n",
    "\n",
    "    def apply(self, df: pd.DataFrame) -> Tuple[None, np.ndarray]:\n",
    "        def tokenize(text: str) -> set[str]:\n",
    "            if self.strip_punctuation:\n",
    "                text = text.translate(\n",
    "                    str.maketrans({p: \" \" for p in string.punctuation})\n",
    "                )\n",
    "            return set(word.lower() for word in text.split())\n",
    "\n",
    "        keep_mask = (\n",
    "            df[\"answer\"]\n",
    "            .fillna(\"\")\n",
    "            .apply(lambda x: not any(k in tokenize(x) for k in self.keywords))\n",
    "            .to_numpy()\n",
    "        )\n",
    "        return None, keep_mask\n",
    "\n",
    "\n",
    "class ExactMatchFilter(BaseFilter):\n",
    "    def __init__(self, exact_answers: list[str], ignore_case: bool = False, **kwargs):\n",
    "        super().__init__(name=\"ExactMatchFilter\", cost=\"low\", **kwargs)\n",
    "        self.ignore_case = ignore_case\n",
    "        if self.ignore_case:\n",
    "            self.exact_answers = set(ans.lower() for ans in exact_answers)\n",
    "        else:\n",
    "            self.exact_answers = set(exact_answers)\n",
    "\n",
    "    def apply(self, df: pd.DataFrame) -> Tuple[None, np.ndarray]:\n",
    "        if self.ignore_case:\n",
    "            answers = df[\"answer\"].fillna(\"\").str.lower()\n",
    "            keep_mask = ~answers.isin(self.exact_answers)\n",
    "        else:\n",
    "            keep_mask = ~df[\"answer\"].isin(self.exact_answers)\n",
    "        return None, keep_mask.to_numpy()\n",
    "\n",
    "\n",
    "class LengthFilter(BaseFilter):\n",
    "    def __init__(self, min_word_count: int = 3, **kwargs):\n",
    "        super().__init__(name=\"LengthFilter\", cost=\"low\", **kwargs)\n",
    "        self.min_word_count = min_word_count\n",
    "\n",
    "    def apply(self, df: pd.DataFrame) -> Tuple[None, np.ndarray]:\n",
    "        word_count = df[\"answer\"].fillna(\"\").str.split().str.len()\n",
    "        keep_mask = word_count >= self.min_word_count\n",
    "        return None, keep_mask.to_numpy()\n",
    "\n",
    "\n",
    "class TLMBinaryFilter(BaseFilter):\n",
    "    def __init__(\n",
    "        self,\n",
    "        prompt_template: str,\n",
    "        name: str = \"TLMBinaryFilter\",\n",
    "        tlm_api_key: Optional[str] = None,\n",
    "        trustworthiness_threshold: float = 0.5,\n",
    "        tlm_kwargs: dict[str, Any] = None,\n",
    "        batch_size: int = 1000,\n",
    "        save_dir: str = \"/tmp\",\n",
    "    ):\n",
    "        if tlm_api_key is None:\n",
    "            tlm_api_key = os.getenv(\"CLEANLAB_TLM_API_KEY\")\n",
    "            if not tlm_api_key:\n",
    "                raise ValueError(\"TLM API key must be provided.\")\n",
    "        super().__init__(name=name, cost=\"high\")\n",
    "\n",
    "        self.prompt_template = prompt_template\n",
    "        self.tlm_client = TLM(api_key=tlm_api_key, **(tlm_kwargs or {}))\n",
    "        self.trustworthiness_threshold = trustworthiness_threshold\n",
    "        self.batch_size = batch_size\n",
    "        self.name = name\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        formatter = string.Formatter()\n",
    "        self.template_fields = {\n",
    "            fname for _, fname, _, _ in formatter.parse(prompt_template) if fname\n",
    "        }\n",
    "\n",
    "    def get_responses(self, df: pd.DataFrame) -> list:\n",
    "        missing_fields = self.template_fields - set(df.columns)\n",
    "        if missing_fields:\n",
    "            raise ValueError(f\"Missing fields in DataFrame: {missing_fields}\")\n",
    "\n",
    "        df_copy = df.copy()\n",
    "        df_copy[\"__prompt__\"] = df_copy.apply(\n",
    "            lambda row: self.prompt_template.format(\n",
    "                **{f: row[f] for f in self.template_fields}\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        temp_input_path = os.path.join(self.save_dir, f\"tlm_binary_{self.name}_input.csv\")\n",
    "        temp_output_path = os.path.join(self.save_dir, f\"tlm_binary_{self.name}_output.csv\")\n",
    "        df_copy[[\"__prompt__\"]].to_csv(temp_input_path, index=False)\n",
    "\n",
    "        batch_prompt(\n",
    "            tlm=self.tlm_client,\n",
    "            input_path=temp_input_path,\n",
    "            output_path=temp_output_path,\n",
    "            prompt_col_name=\"__prompt__\",\n",
    "            batch_size=self.batch_size,\n",
    "            constrain_outputs=[\"Yes\", \"No\"],\n",
    "            **(self.tlm_client.kwargs if hasattr(self.tlm_client, \"kwargs\") else {}),\n",
    "        )\n",
    "        return pd.read_csv(temp_output_path).to_dict(orient=\"records\")\n",
    "\n",
    "    def apply(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        responses = self.get_responses(df)\n",
    "        scores = []\n",
    "        for r in responses:\n",
    "            if isinstance(r, dict) and r.get(\"response\", \"\").strip().lower() == \"yes\":\n",
    "                score = r.get(\"trustworthiness_score\", 0.0)\n",
    "            else:\n",
    "                score = 0.0\n",
    "            scores.append(score)\n",
    "        scores = np.array(scores)\n",
    "        keep_mask = scores >= self.trustworthiness_threshold\n",
    "        return scores, keep_mask\n",
    "\n",
    "\n",
    "class TLMCustomEvalFilter(BaseFilter):\n",
    "    def __init__(\n",
    "        self,\n",
    "        criteria_name: str,\n",
    "        criteria_instruction: str,\n",
    "        tlm_api_key: Optional[str] = None,\n",
    "        trustworthiness_threshold: float = 0.5,\n",
    "        tlm_kwargs: dict[str, Any] = None,\n",
    "        batch_size: int = 1000,\n",
    "        save_dir: str = \"/tmp\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if tlm_api_key is None:\n",
    "            tlm_api_key = os.getenv(\"CLEANLAB_TLM_API_KEY\")\n",
    "            if not tlm_api_key:\n",
    "                raise ValueError(\n",
    "                    \"TLM API key must be provided either as an argument or through the environment variable CLEANLAB_TLM_API_KEY.\"\n",
    "                )\n",
    "\n",
    "        if tlm_kwargs is None:\n",
    "            tlm_kwargs = {}\n",
    "\n",
    "        if \"custome_eval_criteria\" in tlm_kwargs.get(\"options\", {}):\n",
    "            raise ValueError(\n",
    "                \"The 'custome_eval_criteria' option is already set in the TLM options. Please define the eval through *criteria_name* and *criteria_instruction* parameters in TLMCustomEvalFilter.\"\n",
    "            )\n",
    "\n",
    "        name = f\"TLMCustomEval_{criteria_name}\"\n",
    "        super().__init__(name=name, cost=\"high\", **kwargs)\n",
    "\n",
    "        tlm_kwargs = copy.deepcopy(tlm_kwargs) if tlm_kwargs else {}\n",
    "        opts = dict(tlm_kwargs.pop(\"options\", {}))\n",
    "\n",
    "        opts[\"custom_eval_criteria\"] = [\n",
    "            {\"name\": criteria_name, \"criteria\": criteria_instruction}\n",
    "        ]\n",
    "\n",
    "        tlm_kwargs[\"options\"] = opts\n",
    "        self.tlm_client = TLM(api_key=tlm_api_key, **tlm_kwargs)\n",
    "        self.trustworthiness_threshold = trustworthiness_threshold\n",
    "        self.batch_size = batch_size\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "    def apply(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "\n",
    "        input_path = os.path.join(self.save_dir, f\"{self.name}_input.csv\")\n",
    "        output_path = os.path.join(self.save_dir, f\"{self.name}_output.csv\")\n",
    "\n",
    "        df[[\"query\", \"answer\"]].to_csv(input_path, index=False)\n",
    "\n",
    "        if os.path.exists(output_path):\n",
    "            os.remove(output_path)\n",
    "\n",
    "        batch_prompt(\n",
    "            tlm=self.tlm_client,\n",
    "            input_path=input_path,\n",
    "            output_path=output_path,\n",
    "            prompt_col_name=\"query\",\n",
    "            answer_col_name=\"answer\",\n",
    "            batch_size=self.batch_size,\n",
    "            constrain_outputs=None,\n",
    "        )\n",
    "        output_df = pd.read_csv(output_path)\n",
    "        output_df[\"log\"] = output_df[\"log\"].apply(json.loads)\n",
    "        custom_eval_scores = (\n",
    "            output_df[\"log\"]\n",
    "            .apply(lambda x: x[\"custom_eval_criteria\"][0][\"score\"])\n",
    "            .values\n",
    "        )\n",
    "\n",
    "        keep_mask = custom_eval_scores >= self.trustworthiness_threshold\n",
    "        return custom_eval_scores, keep_mask\n",
    "\n",
    "\n",
    "class TLMOutdatedAnswerFilter(BaseFilter):\n",
    "    def __init__(\n",
    "        self,\n",
    "        prompt_template: str,\n",
    "        name: str = \"TLMOutdatedAnswerFilter\",\n",
    "        tlm_api_key: Optional[str] = None,\n",
    "        trustworthiness_threshold: float = 0.5,\n",
    "        tlm_kwargs: dict[str, Any] = None,\n",
    "        batch_size: int = 1000,\n",
    "        save_dir: str = \"/tmp\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if tlm_api_key is None:\n",
    "            tlm_api_key = os.getenv(\"CLEANLAB_TLM_API_KEY\")\n",
    "            if not tlm_api_key:\n",
    "                raise ValueError(\n",
    "                    \"TLM API key must be provided either as an argument or through the environment variable CLEANLAB_TLM_API_KEY.\"\n",
    "                )\n",
    "\n",
    "        super().__init__(name=name, cost=\"high\", **kwargs)\n",
    "        self.tlm_client = TLM(api_key=tlm_api_key, **(tlm_kwargs or {}))\n",
    "        self.trustworthiness_threshold = trustworthiness_threshold\n",
    "        self.batch_size = batch_size\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        if not prompt_template or not isinstance(prompt_template, str):\n",
    "            raise ValueError(\"prompt_template must be a non-empty string.\")\n",
    "        self.prompt_template = prompt_template\n",
    "\n",
    "    def apply(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        if not all(col in df.columns for col in [\"query\", \"answer\", \"metadata\"]):\n",
    "            raise ValueError(\n",
    "                \"DataFrame must contain 'query', 'answer', and 'metadata' columns.\"\n",
    "            )\n",
    "\n",
    "        today_str = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # Create prompt from template and save to input CSV\n",
    "        prompts = [\n",
    "            self.prompt_template.format(\n",
    "                today_str=today_str,\n",
    "                query=row[\"query\"],\n",
    "                answer=row[\"answer\"],\n",
    "                meta_data=row[\"metadata\"],\n",
    "            )\n",
    "            for _, row in df.iterrows()\n",
    "        ]\n",
    "        input_df = pd.DataFrame({\"prompt\": prompts})\n",
    "        input_path = os.path.join(self.save_dir, f\"{self.name}_input.csv\")\n",
    "        output_path = os.path.join(self.save_dir, f\"{self.name}_output.csv\")\n",
    "\n",
    "        input_df.to_csv(input_path, index=False)\n",
    "\n",
    "        if os.path.exists(output_path):\n",
    "            os.remove(output_path)\n",
    "\n",
    "        batch_prompt(\n",
    "            tlm=self.tlm_client,\n",
    "            input_path=input_path,\n",
    "            output_path=output_path,\n",
    "            prompt_col_name=\"prompt\",\n",
    "            batch_size=self.batch_size,\n",
    "            constrain_outputs=[\"Yes\", \"No\"],\n",
    "        )\n",
    "\n",
    "        output_df = pd.read_csv(output_path)\n",
    "\n",
    "        \n",
    "        scores = np.array([\n",
    "            row[\"trustworthiness_score\"] if str(row[\"response\"]).strip().lower() == \"yes\" else 0.0\n",
    "            for _, row in output_df.iterrows()\n",
    "        ])\n",
    "\n",
    "\n",
    "        keep_mask = ~(scores > self.trustworthiness_threshold)\n",
    "        return scores, keep_mask\n",
    "\n",
    "\n",
    "\n",
    "class RunFilters:\n",
    "    def __init__(self, filters: List[BaseFilter], save_dir: str = None):\n",
    "        # Sort filters by cost priority (low -> high)\n",
    "        cost_order = {\"low\": 0, \"med\": 1, \"high\": 2}\n",
    "        self.filters = sorted(filters, key=lambda f: cost_order.get(f.cost, 3))\n",
    "        self.save_dir = save_dir\n",
    "        print(\n",
    "            f\"Initialized with {len(self.filters)} filters sorted by cost priority: {[f.name for f in self.filters]}\"\n",
    "        )\n",
    "\n",
    "    def run(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        self.original_df = df.copy()\n",
    "        keep_mask = np.ones(len(df), dtype=bool)\n",
    "        all_scores = pd.DataFrame(index=df.index)\n",
    "\n",
    "        for filt in self.filters:\n",
    "            filtered_df = df[keep_mask]\n",
    "            print(f\"Running {filt.name}\")\n",
    "            scores, local_keep = filt.apply(filtered_df)\n",
    "            print(f\"sum of {filt.name}: {local_keep.sum()}\")\n",
    "            full_local_keep = np.zeros(len(df), dtype=bool)\n",
    "            full_local_keep[keep_mask] = local_keep\n",
    "\n",
    "            if scores is not None:\n",
    "                score_series = pd.Series(np.nan, index=df.index, dtype=float)\n",
    "                score_series[keep_mask] = scores\n",
    "                all_scores[f\"{filt.name}_score\"] = score_series\n",
    "\n",
    "            score_series = pd.Series(np.nan, index=df.index, dtype=object)\n",
    "            score_series[keep_mask] = local_keep\n",
    "            all_scores[filt.name] = score_series\n",
    "            if self.save_dir is not None:\n",
    "\n",
    "                os.makedirs(self.save_dir, exist_ok=True)\n",
    "                out_path = os.path.join(self.save_dir, \"all_scores.csv\")\n",
    "                all_scores.to_csv(out_path, index=False)\n",
    "            keep_mask &= full_local_keep\n",
    "\n",
    "        self.filtered_df = df[keep_mask].reset_index(drop=True)\n",
    "        all_scores[\"filtered_mask\"] = keep_mask == False\n",
    "        self.all_filtered_scores = all_scores\n",
    "\n",
    "        print(f\"Final size after applying filters: {self.filtered_df.shape}\")\n",
    "\n",
    "        return self.filtered_df, all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fd56e6-b5c6-41e9-a4ea-b60b49dfd525",
   "metadata": {},
   "source": [
    "**Optional: Defining custom filters**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fde730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tlm_kwargs = {\n",
    "    \"quality_preset\": \"base\",\n",
    "    \"options\": {\"model\": \"gpt-4.1-nano\"},\n",
    "}\n",
    "\n",
    "tlm_filter_pii_protection = TLMCustomEvalFilter(\n",
    "    criteria_name=\"pii_protection\",\n",
    "    criteria_instruction=\"\"\"\n",
    "Determine if the answer contains or exposes sensitive personally identifiable information (PII).\n",
    "\n",
    "A bad answer:\n",
    "- Contains private data such as credit card numbers, Social Security numbers, or other government-issued IDs.\n",
    "- Includes full personal addresses, private phone numbers, or personal email addresses.\n",
    "- Reveals sensitive account information such as bank details, usernames, or passwords.\n",
    "- Repeats or reflects any confidential personal details shared by the user.\n",
    "- Mentions information not typically accessible to the public.\n",
    "\n",
    "A great answer:\n",
    "- Contains no sensitive or private PII.\n",
    "- May include **publicly available contact information** such as:\n",
    "  - Government helplines\n",
    "  - City service phone numbers\n",
    "  - Support emails published on official websites\n",
    "- Does **not** leak or infer sensitive user-specific details.\n",
    "\n",
    "Only flag answers that contain **private, confidential, or user-specific** personal data.\n",
    "\"\"\",\n",
    "    tlm_kwargs=tlm_kwargs,\n",
    "    trustworthiness_threshold=0.5,\n",
    "    save_dir=SAVE_DIR\n",
    ")\n",
    "\n",
    "tlm_binary_query_is_good = TLMBinaryFilter(\n",
    "    prompt_template=\"\"\"Is the message a query?\n",
    "    query: {query}\n",
    "\n",
    "    --\n",
    "    Respond with \"Yes\" if the message is a question, request or search term, otherwise respond with \"No\".\n",
    "    Please respond with only \"Yes\" or \"No\" with no leading or trailing text. Do not return anything else.\n",
    "    \"\"\",\n",
    "    trustworthiness_threshold=0.5,\n",
    "    name=\"TLMBinary_query_is_good\",\n",
    "    tlm_kwargs=tlm_kwargs,\n",
    "    save_dir=SAVE_DIR\n",
    "\n",
    ")\n",
    "\n",
    "tlm_filter_thankyou_or_question_only = TLMCustomEvalFilter(\n",
    "    criteria_name=\"thankyou_or_question_only\",\n",
    "    criteria_instruction=\"\"\"Determine if the answer fails to address the user's query by either:\n",
    "\n",
    "1. Only including salutations or closing phrases (such as \"thank you\", \"goodbye\", etc.).\n",
    "2. Only asking a clarifying or follow-up question without providing a useful answer.\n",
    "3. Giving a vague, off-topic, or unhelpful reply that does not meaningfully respond to the query.\n",
    "\n",
    "A bad answer:\n",
    "- Provides no informational content beyond a greeting or farewell.\n",
    "- Ends the interaction without addressing the user's query.\n",
    "- Only asks a question and does not answer the original query.\n",
    "- Gives an answer that appears unrelated or avoids the core of the user\u2019s query.\n",
    "\n",
    "A great answer:\n",
    "- Answers the query clearly and directly.\n",
    "- May include greetings or closings, but also contains relevant, helpful information.\n",
    "\"\"\",\n",
    "    tlm_kwargs=tlm_kwargs,\n",
    "    trustworthiness_threshold=0.5,\n",
    "    save_dir=SAVE_DIR\n",
    "\n",
    ")\n",
    "\n",
    "tlm_filter_non_informative_answer = TLMCustomEvalFilter(\n",
    "    criteria_name=\"non_informative_answer\",\n",
    "    criteria_instruction=\"\"\"Determine if the answer fails to provide a meaningful or useful answer to the query.\n",
    "\n",
    "A bad answer:\n",
    "- States it cannot answer or lacks the information, without offering help.\n",
    "- Summarizes a solution without giving specific, actionable details.\n",
    "- Avoids addressing the actual query.\n",
    "- References resources or solutions but doesn't explain or elaborate.\n",
    "- Assumes facts not stated, leading to potentially inaccurate or misleading advice.\n",
    "- Requests personalized, time-bound, or specific updates without sufficient context.\n",
    "- Provides an answer tailored to one user\u2019s specific circumstance rather than offering generalizable guidance.\n",
    "\n",
    "A great answer:\n",
    "- Directly answers the query with clear, specific information.\n",
    "- Provides actionable steps, examples, or guidance.\n",
    "- May include follow-up questions or clarifications, but only after delivering a substantive answer.\n",
    "\"\"\",\n",
    "    tlm_kwargs=tlm_kwargs,\n",
    "    trustworthiness_threshold=0.25,\n",
    "    save_dir=SAVE_DIR\n",
    "\n",
    ")\n",
    "\n",
    "tlm_filter_too_specific = TLMCustomEvalFilter(\n",
    "    criteria_name=\"too_specific\",\n",
    "    criteria_instruction=\"\"\"Determine if the query is too specific to the user's personal or narrow situation, making it unlikely to help others with similar questions.\n",
    "\n",
    "A bad query:\n",
    "- Asks about uncommon, highly personalized, or overly detailed scenarios.\n",
    "- Limits the usefulness of any answer to a wider audience.\n",
    "- Relies on information that is time-sensitive or valid only for a short period.\n",
    "- Often involves specific items, account actions, or events relevant only to one user.\n",
    "- Lacks sufficient context to be broadly interpretable or reusable by others.\n",
    "\n",
    "A good query:\n",
    "- Asks about a general topic or situation relevant to many users.\n",
    "- May include details, but still invites broadly useful information.\n",
    "\n",
    "Give a low score for a bad query.\n",
    "\"\"\",\n",
    "    tlm_kwargs=tlm_kwargs,\n",
    "    trustworthiness_threshold=0.5,\n",
    "    save_dir=SAVE_DIR\n",
    "\n",
    ")\n",
    "\n",
    "tlm_outdated_answer_filter = TLMOutdatedAnswerFilter(prompt_template= \"\"\"\n",
    "Today's date is {today_str}.\n",
    "{meta_data}\n",
    "\n",
    "<query>\n",
    "{query}\n",
    "</query>\n",
    "\n",
    "<answer>\n",
    "{answer}\n",
    "</answer>\n",
    "\n",
    "Your task is to determine:\n",
    "\n",
    "Is this answer likely outdated today? Yes or No.\n",
    "\n",
    "Only say \u201cYes\u201d if the information is likely no longer useful, accurate, or valid due to changes over time, such as:\n",
    "  - Obsolete procedures\n",
    "  - Outdated rates or policies\n",
    "  - Time-sensitive answers (e.g., booked dates or one-time confirmations)\n",
    "  - Temporary time-bound information (e.g., restrictions in effect only during specific past dates)\n",
    "  - Mentions a specific date or event (e.g., a sale) that has already passed relative to today\n",
    "  + Contains vague or relative status updates that may no longer be accurate (e.g., \u201csoon\u201d, \u201ca few days\u201d, \u201crecently processed\u201d, \u201cin progress\u201d)\n",
    "\n",
    "                                                     \n",
    "Answer \"No\" if:\n",
    "  - Provides general information, definitions, or explanations that are likely still valid (e.g., what \"manual watering\" means)\n",
    "  - Describes standard processes or requirements for making a request (e.g., setting up inspection)\n",
    "\n",
    "Respond with Yes or No only.\n",
    "\"\"\",\n",
    "trustworthiness_threshold=0.9,\n",
    "save_dir=SAVE_DIR\n",
    ")\n",
    "\n",
    "\n",
    "# Optionally improve outdated filter via days-since-conversation calculation:\n",
    "def extract_date_from_metadata(metadata):\n",
    "    date_match = re.search(r'\\d{4}-\\d{2}-\\d{2}', metadata)\n",
    "    return date_match.group(0) if date_match else None\n",
    "\n",
    "metadata = df['metadata'][0]\n",
    "df['conv_date'] = pd.to_datetime(df['metadata'].apply(extract_date_from_metadata))\n",
    "from datetime import datetime\n",
    "\n",
    "# Calculate the number of days passed since conv_date\n",
    "df['days_passed'] = (datetime.now() - df['conv_date']).dt.days\n",
    "df['metadata'] = df.apply(\n",
    "    lambda row: row['metadata'].replace(\n",
    "        'The conversation started',\n",
    "        f'The conversation started {row[\"days_passed\"]} days ago'\n",
    "    ),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a0ac9e",
   "metadata": {},
   "source": [
    "Now that they're defined, let's apply our filters to the QA pairs data. You can either use the filters provided here directly and/or define your own filters using the above classes. Here we filter out bad examples based on: keywords, exact string matching, response-length, bad queries, potentially outdated answers, answers that contain PII, non-informative answers, and overly-specific answers (that won't help other customers besides the one who originally received this answer). Note that you can save runtime/costs by running the filters in a specific order, prioritizing cheaper/faster filters as well as filters expected to remove lots of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1a2168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized with 10 filters sorted by cost priority: ['LengthFilter', 'ExactMatchFilter', 'KeywordFilter', 'KeywordFilter', 'TLMBinary_query_is_good', 'TLMCustomEval_non_informative_answer', 'TLMCustomEval_thankyou_or_question_only', 'TLMCustomEval_pii_protection', 'TLMCustomEval_too_specific', 'TLMOutdatedAnswerFilter']\n",
      "Running LengthFilter\n",
      "sum of LengthFilter: 99\n",
      "Running ExactMatchFilter\n",
      "sum of ExactMatchFilter: 99\n",
      "Running KeywordFilter\n",
      "sum of KeywordFilter: 99\n",
      "Running KeywordFilter\n",
      "sum of KeywordFilter: 87\n",
      "Running TLMBinary_query_is_good\n",
      "If this progress bar appears frozen, TLM is still processing your dataset so just continue waiting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Querying TLM... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of TLMBinary_query_is_good: 87\n",
      "Running TLMCustomEval_non_informative_answer\n",
      "If this progress bar appears frozen, TLM is still processing your dataset so just continue waiting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Querying TLM... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of TLMCustomEval_non_informative_answer: 66\n",
      "Running TLMCustomEval_thankyou_or_question_only\n",
      "If this progress bar appears frozen, TLM is still processing your dataset so just continue waiting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Querying TLM... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of TLMCustomEval_thankyou_or_question_only: 32\n",
      "Running TLMCustomEval_pii_protection\n",
      "If this progress bar appears frozen, TLM is still processing your dataset so just continue waiting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Querying TLM... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of TLMCustomEval_pii_protection: 32\n",
      "Running TLMCustomEval_too_specific\n",
      "If this progress bar appears frozen, TLM is still processing your dataset so just continue waiting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Querying TLM... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of TLMCustomEval_too_specific: 12\n",
      "Running TLMOutdatedAnswerFilter\n",
      "If this progress bar appears frozen, TLM is still processing your dataset so just continue waiting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Querying TLM... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of TLMOutdatedAnswerFilter: 10\n",
      "Final size after applying filters: (10, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply the simplest non-TLM filters first (such as removing exact unhelpful answers, and keyword matches), then proceed to the more advanced TLM-based filters in order of strictness.\n",
    "exact_answers = ['If you have another question, please go ahead.' ,\n",
    " 'Great talking to you! If you have more questions later, I\u2019m here to help. Bye for now!',\n",
    " 'Thank you for the feedback'\n",
    "]\n",
    "\n",
    "length_filter = LengthFilter(min_word_count=3)\n",
    "exact_match_filter = ExactMatchFilter(exact_answers=exact_answers,ignore_case=True)\n",
    "keyword_filter = KeywordFilter(keywords=[\"<person>\", \"<redacted>\"])\n",
    "keyword_filter_no_punctuation = KeywordFilter(keywords=[\"redacted\", 'assistant'], strip_punctuation=True)\n",
    "filters = [\n",
    "    length_filter,\n",
    "    exact_match_filter,\n",
    "    keyword_filter,\n",
    "    keyword_filter_no_punctuation,\n",
    "    tlm_binary_query_is_good, \n",
    "    tlm_filter_non_informative_answer,\n",
    "    tlm_filter_thankyou_or_question_only,\n",
    "    tlm_filter_pii_protection,\n",
    "    tlm_filter_too_specific,\n",
    "    tlm_outdated_answer_filter,\n",
    "]\n",
    "run_filters = RunFilters(filters = filters)\n",
    "\n",
    "filtered_df, scores = run_filters.run(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe43f391",
   "metadata": {},
   "source": [
    "## Apply a Specific Filter to the Entire DataFrame\n",
    " To apply a specific filter across the whole DataFrame, simply run the filter on your DataFrame. For example, you can use `tlm_filter_too_specific` to obtain a score and mask for each row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ae23fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If this progress bar appears frozen, TLM is still processing your dataset so just continue waiting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Querying TLM... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|\n"
     ]
    }
   ],
   "source": [
    "score, mask = tlm_filter_too_specific.apply(df)\n",
    "df['tlm_filter_too_specific_score'] = score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc8389a",
   "metadata": {},
   "source": [
    "## Review Filter Scores\n",
    "Let's take a closer look at some of the filter scores. By exploring these values, we can better understand how our filters are performing and make informed decisions about setting appropriate thresholds for our data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60732c6",
   "metadata": {},
   "source": [
    "Let's review the following data for examples that may be too specific or personalized.\n",
    "Information tailored to individual cases might not be useful for serving as expert answers from Codex,\n",
    "so consider removing or revising such entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bfe5797",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, scores], axis=1) # add all computed scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c006b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: How can I reset my account password and check my order information?\n",
      "answer: To reset your account password, you need to provide your username and security information such as your mother's maiden name or security answer. In this case, after providing your username (normanbouchard461) and security answer (rachel), the assistant generated a new password for you: wkmnhf5enq8. You can use this password to access your account and check your order information.\n",
      "tlm_filter_too_specific_score: 0.011776311443693182\n",
      "----------------------------------------------------------------------------------------------------\n",
      "query: What are my options for returning or exchanging a purchase made more than 30 days ago?\n",
      "answer: Since the purchase was made on July 21, 2019, and returns are only possible within 30 days of purchase, you are unable to return or exchange the item through the standard process.\n",
      "tlm_filter_too_specific_score: 0.019290918929105706\n",
      "----------------------------------------------------------------------------------------------------\n",
      "query: How can I resolve the issue of receiving an email confirmation for an order that shows I ordered two items when I only ordered one?\n",
      "answer: The order was a mistake by the company. The assistant offered a refund and confirmed that the unwanted item had been removed from the order, with the item valued at $49.\n",
      "tlm_filter_too_specific_score: 0.02135825803012212\n",
      "----------------------------------------------------------------------------------------------------\n",
      "query: Can I get an extension on my subscription payment due to financial hardship caused by a farm fire?\n",
      "answer: Because you are a prized gold member, you will be able to get a subscription extension.\n",
      "tlm_filter_too_specific_score: 0.02902835087734005\n",
      "----------------------------------------------------------------------------------------------------\n",
      "query: Can I get an extension on my premium subscription fee due to financial hardship?\n",
      "answer: Unfortunately, I cannot give you an extension on your premium subscription fee due to your membership level.\n",
      "tlm_filter_too_specific_score: 0.03931668614064791\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "display_filter_results(df,'tlm_filter_too_specific_score', n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1370a147",
   "metadata": {},
   "source": [
    "It's important to remove outdated responses to ensure users receive accurate and relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2244de10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: What is the status of my refund and will it be credited to my original payment method?\n",
      "answer: Your refund is currently in progress and will be completed by tomorrow. The original payment method will be credited for the amount of the refund.\n",
      "TLMOutdatedAnswerFilter_score: 0.9999923903446042\n",
      "----------------------------------------------------------------------------------------------------\n",
      "query: Is there any trouble with the website today that is causing it to run very slow?\n",
      "answer: The website is experiencing slow performance, and a report has been sent to the web team to investigate the issue.\n",
      "TLMOutdatedAnswerFilter_score: 0.9999875564808334\n",
      "----------------------------------------------------------------------------------------------------\n",
      "query: How can I regain access to my account when I no longer have access to the two-factor authentication phone number?\n",
      "answer: To regain access to your account without the two-factor authentication phone number, you can request a reset by verifying your identity through your registered email address. The process involves providing your full name and email address, after which a reset code will be sent to your email. Once you receive the code, you can use it to access your account and update your security settings.\n",
      "TLMOutdatedAnswerFilter_score: 0.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "query: How can I regain access to my account after losing access to my two-factor authentication device?\n",
      "answer: To regain access to your account after losing your two-factor authentication device, you can request a reset by providing your account email to receive instructions for disabling two-factor authentication. Once you receive the reset email, follow the instructions to disable two-factor authentication and then verify your login to ensure access.\n",
      "TLMOutdatedAnswerFilter_score: 0.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "query: How can I return an item I received in the wrong color?\n",
      "answer: To return an item received in the wrong color, you can request a return shipping label by providing your shipping address and choosing to return by mail. You should receive the return shipping label shortly to send back the product.\n",
      "TLMOutdatedAnswerFilter_score: 0.0\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "display_filter_results(df,'TLMOutdatedAnswerFilter_score',columns=['query', 'answer'], n=5,ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8526a3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: How can I return an item I purchased, and what are the steps involved?\n",
      "answer: To return an item, you need to provide your full name, account ID, username, email address, order ID, membership level, and shipping address. You can choose to process the return by mail, in store, or at a drop-off center. If you are a gold member, you are eligible for unlimited returns.\n",
      "TLMCustomEval_non_informative_answer_score: 0.0055800121961026876\n",
      "----------------------------------------------------------------------------------------------------\n",
      "query: When does the annual sale end?\n",
      "answer: This year's sale ended on January 31st.\n",
      "TLMCustomEval_non_informative_answer_score: 0.01743464663794691\n",
      "----------------------------------------------------------------------------------------------------\n",
      "query: What is the width of the Guess boots priced at $59, and are they considered wide?\n",
      "answer: The boots are generally 1/6 wider than the usual U.S. system, so if you're unsure whether they will fit, it's recommended to try them on and return if there's a problem.\n",
      "TLMCustomEval_non_informative_answer_score: 0.021357812195540303\n",
      "----------------------------------------------------------------------------------------------------\n",
      "query: How can I return an item that is the wrong size?\n",
      "answer: To return an item that is the wrong size, you need to provide your full name or account ID, your email address, order ID, membership level, and full address. You can then choose to process the return by mail, in store, or at a drop-off center.\n",
      "TLMCustomEval_non_informative_answer_score: 0.023656485392906296\n",
      "----------------------------------------------------------------------------------------------------\n",
      "query: What are the differences between the silver and gold membership levels in the premium subscription service?\n",
      "answer: Silver members can cancel orders at any time and have an agent make purchases for them, while gold members can make unlimited refunds and are trusted more.\n",
      "TLMCustomEval_non_informative_answer_score: 0.02620633900709706\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "display_filter_results(df,'TLMCustomEval_non_informative_answer_score', n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34c1c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv(\"filtered_question_answer_pairs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c97114c",
   "metadata": {},
   "source": [
    " Here is the final set of high-quality question-answer pairs ready to be added to Codex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b26ad96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can I regain access to my account when I n...</td>\n",
       "      <td>To regain access to your account without the t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How can I regain access to my account after lo...</td>\n",
       "      <td>To regain access to your account after losing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I return an item I received in the wro...</td>\n",
       "      <td>To return an item received in the wrong color,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0  How can I regain access to my account when I n...   \n",
       "1  How can I regain access to my account after lo...   \n",
       "2  How can I return an item I received in the wro...   \n",
       "\n",
       "                                              answer  \n",
       "0  To regain access to your account without the t...  \n",
       "1  To regain access to your account after losing ...  \n",
       "2  To return an item received in the wrong color,...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df[['query','answer']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b453653f",
   "metadata": {},
   "source": [
    "## Add Question-Answer Pairs into Codex Project\n",
    "\n",
    "Finally, we initialize a Codex Project and load our high-quality QA pairs into it, so they can be served as expert answers. This is a great way to hot-start any Codex Project with a large set of expert answers, which didn't require any human work to obtain!\n",
    "\n",
    "Over time, as you collect more customer support tickets with good human answers in them, you can repeat this process, except now adding the new QA pairs to your existing Codex Project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09c85ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a project\n",
    "project = codex_client.create_project(\n",
    "    name=\"Filtered: ABCD\",\n",
    "    description=\"QA pairs for ABCD\",\n",
    ")\n",
    "access_key = project.create_access_key(\"test access key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e81f4fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered DataFrame shape: (10, 6)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filtered_df = pd.read_csv(\"filtered_question_answer_pairs.csv\")\n",
    "print(\"Filtered DataFrame shape:\", filtered_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9f6987",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:02,  1.11s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for row in tqdm(filtered_df.itertuples(index=False)):\n",
    "\n",
    "    project.add_remediation(\n",
    "        question=row.query,\n",
    "        answer=row.answer,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (codex)",
   "language": "python",
   "name": "codex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}