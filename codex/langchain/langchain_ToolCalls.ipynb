{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with Tool Calls in LangChain\n",
    "\n",
    "This notebook covers the basics of building an agentic RAG application with LangChain (the specific RAG app\u00a0used in our [Integrate Codex as-a-Tool with LangChain](/codex/tutorials/langchain/langchain_CodexAsTool/) tutorial)\n",
    "\n",
    "The LangChain framework implements *agentic* RAG, which treats Retrieval as a tool that can be called (rather than as a step hardcoded into every user interaction, as is done in standard RAG). For standard RAG (where retrieval is a hardcoded step), refer to our [Adding Tool Calls to RAG](/codex/tutorials/other_rag_frameworks/OtherRAG_ToolCalls/) tutorial.\n",
    "\n",
    "Here's a typical architecture for agentic RAG apps with tool calling:\n",
    "\n",
    "![RAG Workflow](../assets/codexastool_retrievalastool_simple.png)\n",
    "\n",
    "Let's first install required packages for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-text-splitters langchain-community langgraph langchain-openai  # we used package-versions 0.3.5, 0.3.16, 1.59.7, 0.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR-KEY-HERE>\"  # Replace with your OpenAI API key\n",
    "generation_model = \"gpt-4o\"  # model used by RAG system (has to support tool calling)\n",
    "embedding_model = \"text-embedding-3-small\"   # any LangChain embeddings model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Customer Service for a New Product\n",
    "\n",
    "Consider a customer support / e-commerce RAG use-case where the Knowledge Base contains product listings like the following:\n",
    "\n",
    "![Image of a beautiful simple water bottle that is definitely worth more than the asking price](../assets/simple_water_bottle.png)\n",
    "\n",
    "To keep this example minimal, we'll use a simple in-memory vector store with a single document. The document will contain the `context` (product information) on the product above. The current setup can be updated with any LangChain embeddings model and vector store. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803fa030-fb0e-496c-9d09-97971090f3ad",
   "metadata": {},
   "source": [
    "**Optional: Initialize vector store + add document**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize vector store\n",
    "embeddings = OpenAIEmbeddings(model=embedding_model)\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n",
    "# Sample document to demonstrate Codex integration\n",
    "product_page_content = \"\"\"Simple Water Bottle - Amber (limited edition launched Jan 1st 2025)\n",
    "A water bottle designed with a perfect blend of functionality and aesthetics in mind. Crafted from high-quality, durable plastic with a sleek honey-colored finish.\n",
    "Price: $24.99 \\nDimensions: 10 inches height x 4 inches width\"\"\"\n",
    "documents =[\n",
    "    Document(\n",
    "        id=\"simple_water_bottle.txt\",\n",
    "        page_content=product_page_content,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Standard LangChain text splitting - use any splitter that fits your docs\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# Add documents to your chosen vector store\n",
    "_ = vector_store.add_documents(documents=all_splits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Chat App with Tool Calls\n",
    "\n",
    "We now define a tool-calling RAG app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import BaseTool\n",
    "from langchain_core.messages import (\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    BaseMessage,\n",
    ")\n",
    "from typing import List, Optional\n",
    "\n",
    "class RAGApp:\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: ChatOpenAI,\n",
    "        tools: List[BaseTool],\n",
    "        retriever: BaseTool,\n",
    "        messages: Optional[List[BaseMessage]] = None\n",
    "    ):\n",
    "        \"\"\"Initialize RAG application with provided components.\"\"\"\n",
    "        _tools = [retriever] + tools\n",
    "        self.tools = {tool.name: tool for tool in _tools}\n",
    "        self.llm = llm.bind_tools(_tools)\n",
    "        self.messages: List[BaseMessage] = messages or []\n",
    "\n",
    "    def chat(self, user_query: str) -> str:\n",
    "        \"\"\"Process user input and handle any necessary tool calls.\"\"\"\n",
    "        # Add user query to messages\n",
    "        self.messages.append(HumanMessage(content=user_query))\n",
    "        \n",
    "        # Get initial response (may include tool calls)\n",
    "        print(f\"[internal log] Invoking LLM text\\n{user_query}\\n\\n\")\n",
    "        response = self.llm.invoke(self.messages)\n",
    "        self.messages.append(response)\n",
    "        \n",
    "        # Handle any tool calls\n",
    "        while response.tool_calls:\n",
    "            # Process each tool call\n",
    "            for tool_call in response.tool_calls:\n",
    "                # Get the appropriate tool\n",
    "                tool = self.tools[tool_call[\"name\"].lower()]\n",
    "                \n",
    "                # Call the tool and get result\n",
    "                tool_name = tool_call[\"name\"]\n",
    "                tool_args = tool_call[\"args\"]\n",
    "                print(f\"[internal log] Calling tool: {tool_name} with args: {tool_args}\")\n",
    "                tool_result = tool.invoke(tool_call)\n",
    "                print(f'[internal log] Tool response: {str(tool_result)}')\n",
    "                self.messages.append(tool_result)\n",
    "            \n",
    "            # Get next response after tool calls\n",
    "            response = self.llm.invoke(self.messages)\n",
    "            self.messages.append(response)\n",
    "        \n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example tool: get_todays_date\n",
    "\n",
    "Let's define an example tool `get_todays_date()` that our RAG app can rely on. LangChain does not need function schemas meaning adding tools much is easier - write normal Python functions and it automatically: reads your function name and docstring, understands your parameters and type hints, and creates the LLM-friendly format for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from datetime import datetime\n",
    "\n",
    "@tool\n",
    "def get_todays_date(date_format: str) -> str:\n",
    "  \"A tool that returns today's date in the date format requested. Options for date_format parameter are: '%Y-%m-%d', '%d', '%m', '%Y'.\"\n",
    "  datetime_str = datetime.now().strftime(date_format)\n",
    "  return datetime_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Retriever Tool\n",
    "In addition to our example tool, we need to explicitly provide the system with a tool that works as a `retriever`. It searches the vector store for relevant Context as is needed if we want our system to do any context retrieval. Let's define it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def retrieve(query: str) -> str:\n",
    "    \"\"\"Search through available documents to find relevant information.\"\"\"\n",
    "    docs = vector_store.similarity_search(query, k=2)\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update our LLM system prompt with tool call instructions\n",
    "\n",
    "For the best performance, **add instructions on when to use the tool into the system prompt** that governs your LLM. Below we simply added Step **3.** and **4.** in our list of instructions, which otherwise represent a typical RAG system prompt. In most RAG apps, one instructs the LLM on what `fallback_answer` to respond with when it does not know how to answer a user's query. Such fallback instructions help you reduce hallucinations and more precisely control the AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fallback_answer = \"Based on the available information, I cannot provide a complete answer to this question.\"\n",
    "\n",
    "system_message = f\"\"\"You are a helpful assistant designed to help users navigate a complex set of documents for question-answering tasks. Answer the user's Question based on the following possibly relevant Context and previous chat history using the tools provided if necessary. Follow these rules in order:\n",
    "    1. NEVER use phrases like \"according to the context\", \"as the context states\", etc. Treat the Context as your own knowledge, not something you are referencing.\n",
    "    2. Use only information from the provided Context.\n",
    "    3. Give a clear, short, and accurate Answer. Explain complex terms if needed.\n",
    "    4. You have access to the retrieve tool, to retrieve relevant information to the query as Context.\n",
    "    5. If the answer to the question requires today's date, use the following tool: get_todays_date. Return the date in the exact format the tool provides it.\n",
    "    6. If the Context doesn't adequately address the Question or you are unsure how to answer the Question, say: \"{fallback_answer}\" only, nothing else.\n",
    "\n",
    "    Remember, your purpose is to provide information based on the Context, not to offer original advice.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize our RAG App\n",
    "Finally, let's set up our LLM that supports tool calling and initialize our RAG App. Any LangChain-compatible LLM can be used here, as long as it supports tool calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=generation_model)\n",
    "\n",
    "rag = RAGApp(\n",
    "    llm=llm,\n",
    "    tools=[get_todays_date],  # Add your tools here\n",
    "    retriever=retrieve,\n",
    "    messages=[SystemMessage(content=system_message)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG in action\n",
    "\n",
    "Let's run our RAG application over different questions commonly asked by users about the *Simple Water Bottle* in our example.\n",
    "\n",
    "### Scenario 1: RAG can answer the question without tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[internal log] Invoking LLM text\n",
      "How big is the water bottle?\n",
      "\n",
      "\n",
      "[internal log] Calling tool: retrieve with args: {'query': 'water bottle size'}\n",
      "[internal log] Tool response: content='Simple Water Bottle - Amber (limited edition launched Jan 1st 2025)\\nA water bottle designed with a perfect blend of functionality and aesthetics in mind. Crafted from high-quality, durable plastic with a sleek honey-colored finish.\\nPrice: $24.99 \\nDimensions: 10 inches height x 4 inches width' name='retrieve' tool_call_id='call_7oOCRF3Mu6FHhcSUgWRJ7ZLF'\n",
      "\n",
      "[RAG response] The water bottle is 10 inches in height and 4 inches in width.\n"
     ]
    }
   ],
   "source": [
    "response = rag.chat(\"How big is the water bottle?\")\n",
    "print(f\"\\n[RAG response] {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2: RAG can answer the question (using other tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[internal log] Invoking LLM text\n",
      "Check today's date. Has the limited edition Amber water bottle already launched?\n",
      "\n",
      "\n",
      "[internal log] Calling tool: get_todays_date with args: {'date_format': '%Y-%m-%d'}\n",
      "[internal log] Tool response: content='2025-02-25' name='get_todays_date' tool_call_id='call_B5WNeIMURcLAL04atuSmZfr7'\n",
      "\n",
      "[RAG response] Yes, the limited edition Amber water bottle has already launched, as it was released on January 1st, 2025.\n"
     ]
    }
   ],
   "source": [
    "response = rag.chat(\"Check today's date. Has the limited edition Amber water bottle already launched?\")\n",
    "print(f\"\\n[RAG response] {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 3: RAG cannot answer the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[internal log] Invoking LLM text\n",
      "Can I return my simple water bottle?\n",
      "\n",
      "\n",
      "[internal log] Calling tool: retrieve with args: {'query': 'water bottle return policy'}\n",
      "[internal log] Tool response: content='Simple Water Bottle - Amber (limited edition launched Jan 1st 2025)\\nA water bottle designed with a perfect blend of functionality and aesthetics in mind. Crafted from high-quality, durable plastic with a sleek honey-colored finish.\\nPrice: $24.99 \\nDimensions: 10 inches height x 4 inches width' name='retrieve' tool_call_id='call_rWrqgYfJtkcfggqsuWf159Up'\n",
      "\n",
      "[RAG response] Based on the available information, I cannot provide a complete answer to this question.\n"
     ]
    }
   ],
   "source": [
    "response = rag.chat(\"Can I return my simple water bottle?\")\n",
    "print(f\"\\n[RAG response] {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Context does not contain information about the return policy, and the `get_todays_date` tool would not help either.\n",
    "In this case, we want to return our fallback response to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Adding tool calls to your RAG system expands the capabilities of what your AI can do and the types of questions it can answer.\n",
    "\n",
    "Once you have a RAG app with tools set up, adding **Codex as-a-Tool** takes only a few lines of code.\n",
    "Codex enables your RAG app to answer questions it previously could not (like Scenario 3 above). Learn how via our tutorial: [Integrate Codex as-a-Tool with LangChain](/codex/tutorials/langchain/langchain_CodexAsTool/).\n",
    "\n",
    "Need help? Check the [FAQ](/codex/FAQ/) or email us at: support@cleanlab.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}