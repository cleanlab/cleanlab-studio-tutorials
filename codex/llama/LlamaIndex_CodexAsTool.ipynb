{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate Codex as-a-Tool with LlamaIndex\n",
    "\n",
    "This tutorial assumes you have a [LlamaIndex](https://github.com/run-llama/llama_index) RAG app that supports tool calls. Learn how to add tool calls to any LlamaIndex application via our tutorial: [RAG With Tool Calls in LlamaIndex](/codex/tutorials/llama/LlamaIndex_ToolCalls/).\n",
    "\n",
    "Once you have a RAG app that supports tool calling, **adding Codex as an additional Tool takes minimal effort but guarantees better responses from your AI application**.\n",
    "\n",
    "![RAG Workflow](../assets/codexastool_retrievalfirst.png)\n",
    "\n",
    "If you prefer to integrate Codex without adding tool calls to your application, check out our [other integrations](/codex/concepts/integrations/).\n",
    "\n",
    "Let's first install packages required for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade llama-index llama-index-llms-openai  # we used package-versions 0.12.10 and 0.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install cleanlab_codex  # we used package-version 1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<YOU-KEY-HERE>\"  # Replace with your OpenAI API key\n",
    "model =  \"gpt-4o\"  # model used by RAG system (has to support tool calling)\n",
    "\n",
    "llm = OpenAI(model=model)  # API key can be set via OPENAI_API_KEY environment variable or .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe622111-51dc-44eb-be1e-54300c9f17d3",
   "metadata": {},
   "source": [
    "**Optional: Helper methods for basic RAG from prior tutorial (Adding Tool Calls to RAG)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_todays_date(date_format: str) -> str:\n",
    "  \"A tool that returns today's date in the date format requested. Options for date_format parameter are: '%Y-%m-%d', '%d', '%m', '%Y'.\"\n",
    "  datetime_str = datetime.now().strftime(date_format)\n",
    "  return datetime_str\n",
    "\n",
    "fallback_answer = \"Based on the available information, I cannot provide a complete answer to this question.\"\n",
    "\n",
    "system_message_without_codex = f\"\"\"You are a helpful assistant designed to help users navigate a complex set of documents for question-answering tasks. Answer the user's Question based on the following possibly relevant Context and previous chat history using the tools provided if necessary. Follow these rules in order:\n",
    "    1. NEVER use phrases like \"according to the context\", \"as the context states\", etc. Treat the Context as your own knowledge, not something you are referencing.\n",
    "    2. Use only information from the provided Context.\n",
    "    3. Give a clear, short, and accurate Answer. Explain complex terms if needed.\n",
    "    4. If the answer to the question requires today's date, use the following tool: get_todays_date. Return the date in the exact format the tool provides it.\n",
    "    5. If the Context doesn't adequately address the Question or you are unsure how to answer the Question, say: \"{fallback_answer}\" only, nothing else.\n",
    "\n",
    "    Remember, your purpose is to provide information based on the Context, not to offer original advice.\n",
    "\"\"\"\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, Document\n",
    "from llama_index.core.llms import ChatMessage, ChatResponse\n",
    "from llama_index.core.llms.function_calling import FunctionCallingLLM\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "# Ingest documents into a vector database, and set up a retriever\n",
    "documents = [\n",
    "    Document(text=\"Simple Water Bottle - Amber (limited edition launched Jan 1st 2025) \\n\\nA water bottle designed with a perfect blend of functionality and aesthetics in mind. Crafted from high-quality, durable plastic with a sleek honey-colored finish.\"),\n",
    "    Document(text=\"Price: $24.99 \\nDimensions: 10 inches height x 4 inches width\"),\n",
    "]\n",
    "index = VectorStoreIndex.from_documents(documents) # Set up your own doc-store and vector database here\n",
    "retriever = index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "class RAGApp:\n",
    "    def __init__(self,\n",
    "        llm: FunctionCallingLLM,\n",
    "        tools: list[FunctionTool],\n",
    "        retriever: BaseRetriever,\n",
    "        messages: list[ChatMessage] | None = None,\n",
    "    ):\n",
    "        self.llm = llm\n",
    "        self.tools = tools\n",
    "        self._tools_map = {tool.metadata.name: tool for tool in tools}\n",
    "        self.retriever = retriever\n",
    "        self.chat_history = messages or []\n",
    "\n",
    "    def __call__(self, user_query: str) -> ChatResponse:\n",
    "        \"\"\"Process user input: retrieve context to enrich query, get response (possibly using tools), update conversation.\"\"\"\n",
    "        self.chat_history.append(ChatMessage(role=\"user\", content=user_query))\n",
    "        context = self._retrieve_context(user_query)\n",
    "        query_with_context = self._form_prompt(user_question=user_query, retrieved_context=context)\n",
    "        response = self.handle_response_and_tools(query_with_context)\n",
    "        self.chat_history.append(response.message)\n",
    "        return response\n",
    "    \n",
    "    def _form_prompt(self, user_question: str, retrieved_context: str) -> str:\n",
    "        question_with_context = f\"Context:\\n{retrieved_context}\\n\\nUser Question:\\n{user_question}\"\n",
    "        # Below step is just formatting the final prompt for readability in the tutorial\n",
    "        indented_question_with_context = \"\\n\".join(f\"  {line}\" for line in question_with_context.splitlines())\n",
    "        return indented_question_with_context\n",
    "\n",
    "    def _retrieve_context(self, user_query: str) -> str:\n",
    "        \"\"\"Retrieves and formats context from documents matching the user query.\"\"\"\n",
    "        context_strings = [node.text for node in self.retriever.retrieve(user_query)]\n",
    "        return \"\\n\".join(context_strings)  # Basic context formatting for demo-purposes\n",
    "\n",
    "    def handle_response_and_tools(self, query: str) -> ChatResponse:\n",
    "        \"\"\"Manages tool-calling conversation loop using transient message history.\n",
    "        \n",
    "        Creates temporary chat history to track tool interactions without affecting main conversation.\n",
    "        Loops through tool calls and responses until completion, then returns final response to user.\n",
    "        \"\"\"\n",
    "        # Create a temporary chat history for tool interactions\n",
    "        temp_chat_history = self.chat_history.copy()\n",
    "        print(f\"[internal log] Invoking LLM text\\n{query}\\n\\n\")\n",
    "\n",
    "        response = self.llm.chat_with_tools(\n",
    "            tools=self.tools, \n",
    "            user_msg=query, \n",
    "            chat_history=temp_chat_history[:-1],\n",
    "        )\n",
    "        tool_calls = self.llm.get_tool_calls_from_response(\n",
    "            response, error_on_no_tool_call=False\n",
    "        )\n",
    "\n",
    "        while tool_calls:\n",
    "            temp_chat_history.append(response.message)\n",
    "            # If any tools are called, run with the tools until we hit the Alpha tool and return\n",
    "            for tool_call in tool_calls:\n",
    "                print(f'[internal log] Called {tool_call.tool_name} tool, with arguments: {tool_call.tool_kwargs}')\n",
    "                tool = self._tools_map[tool_call.tool_name]\n",
    "                tool_kwargs = tool_call.tool_kwargs\n",
    "                tool_output = tool(**tool_kwargs)\n",
    "                temp_chat_history.append(ChatMessage(role=\"tool\", content=str(tool_output), additional_kwargs={\"tool_call_id\": tool_call.tool_id}))\n",
    "                \n",
    "                response = self.llm.chat_with_tools([tool], chat_history=temp_chat_history)\n",
    "                print(f'[internal log] Tool response: {response.message.content}')\n",
    "                tool_calls = self.llm.get_tool_calls_from_response(\n",
    "                    response, error_on_no_tool_call=False\n",
    "                )\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Customer Service for a New Product\n",
    "\n",
    "Let's revisit our RAG app built in the [RAG with Tool Calls in LlamaIndex](/codex/tutorials/llama/LlamaIndex_ToolCalls/) tutorial, which has the option to call a `get_todays_date()` tool. This example represents a customer support / e-commerce use-case where the Knowledge Base contains product listings like the following:\n",
    "\n",
    "![Simple water bottle product listing](../assets/simple_water_bottle.png)\n",
    "\n",
    "The details of this example RAG app are unimportant if you are already familiar with RAG and Tool Calling, otherwise refer to the [RAG with Tool Calls in LlamaIndex](/codex/tutorials/llama/LlamaIndex_ToolCalls/) tutorial. That tutorial walks through the RAG app defined above. Subsequently, we integrate Codex-as-a-Tool and demonstrate its benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5rqMr-6PDHt"
   },
   "source": [
    "## Create Codex Project\n",
    "\n",
    "To use Codex, first [create a Project](/codex/web_tutorials/create_project/).\n",
    "\n",
    "Here we assume some common (question, answer) pairs about the *Simple Water Bottle* have already been added to a Codex Project.\n",
    "To learn how that was done, see our tutorial: [Populating Codex](/codex/web_tutorials/populating_codex/).\n",
    "\n",
    "Our existing Codex Project contains the following entries:\n",
    "\n",
    "![Codex Knowledge Base Example](../assets/codex_kb.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_key = \"<YOUR-PROJECT-ACCESS-KEY>\"  # Obtain from your Project's settings page: https://codex.cleanlab.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Programmatically populate the Codex Project with above (question, answer) pairs. Note: The recommended flow is to do this manually in the Web App.\n",
    "from cleanlab_codex.client import Client\n",
    "\n",
    "os.environ[\"CODEX_API_KEY\"] = \"<YOUR-KEY-HERE>\"  # Replace with your Codex API key\n",
    "codex_client = Client()\n",
    "\n",
    "# Create a project\n",
    "project = codex_client.create_project(\n",
    "    name=\"Product FAQs\",\n",
    "    description=\"Questions about product pages\",\n",
    ")\n",
    "\n",
    "# Add entries to the project\n",
    "project.add_entries(\n",
    "    entries=[\n",
    "        {\"question\": \"How much water can the Simple Water Bottle hold?\", \"answer\": \"32oz\"},\n",
    "        {\"question\": \"Can I return my Simple Water Bottle?\", \"answer\": \"Return it within 30 days for a full refund-- no questions asked. Contact our support team to initiate your return!\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "access_key = project.create_access_key(\"test access key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Codex as an additional tool\n",
    "\n",
    "Integrating Codex into a RAG app that supports tool calling requires minimal code changes:\n",
    "\n",
    "1. Import Codex and add it into your list of `tools`.\n",
    "2. Update your system prompt to include instructions for calling Codex, as demonstrated below in: `system_prompt_with_codex`.\n",
    "\n",
    "After that, call your original RAG pipeline with these updated variables to start experiencing the benefits of Codex!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Import CodexTool\n",
    "from cleanlab_codex import CodexTool\n",
    "\n",
    "codex_tool = CodexTool.from_access_key(access_key=access_key, fallback_answer=fallback_answer)\n",
    "codex_tool_llama = codex_tool.to_llamaindex_tool()\n",
    "\n",
    "globals()[codex_tool.tool_name] = codex_tool.query # Optional step for convenience: make function to call the tool globally accessible\n",
    "\n",
    "# 2: Update the RAG system prompt with instructions for handling Codex (adjust based on your needs)\n",
    "system_message_with_codex = f\"\"\"You are a helpful assistant designed to help users navigate a complex set of documents for question-answering tasks. Answer the user's Question based on the following possibly relevant Context and previous chat history using the tools provided if necessary. Follow these rules in order:\n",
    "    1. NEVER use phrases like \"according to the context\", \"as the context states\", etc. Treat the Context as your own knowledge, not something you are referencing.\n",
    "    2. Use only information from the provided Context.\n",
    "    3. Give a clear, short, and accurate Answer. Explain complex terms if needed.\n",
    "    4. If the answer to the question requires today's date, use the following tool: get_todays_date. Return the date in the exact format the tool provides it.\n",
    "    5. When the Context does not answer the user's Question, call the `{codex_tool.tool_name}` tool.\n",
    "        - Always use `{codex_tool.tool_name}` if the provided Context lacks the necessary information.\n",
    "        - Your query to `{codex_tool.tool_name}` should closely match the user\u2019s original Question, with only minor clarifications if needed.\n",
    "        - Evaluate the response from `{codex_tool.tool_name}`. If the response is helpful, use it to answer the user\u2019s Question. If the response is not helpful, ignore it.\n",
    "    6. If you still cannot confidently answer the user's Question (even after using `{codex_tool.tool_name}` and other tools), say: \"{fallback_answer}\".\n",
    "    \n",
    "    Remember, your purpose is to provide information based on the Context and make effective use of `{codex_tool.tool_name}` when necessary, not to offer original advice.\n",
    "\"\"\"\n",
    "\n",
    "# 3: Initialize RAGApp with the CodexTool\n",
    "llm = OpenAI(model=model)\n",
    "chat_history = [\n",
    "    ChatMessage(role=\"system\", content=system_message_with_codex),  # Add Codex instructions here\n",
    "]\n",
    "tools = [\n",
    "    FunctionTool.from_defaults(fn=get_todays_date),\n",
    "    codex_tool_llama  # Add Codex to list of tools\n",
    "]\n",
    "rag_with_codex = RAGApp(llm=llm, tools=tools, retriever=retriever, messages=chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e7af04-2cc9-4c01-ae7c-9a64a3c871ee",
   "metadata": {},
   "source": [
    "**Optional: Initialize RAG App without CodexTool**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [\n",
    "    ChatMessage(role=\"system\", content=system_message_without_codex),  # Add Codex instructions here\n",
    "]\n",
    "tools = [\n",
    "    FunctionTool.from_defaults(fn=get_todays_date),\n",
    "    codex_tool_llama # Add Codex to list of tools\n",
    "]\n",
    "rag_without_codex = RAGApp(llm=llm, tools=tools, retriever=retriever, messages=chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG with Codex in action \n",
    "\n",
    "Integrating Codex as-a-Tool allows your RAG app to answer more questions than it was originally capable of.\n",
    "\n",
    "### Example 1\n",
    "\n",
    "Let's ask a question to our **original** RAG app (before Codex was integrated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[internal log] Invoking LLM text\n",
      "  Context:\n",
      "  Simple Water Bottle - Amber (limited edition launched Jan 1st 2025) \n",
      "  \n",
      "  A water bottle designed with a perfect blend of functionality and aesthetics in mind. Crafted from high-quality, durable plastic with a sleek honey-colored finish.\n",
      "  Price: $24.99 \n",
      "  Dimensions: 10 inches height x 4 inches width\n",
      "  \n",
      "  User Question:\n",
      "  Can I return my Simple Water Bottle?\n",
      "\n",
      "\n",
      "\n",
      "[RAG response] Based on the available information, I cannot provide a complete answer to this question.\n"
     ]
    }
   ],
   "source": [
    "response = rag_without_codex(\"Can I return my Simple Water Bottle?\")\n",
    "print(f'\\n[RAG response] {response.message.content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **original RAG app is unable to answer**, in this case because the required information is not in its Knowledge Base.\n",
    "\n",
    "Let's ask the same question to our RAG app with Codex added as an additional tool. \n",
    "Note that we use the updated system prompt and tool list when Codex is integrated in the RAG app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[internal log] Invoking LLM text\n",
      "  Context:\n",
      "  Simple Water Bottle - Amber (limited edition launched Jan 1st 2025) \n",
      "  \n",
      "  A water bottle designed with a perfect blend of functionality and aesthetics in mind. Crafted from high-quality, durable plastic with a sleek honey-colored finish.\n",
      "  Price: $24.99 \n",
      "  Dimensions: 10 inches height x 4 inches width\n",
      "  \n",
      "  User Question:\n",
      "  Can I return my Simple Water Bottle?\n",
      "\n",
      "\n",
      "[internal log] Called consult_codex tool, with arguments: {'question': 'Can I return my Simple Water Bottle?'}\n",
      "[internal log] Tool response: You can return your Simple Water Bottle within 30 days for a full refund. To start the return process, contact the support team.\n",
      "\n",
      "[RAG response] You can return your Simple Water Bottle within 30 days for a full refund. To start the return process, contact the support team.\n"
     ]
    }
   ],
   "source": [
    "response = rag_with_codex(\"Can I return my Simple Water Bottle?\")\n",
    "print(f'\\n[RAG response] {response.message.content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, **integrating Codex enables your RAG app to answer questions it originally strugged with**, as long as a similar question was already answered in the corresponding Codex Project.\n",
    "\n",
    "### Example 2\n",
    "\n",
    "Let's ask another question to our RAG app with Codex integrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[internal log] Invoking LLM text\n",
      "  Context:\n",
      "  Simple Water Bottle - Amber (limited edition launched Jan 1st 2025) \n",
      "  \n",
      "  A water bottle designed with a perfect blend of functionality and aesthetics in mind. Crafted from high-quality, durable plastic with a sleek honey-colored finish.\n",
      "  Price: $24.99 \n",
      "  Dimensions: 10 inches height x 4 inches width\n",
      "  \n",
      "  User Question:\n",
      "  How exactly can I order the Simple Water Bottle in bulk?\n",
      "\n",
      "\n",
      "[internal log] Called consult_codex tool, with arguments: {'question': 'How can I order the Simple Water Bottle in bulk?'}\n",
      "[internal log] Tool response: Based on the available information, I cannot provide a complete answer to this question.\n",
      "\n",
      "[RAG response] Based on the available information, I cannot provide a complete answer to this question.\n"
     ]
    }
   ],
   "source": [
    "response = rag_with_codex(\"How exactly can I order the Simple Water Bottle in bulk?\")\n",
    "print(f'\\n[RAG response] {response.message.content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our RAG app is unable to answer this question because there is no relevant information in its Knowledge Base, nor has a similar question been answered in the Codex Project (see the contents of the Codex Project above).\n",
    "\n",
    "**Codex automatically recognizes this question could not be answered and logs it into the Project where it awaits an answer from a SME**.\n",
    "\n",
    "![Codex Project with asked question that has not been answered yet](../assets/codex_kb_unanswered.png)\n",
    "\n",
    "As soon as an answer is provided in Codex, our RAG app will be able to answer all similar questions going forward (as seen for the previous query).\n",
    "\n",
    "### Example 3\n",
    "\n",
    "Let's ask another query to our RAG app with Codex integrated. This is a query the  original RAG app was able to correctly answer without Codex (since the relevant information exists in the Knowledge Base)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[internal log] Invoking LLM text\n",
      "  Context:\n",
      "  Simple Water Bottle - Amber (limited edition launched Jan 1st 2025) \n",
      "  \n",
      "  A water bottle designed with a perfect blend of functionality and aesthetics in mind. Crafted from high-quality, durable plastic with a sleek honey-colored finish.\n",
      "  Price: $24.99 \n",
      "  Dimensions: 10 inches height x 4 inches width\n",
      "  \n",
      "  User Question:\n",
      "  How big is the water bottle?\n",
      "\n",
      "\n",
      "\n",
      "[RAG response] The Simple Water Bottle is 10 inches in height and 4 inches in width.\n"
     ]
    }
   ],
   "source": [
    "response = rag_with_codex(\"How big is the water bottle?\")\n",
    "print(f'\\n[RAG response] {response.message.content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the RAG app with Codex integrated is still able to correctly answer this query. **Integrating Codex has no negative effect on questions your original RAG app could answer**.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that Codex is integrated with your RAG app, you and SMEs can [open the Codex Project and answer questions](/codex/web_tutorials/codex_as_sme/) logged there to continuously improve your AI.\n",
    "\n",
    "**Adding Codex only improves your RAG app.** Once integrated, Codex automatically logs all user queries that your original RAG app handles poorly. Using a [simple web interface](/codex/web_tutorials/codex_as_sme/), SMEs at your company can answer the highest priority questions in the Codex Project. As soon as an answer is entered in Codex, your RAG app will be able to properly handle all similar questions encountered in the future.\n",
    "\n",
    "Codex is **the fastest way for nontechnical SMEs to directly improve your AI application**. As the Developer, you simply integrate Codex once, and from then on, SMEs can continuously improve how your AI handles common user queries without needing your help.\n",
    "\n",
    "Need help, more capabilities, or other deployment options? \n",
    "Check the [FAQ](/codex/FAQ/) or email us at: support@cleanlab.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}