{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG With Tool Calls in LlamaIndex\n",
    "\n",
    "This notebook covers the basics of building a RAG app that supports tool calls. Here we demonstrate how to build the specific RAG app\u00a0used in our tutorial: [Integrate Codex as-a-Tool with LlamaIndex](/codex/tutorials/llama/LlamaIndex_CodexAsTool/).\n",
    "\n",
    "Here's a typical architecture for RAG apps with tool calling:\n",
    "\n",
    "![RAG Workflow](../assets/codexastool_retrievalfirst.png)\n",
    "\n",
    "Let's first install and setup LlamaIndex (this tutorial runs LlamaIndex with OpenAI LLMs, but you can use another LLM instead)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary LlamaIndex packages\n",
    "%pip install --upgrade llama-index llama-index-llms-openai  # we used package-versions 0.12.10 0.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR-KEY-HERE>\"  # Replace with your OpenAI API key\n",
    "model = \"gpt-4o\"  # Which LLM to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Customer Service for a New Product\n",
    "\n",
    "Consider a customer support / e-commerce RAG use-case where the Knowledge Base contains product listings like the following:\n",
    "\n",
    "![Image of a beautiful simple water bottle that is definitely worth more than the asking price](../assets/simple_water_bottle.png)\n",
    "\n",
    "To keep this example minimal, we'll use a simple in-memory vector store with a two documents. The documents will contain the `context` (product information) in the image above. Next, we specify functions to load the documents from the vector store, then connect the assistant to the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792d4791-746a-403c-9b48-cdee3d20b44a",
   "metadata": {},
   "source": [
    "**Optional: Initialize vector store + add documents**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import VectorStoreIndex, Document\n",
    "\n",
    "# Ingest documents into a vector database, and set up a retriever\n",
    "documents = [\n",
    "    Document(text=\"Simple Water Bottle - Amber (limited edition launched Jan 1st 2025) \\n\\nA water bottle designed with a perfect blend of functionality and aesthetics in mind. Crafted from high-quality, durable plastic with a sleek honey-colored finish.\"),\n",
    "    Document(text=\"Price: $24.99 \\nDimensions: 10 inches height x 4 inches width\"),\n",
    "]\n",
    "index = VectorStoreIndex.from_documents(documents) # Set up your own doc-store and vector database here\n",
    "retriever = index.as_retriever(similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Chat App with Tool Calls\n",
    "\n",
    "We now define a tool-calling RAG app."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640c324b-3dbd-4f03-90ae-afee72742c30",
   "metadata": {},
   "source": [
    "**Optional: Define RAG app (following the LlamaIndex format)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage, ChatResponse\n",
    "from llama_index.core.llms.function_calling import FunctionCallingLLM\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "\n",
    "class RAGApp:\n",
    "    def __init__(self,\n",
    "        llm: FunctionCallingLLM,\n",
    "        tools: list[FunctionTool],\n",
    "        retriever: BaseRetriever,\n",
    "        messages: list[ChatMessage] | None = None,\n",
    "    ):\n",
    "        self.llm = llm\n",
    "        self.tools = tools\n",
    "        self._tools_map = {tool.metadata.name: tool for tool in tools}\n",
    "        self.retriever = retriever\n",
    "        self.chat_history = messages or []\n",
    "\n",
    "    def __call__(self, user_query: str) -> ChatResponse:\n",
    "        \"\"\"Process user input: retrieve context to enrich query, get response (possibly using tools), update conversation.\"\"\"\n",
    "        self.chat_history.append(ChatMessage(role=\"user\", content=user_query))\n",
    "        context = self._retrieve_context(user_query)\n",
    "        query_with_context = self._form_prompt(user_question=user_query, retrieved_context=context)\n",
    "        response = self.handle_response_and_tools(query_with_context)\n",
    "        self.chat_history.append(response.message)\n",
    "        return response\n",
    "    \n",
    "    def _form_prompt(self, user_question: str, retrieved_context: str) -> str:\n",
    "        question_with_context = f\"Context:\\n{retrieved_context}\\n\\nUser Question:\\n{user_question}\"\n",
    "        # Below step is just formatting the final prompt for readability in the tutorial\n",
    "        indented_question_with_context = \"\\n\".join(f\"  {line}\" for line in question_with_context.splitlines())\n",
    "        return indented_question_with_context\n",
    "\n",
    "    def _retrieve_context(self, user_query: str) -> str:\n",
    "        \"\"\"Retrieves and formats context from documents matching the user query.\"\"\"\n",
    "        context_strings = [node.text for node in self.retriever.retrieve(user_query)]\n",
    "        return \"\\n\".join(context_strings)  # Basic context formatting for demo-purposes\n",
    "\n",
    "    def handle_response_and_tools(self, query: str) -> ChatResponse:\n",
    "        \"\"\"Manages tool-calling conversation loop using transient message history.\n",
    "        \n",
    "        Creates temporary chat history to track tool interactions without affecting main conversation.\n",
    "        Loops through tool calls and responses until completion, then returns final response to user.\n",
    "        \"\"\"\n",
    "        # Create a temporary chat history for tool interactions\n",
    "        temp_chat_history = self.chat_history.copy()\n",
    "        print(f\"[internal log] Invoking LLM with prompt\\n{query}\\n\\n\")\n",
    "\n",
    "        response = self.llm.chat_with_tools(\n",
    "            tools=self.tools, \n",
    "            user_msg=query, \n",
    "            chat_history=temp_chat_history[:-1],\n",
    "        )\n",
    "        tool_calls = self.llm.get_tool_calls_from_response(\n",
    "            response, error_on_no_tool_call=False\n",
    "        )\n",
    "\n",
    "        while tool_calls:\n",
    "            temp_chat_history.append(response.message)\n",
    "            # If any tools are called, run with the tools until we hit the Alpha tool and return\n",
    "            for tool_call in tool_calls:\n",
    "                print(f'[internal log] Called {tool_call.tool_name} tool, with arguments: {tool_call.tool_kwargs}')\n",
    "                tool = self._tools_map[tool_call.tool_name]\n",
    "                tool_kwargs = tool_call.tool_kwargs\n",
    "                tool_output = tool(**tool_kwargs)\n",
    "                temp_chat_history.append(ChatMessage(role=\"tool\", content=str(tool_output), additional_kwargs={\"tool_call_id\": tool_call.tool_id}))\n",
    "                \n",
    "                response = self.llm.chat_with_tools([tool], chat_history=temp_chat_history)\n",
    "                print(f'[internal log] Tool response: {response.message.content}')\n",
    "                tool_calls = self.llm.get_tool_calls_from_response(\n",
    "                    response, error_on_no_tool_call=False\n",
    "                )\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example tool: get_todays_date\n",
    "\n",
    "Let's define an example tool `get_todays_date()` that our RAG app can rely on. LlamaIndex makes this much easier - just write normal Python functions and it automatically: reads your function name and docstring, understands your parameters and type hints, and creates the LLM-friendly format for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_todays_date(date_format: str) -> str:\n",
    "  \"A tool that returns today's date in the date format requested. Options for date_format parameter are: '%Y-%m-%d', '%d', '%m', '%Y'.\"\n",
    "  datetime_str = datetime.now().strftime(date_format)\n",
    "  return datetime_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update our LLM system prompt with tool call instructions\n",
    "\n",
    "For the best performance, **add instructions on when to use the tool into the system prompt** that governs your LLM. Below we simply added Step **3.** in our list of instructions, which are otherwise represent a typical RAG system prompt. In most RAG apps, one instructs the LLM on what `fallback_answer` to respond with when it does not know how to answer a user's query. Such fallback instructions help you reduce hallucinations and more precisely control the AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fallback_answer = \"Based on the available information, I cannot provide a complete answer to this question.\"\n",
    "\n",
    "system_message = f\"\"\"\n",
    "    Answer the user's Question based on the following possibly relevant Context. Follow these rules:\n",
    "    1. Never use phrases like \"according to the context,\" \"as the context states,\" etc. Treat the Context as your own knowledge, not something you are referencing.\n",
    "    2. Give a clear, short, and accurate answer. Explain complex terms if needed.\n",
    "    3. If the answer to the question requires today's date, use the following tool: get_todays_date.\n",
    "    4. If the Context doesn't adequately address the Question, say: \"{fallback_answer}\" only, nothing else.\n",
    "\n",
    "    Remember, your purpose is to provide information based on the Context, not to offer original advice.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize our RAG App\n",
    "Finally, let's set up our LLM that supports tool calling and initialize our RAG App. Any LlamaIndex-compatible LLM can be used here, as long as it supports tool calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=model)  # API key can be set via OPENAI_API_KEY environment variable or .env file\n",
    "chat_history = [\n",
    "    ChatMessage(role=\"system\", content=system_message),\n",
    "]\n",
    "tools = [FunctionTool.from_defaults(fn=get_todays_date)]  # Add your tools here\n",
    "\n",
    "rag = RAGApp(llm=llm, tools=tools, retriever=retriever, messages=chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG in action\n",
    "\n",
    "Let's run our RAG application over different questions commonly asked by users about the *Simple Water Bottle* in our example.\n",
    "\n",
    "### Scenario 1: RAG can answer the question without tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[internal log] Invoking LLM with prompt\n",
      "  Context:\n",
      "  Simple Water Bottle - Amber (limited edition launched Jan 1st 2025) \n",
      "  \n",
      "  A water bottle designed with a perfect blend of functionality and aesthetics in mind. Crafted from high-quality, durable plastic with a sleek honey-colored finish.\n",
      "  Price: $24.99 \n",
      "  Dimensions: 10 inches height x 4 inches width\n",
      "  \n",
      "  User Question:\n",
      "  How big is the water bottle?\n",
      "\n",
      "\n",
      "\n",
      "[RAG response] The water bottle is 10 inches in height and 4 inches in width.\n"
     ]
    }
   ],
   "source": [
    "response = rag(\"How big is the water bottle?\")\n",
    "print(f'\\n[RAG response] {response.message.content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the LLM was able to provide a good answer because the retrieved context contains the necessary information.\n",
    "\n",
    "### Scenario 2: RAG can answer the question using tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[internal log] Invoking LLM with prompt\n",
      "  Context:\n",
      "  Simple Water Bottle - Amber (limited edition launched Jan 1st 2025) \n",
      "  \n",
      "  A water bottle designed with a perfect blend of functionality and aesthetics in mind. Crafted from high-quality, durable plastic with a sleek honey-colored finish.\n",
      "  Price: $24.99 \n",
      "  Dimensions: 10 inches height x 4 inches width\n",
      "  \n",
      "  User Question:\n",
      "  Has the limited edition Amber water bottle already launched?\n",
      "\n",
      "\n",
      "[internal log] Called get_todays_date tool, with arguments: {'date_format': '%Y-%m-%d'}\n",
      "[internal log] Tool response: Yes, the limited edition Amber water bottle launched on January 15, 2025.\n",
      "\n",
      "[RAG response] Yes, the limited edition Amber water bottle launched on January 15, 2025.\n"
     ]
    }
   ],
   "source": [
    "response = rag(\"Has the limited edition Amber water bottle already launched?\")\n",
    "print(f'\\n[RAG response] {response.message.content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the LLM chose to call our `get_todays_date` tool to obtain information necessary for properly answering the user's query. Note that a proper answer to this question also requires considering information from the retrieved context as well.\n",
    "\n",
    "### Scenario 3: RAG cannot answer the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[internal log] Invoking LLM with prompt\n",
      "  Context:\n",
      "  Simple Water Bottle - Amber (limited edition launched Jan 1st 2025) \n",
      "  \n",
      "  A water bottle designed with a perfect blend of functionality and aesthetics in mind. Crafted from high-quality, durable plastic with a sleek honey-colored finish.\n",
      "  Price: $24.99 \n",
      "  Dimensions: 10 inches height x 4 inches width\n",
      "  \n",
      "  User Question:\n",
      "  Can I return my simple water bottle?\n",
      "\n",
      "\n",
      "\n",
      "[RAG response] Based on the available information, I cannot provide a complete answer to this question.\n"
     ]
    }
   ],
   "source": [
    "response = rag(\"Can I return my simple water bottle?\")\n",
    "print(f'\\n[RAG response] {response.message.content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Context does not contain information about the return policy, and the `get_todays_date` tool would not help either.\n",
    "In this case, we want to return our fallback response to the user.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Adding tool calls to your RAG system expands the capabilities of what your AI can do and the types of questions it can answer.\n",
    "\n",
    "Once you have a RAG app with tools set up, adding **Codex as-a-Tool** takes only a few lines of code.\n",
    "Codex enables your RAG app to answer questions it previously could not (like Scenario 3 above). Learn how via our tutorial: [Integrate Codex as-a-Tool with LlamaIndex](/codex/tutorials/llama/LlamaIndex_ToolCalls/).\n",
    "\n",
    "Need help? Check the [FAQ](/codex/FAQ/) or email us at: support@cleanlab.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}