{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate Codex-as-a-tool into OpenAI Assistants\n",
    "\n",
    "This notebook assumes you have an OpenAI Assistants application that can handle tool calls. Learn how to build such an app via our tutorial: [Agentic RAG with OpenAI Assistants](/codex/tutorials/openai/OpenAIAssistants_ToolCalls/) tutorial.\n",
    "\n",
    "Once you have an OpenAI Assistant running, **adding Codex as an additional tool takes minimal effort but leads to guaranteed performance increase**. \n",
    "\n",
    "![RAG Workflow](../assets/codexastool_retrievalfirst.png)\n",
    "\n",
    "If you prefer to implement Codex without adding tool calls to your Assistant, check out our tutorial: [Integrate Codex as-a-Backup with OpenAI Assistants](/codex/tutorials/openai/OpenAIAssistants_CodexAsBackup/).\n",
    "\n",
    "Let's first install packages required for this tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai  # we used package-version 1.59.7\n",
    "%pip install --upgrade cleanlab_codex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7675fdb9-daf1-40f4-9ab0-a409e9cf2d90",
   "metadata": {},
   "source": [
    "**Optional: Helper methods for an OpenAI Assistants app from prior tutorial (Agentic RAG with OpenAI Assistants)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "from io import BytesIO\n",
    "import json\n",
    "\n",
    "from openai.types.beta.threads import Run\n",
    "from openai.types.beta.assistant import Assistant\n",
    "from openai.types.beta.assistant_tool_param import AssistantToolParam\n",
    "from openai.types.beta.thread import Thread\n",
    "from openai.types.beta.threads.run import Run as RunObject\n",
    "from openai.types.beta.threads.message_content import MessageContent\n",
    "from openai.types.beta.threads.run_submit_tool_outputs_params import ToolOutput\n",
    "import os\n",
    "\n",
    "\n",
    "fallback_answer = \"Based on the available information, I cannot provide a complete answer to this question.\"\n",
    "\n",
    "system_prompt_without_codex = f\"\"\"\n",
    "    Answer the user's Question based on the following possibly relevant Context. Follow these rules:\n",
    "    1. Never use phrases like \"according to the context,\" \"as the context states,\" etc. Treat the Context as your own knowledge, not something you are referencing.\n",
    "    2. Give a clear, short, and accurate answer. Explain complex terms if needed.\n",
    "    3. If the answer to the question requires today's date, use the following tool: get_todays_date.\n",
    "    4. If the Context doesn't adequately address the Question, say: \"{fallback_answer}\" only, nothing else.\n",
    "\n",
    "    Remember, your purpose is to provide information based on the Context, not to offer original advice.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_todays_date(date_format: str) -> str:\n",
    "  \"\"\"A tool that returns today's date in the date format requested.\"\"\"\n",
    "  datetime_str = datetime.now().strftime(date_format)\n",
    "  return datetime_str\n",
    "\n",
    "todays_date_tool_json = {\n",
    "  \"type\": \"function\",\n",
    "  \"function\": {\n",
    "    \"name\": \"get_todays_date\",\n",
    "    \"description\": \"A tool that returns today's date in the date format requested. Options are: 'YYYY-MM-DD', 'DD', 'MM', 'YYYY'.\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"date_format\": {\n",
    "          \"type\": \"string\",\n",
    "          \"enum\": [\"%Y-%m-%d\", \"%d\", \"%m\", \"%Y\"],\n",
    "          \"default\": \"%Y-%m-%d\",\n",
    "          \"description\": \"The date format to return today's date in.\"\n",
    "        }\n",
    "      },\n",
    "      \"required\": [\"date_format\"],\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "DEFAULT_FILE_SEARCH: AssistantToolParam = {\"type\": \"file_search\"}\n",
    "\n",
    "def create_rag_assistant(client: OpenAI, instructions: str, tools: list[AssistantToolParam]) -> Assistant:\n",
    "    \"\"\"Create and configure a RAG-enabled assistant.\"\"\"\n",
    "\n",
    "    assert any(tool[\"type\"] == \"file_search\" for tool in tools), \"File search tool is required\"\n",
    "    \n",
    "    return client.beta.assistants.create(\n",
    "        name=\"RAG Assistant\",\n",
    "        instructions=instructions,\n",
    "        model=\"gpt-4o-mini\",\n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_documents(client: OpenAI):\n",
    "    # Create a vector store\n",
    "    vector_store = client.beta.vector_stores.create(name=\"Simple Context\")\n",
    "\n",
    "    # This is a highly simplified way to provide document content\n",
    "    # In a real application, you would likely:\n",
    "    # - Read documents from files on disk\n",
    "    # - Download documents from a database or cloud storage\n",
    "    # - Process documents from various sources (PDFs, web pages, etc.)\n",
    "    \n",
    "    documents = {\n",
    "        \"simple_water_bottle.txt\": \"Simple Water Bottle - Amber (limited edition launched Jan 1st 2025)\\n\\nA water bottle designed with a perfect blend of functionality and aesthetics in mind. Crafted from high-quality, durable plastic with a sleek honey-colored finish.\\n\\nPrice: $24.99 \\nDimensions: 10 inches height x 4 inches width\",\n",
    "    }\n",
    "\n",
    "    # Ready the files for upload to OpenAI\n",
    "    file_objects = []\n",
    "    for doc_name, doc_content in documents.items():\n",
    "        # Create BytesIO object from document content\n",
    "        file_object = BytesIO(doc_content.encode(\"utf-8\"))\n",
    "        file_object.name = doc_name\n",
    "        file_objects.append(file_object)\n",
    "\n",
    "    # Upload files to vector store\n",
    "    client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "        vector_store_id=vector_store.id,\n",
    "        files=file_objects\n",
    "    )\n",
    "    \n",
    "    return vector_store\n",
    "\n",
    "class ToolRegistry:\n",
    "    \"\"\"Registry for tool implementations\"\"\"\n",
    "    def __init__(self):\n",
    "        self._tools = {}\n",
    "    \n",
    "    def register_tool(self, tool_name: str, handler):\n",
    "        \"\"\"Register a tool handler function\"\"\"\n",
    "        self._tools[tool_name] = handler\n",
    "        \n",
    "    def get_handler(self, tool_name: str):\n",
    "        \"\"\"Get the handler for a tool\"\"\"\n",
    "        return self._tools.get(tool_name)\n",
    "        \n",
    "    def __contains__(self, tool_name: str) -> bool:\n",
    "        \"\"\"Allow using 'in' operator to check if tool exists\"\"\"\n",
    "        return tool_name in self._tools\n",
    "\n",
    "class RAGChat:\n",
    "    def __init__(self, client: OpenAI, assistant_id: str, tool_registry: ToolRegistry):\n",
    "        self.client = client\n",
    "        self.assistant_id = assistant_id\n",
    "        self.tool_registry = tool_registry\n",
    "\n",
    "        # Create a thread for the conversation\n",
    "        self.thread: Thread = self.client.beta.threads.create()\n",
    "\n",
    "    def _handle_tool_calls(self, run: RunObject) -> list[ToolOutput]:\n",
    "        \"\"\"Handle tool calls from the assistant.\"\"\"\n",
    "        if not run.required_action or not run.required_action.submit_tool_outputs:\n",
    "            return []\n",
    "            \n",
    "        tool_outputs: list[ToolOutput] = []\n",
    "        for tool_call in run.required_action.submit_tool_outputs.tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            \n",
    "            if function_name in self.tool_registry:\n",
    "                print(f\"[internal log] Calling tool: {function_name} with args: {function_args}\")\n",
    "                handler = self.tool_registry.get_handler(function_name)\n",
    "                if handler is None:\n",
    "                    raise ValueError(f\"No handler found for called tool: {function_name}\")\n",
    "                output = handler(**function_args)\n",
    "            else:\n",
    "                output = f\"Unknown tool: {function_name}\"\n",
    "                \n",
    "            tool_outputs.append({\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"output\": output\n",
    "            })\n",
    "        \n",
    "        return tool_outputs\n",
    "\n",
    "    def _get_message_text(self, content: MessageContent) -> str:\n",
    "        \"\"\"Extract text from message content.\"\"\"\n",
    "        if hasattr(content, 'text'):\n",
    "            return content.text.value\n",
    "        return \"Error: Message content is not text\"\n",
    "\n",
    "    def chat(self, user_message: str) -> str:\n",
    "        \"\"\"Process a user message and return the assistant's response.\"\"\"\n",
    "        # Add the user message to the thread\n",
    "        self.client.beta.threads.messages.create(\n",
    "            thread_id=self.thread.id,\n",
    "            role=\"user\",\n",
    "            content=user_message\n",
    "        )\n",
    "\n",
    "        # Create a run\n",
    "        run: Run = self.client.beta.threads.runs.create(\n",
    "            thread_id=self.thread.id,\n",
    "            assistant_id=self.assistant_id\n",
    "        )\n",
    "\n",
    "        # Wait for run to complete and handle any tool calls\n",
    "        while True:\n",
    "            run = self.client.beta.threads.runs.retrieve(\n",
    "                thread_id=self.thread.id,\n",
    "                run_id=run.id\n",
    "            )\n",
    "            \n",
    "            if run.status == \"requires_action\":\n",
    "                # Handle tool calls\n",
    "                tool_outputs = self._handle_tool_calls(run)\n",
    "                \n",
    "                # Submit tool outputs\n",
    "                run = self.client.beta.threads.runs.submit_tool_outputs(\n",
    "                    thread_id=self.thread.id,\n",
    "                    run_id=run.id,\n",
    "                    tool_outputs=tool_outputs\n",
    "                )\n",
    "                \n",
    "            elif run.status == \"completed\":\n",
    "                # Get the latest message\n",
    "                messages = self.client.beta.threads.messages.list(\n",
    "                    thread_id=self.thread.id\n",
    "                )\n",
    "                if messages.data:\n",
    "                    return self._get_message_text(messages.data[0].content[0])\n",
    "                return \"Error: No messages found\"\n",
    "                \n",
    "            elif run.status in [\"failed\", \"expired\"]:\n",
    "                return f\"Error: Run {run.status}\"\n",
    "\n",
    "def add_vector_store_to_assistant(client: OpenAI, assistant, vector_store):\n",
    "    assistant = client.beta.assistants.update(\n",
    "        assistant_id=assistant.id,\n",
    "        tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    "    )\n",
    "    return assistant\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example RAG App: Product Customer Support\n",
    "\n",
    "Let's revisit our OpenAI Assistants application built in the tutorial: [Agentic RAG with OpenAI Assistants](/codex/tutorials/openai/OpenAIAssistants_ToolCalls/), which has the option to call a `get_todays_date()` tool. This example represents a customer support / e-commerce use-case where the Knowledge Base contains product listings like the following:\n",
    "\n",
    "![Simple water bottle product listing](../assets/simple_water_bottle.png)\n",
    "\n",
    "For simplicity, our Assistant's Knowledge Base here only contains a single document featuring this one product description.\n",
    "\n",
    "Lets intialize our OpenAI client, and then integrate Codex to improve this Assistant's responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR-KEY-HERE>\"  # Replace with your OpenAI API key\n",
    "model = \"gpt-4o\"  # model used by RAG system\n",
    "    \n",
    "client = OpenAI()  # API key is read from the OPENAI_API_KEY environment variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Codex Project\n",
    "\n",
    "To use Codex, first [create a Project](/codex/sme_tutorials/getting_started/).\n",
    "\n",
    "Here we assume some common (question, answer) pairs about the *Simple Water Bottle* have already been added to a Codex Project.\n",
    "Learn how that was done via our tutorial: [Populating Codex](/codex/sme_tutorials/populating_codex/).\n",
    "\n",
    "Our existing Codex Project contains the following entries:\n",
    "\n",
    "![Codex Knowledge Base Example](../assets/codex_kb.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_key = \"<YOUR-PROJECT-ACCESS-KEY>\"  # Obtain from your Project's settings page: https://codex.cleanlab.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Programmatically populate the Codex Project with above (question, answer) pairs. Note: The recommended flow is to do this manually in the Web App.\n",
    "from cleanlab_codex.client import Client\n",
    "\n",
    "os.environ[\"CODEX_API_KEY\"] = \"<YOUR-KEY-HERE>\"  # Replace with your Codex API key\n",
    "codex_client = Client()\n",
    "\n",
    "# Create a project\n",
    "project = codex_client.create_project(\n",
    "    name=\"Product FAQs\",\n",
    "    description=\"Questions about product pages\",\n",
    ")\n",
    "\n",
    "# Add entries to the project\n",
    "project.add_entries(\n",
    "    entries=[\n",
    "        {\"question\": \"How much water can the Simple Water Bottle hold?\", \"answer\": \"32oz\"},\n",
    "        {\"question\": \"Can I return my Simple Water Bottle?\", \"answer\": \"Return it within 30 days for a full refund-- no questions asked. Contact our support team to initiate your return!\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "access_key = project.create_access_key(\"test access key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate Codex as an additional tool\n",
    "\n",
    "You only need to make minimal changes to your code to include Codex as an additional tool:\n",
    "1. Add Codex to the list of tools provided to the Assistant.\n",
    "2. Update your system prompt to include instructions for calling Codex, as demonstrated below in: `system_prompt_with_codex`.\n",
    "\n",
    "\n",
    "After that, call your original Assistant with these updated variables to start experiencing the benefits of Codex!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab_codex import CodexTool\n",
    "\n",
    "codex_tool = CodexTool.from_access_key(access_key=access_key, fallback_answer=fallback_answer)\n",
    "codex_tool_openai = codex_tool.to_openai_tool()\n",
    "\n",
    "# Add Codex to the list of available tools for our RAGChat class\n",
    "tool_registry = ToolRegistry()\n",
    "tool_registry.register_tool(codex_tool.tool_name, codex_tool.query)\n",
    "tool_registry.register_tool('get_todays_date', get_todays_date)  # register other tools here\n",
    "\n",
    "# Update the RAG system prompt with instructions for handling Codex (adjust based on your needs)\n",
    "system_prompt_with_codex = f\"\"\"For each question:\n",
    "\n",
    "1. Start with file_search tool\n",
    "2. If file_search results are incomplete/empty:\n",
    "   - Inform the user about insufficient file results\n",
    "   - Then use {codex_tool.tool_name} for additional information\n",
    "   - Present {codex_tool.tool_name} findings without citations\n",
    "\n",
    "Only use citations (\u3010source\u3011) for information found directly in files via file_search.\n",
    "Do not abstain from answering without trying both tools. When you do, say: \"{fallback_answer}\", nothing else.\n",
    "\"\"\"\n",
    "\n",
    "# Create Assistant that is integrated with Codex\n",
    "vector_store = load_documents(client)\n",
    "assistant = create_rag_assistant(client, system_prompt_with_codex, [DEFAULT_FILE_SEARCH, todays_date_tool_json, codex_tool_openai])\n",
    "assistant = add_vector_store_to_assistant(client, assistant, vector_store)\n",
    "rag_with_codex = RAGChat(client, assistant.id, tool_registry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6644d96-94de-44f2-bc78-fcf6a541fe86",
   "metadata": {},
   "source": [
    "**Optional: Create another version of the Assistant without Codex (`rag_without_codex`)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = load_documents(client)\n",
    "tool_registry = ToolRegistry()\n",
    "tool_registry.register_tool('get_todays_date', get_todays_date)\n",
    "assistant = create_rag_assistant(client, system_prompt_without_codex, [DEFAULT_FILE_SEARCH, todays_date_tool_json])\n",
    "assistant = add_vector_store_to_assistant(client, assistant, vector_store)\n",
    "rag_without_codex = RAGChat(client, assistant.id, tool_registry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This tutorial uses a Codex tool description provided in OpenAI format via the `to_openai_tool()` function. You can instead manually write the Codex tool description yourself or import it in alternate [provided formats](/reference/python/codex_tool/).\n",
    "\n",
    "In Agentic RAG systems like OpenAI Assistants, retrieval is treated as yet another tool call (called `file_search` for OpenAI Assistants). Our system prompt should carefully instruct how/when the AI should use the Codex tool vs. other available tools. For OpenAI Assistants, we recommend instructing the AI to only consider the Codex tool after it has used `file_search` and is still unsure how to answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG with Codex in action\n",
    "\n",
    "Integrating Codex as-a-Tool allows your RAG app to answer more questions than it was originally capable of.\n",
    "\n",
    "### Example 1\n",
    "\n",
    "Let's ask a question to our **original** RAG app (before Codex was integrated).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the available information, I cannot provide a complete answer to this question.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_question = \"Can I return my simple water bottle?\"\n",
    "rag_without_codex.chat(user_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **original RAG app is unable to answer**, in this case because the required information is not in its Knowledge Base.\n",
    "\n",
    "Let's ask the same question to our RAG app with Codex added as an additional tool. \n",
    "Note that we use the updated system prompt and tool list when Codex is integrated in the RAG app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[internal log] Calling tool: consult_codex with args: {'question': 'Can I return my simple water bottle?'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You can return your simple water bottle within 30 days for a full refund, no questions asked. To initiate the return, you should contact the support team.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_with_codex.chat(user_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, **integrating Codex enables your RAG app to answer questions it originally strugged with**, as long as a similar question was already answered in the corresponding Codex Project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "Let's ask another question to our RAG app with Codex integrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[internal log] Calling tool: consult_codex with args: {'question': 'How can I order the Simple Water Bottle in bulk?'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based on the available information, I cannot provide a complete answer to this question.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_question = \"How can I order the Simple Water Bottle in bulk?\"\n",
    "rag_with_codex.chat(user_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our RAG app is unable to answer this question because there is no relevant information in its Knowledge Base, nor has a similar question been answered in the Codex Project (see the contents of the Codex Project above).\n",
    "\n",
    "**Codex automatically recognizes this question could not be answered and logs it into the Project where it awaits an answer from a SME**.\n",
    "Navigate to your Codex Project in the [Web App](https://codex.cleanlab.ai/) where you (or a SME at your company) can enter the desired answer for this query.\n",
    "\n",
    "![Codex Project with asked question that has not been answered yet](../assets/codex_kb_unanswered.png)\n",
    "\n",
    "As soon as an answer is provided in Codex, our RAG app will be able to answer all similar questions going forward (as seen for the previous query)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3\n",
    "\n",
    "Let's ask another query to our RAG app with Codex integrated. This is a query the  original RAG app was able to correctly answer without Codex (since the relevant information exists in the Knowledge Base)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The size of the Simple Water Bottle is 10 inches in height and 4 inches in width\u301018:0\u2020source\u3011.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_question = \"How big is the water bottle?\"\n",
    "rag_with_codex.chat(user_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the RAG app with Codex integrated is still able to correctly answer this query. **Integrating Codex has no negative effect on questions your original RAG app could answer**.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that Codex is integrated with your RAG app, you and SMEs can [open the Codex Project and answer questions](/codex/sme_tutorials/codex_as_sme/) logged there to continuously improve your AI.\n",
    "\n",
    "**Adding Codex only improves your AI Assistant.** Once integrated, Codex automatically logs all user queries that your original AI Assistant handles poorly. Using a [simple web interface](/codex/sme_tutorials/codex_as_sme/), SMEs at your company can answer the highest priority questions in the Codex Project. As soon as an answer is entered in Codex, your AI Assistant will be able to properly handle all similar questions encountered in the future.\n",
    "\n",
    "Codex is **the fastest way for nontechnical SMEs to directly improve your AI application**. As the Developer, you simply integrate Codex once, and from then on, SMEs can continuously improve how your AI handles common user queries without needing your help.\n",
    "\n",
    "Need help, more capabilities, or other deployment options? \n",
    "Check the [FAQ](/codex/FAQ/) or email us at: support@cleanlab.ai"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}