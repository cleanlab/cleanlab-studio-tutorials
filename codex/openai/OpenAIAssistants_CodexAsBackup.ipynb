{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate Codex as-a-Backup with OpenAI Assistants\n",
    "\n",
    "This tutorial demonstrates how to integrate Codex [as-a-Backup](/codex/concepts/integrations/) with a RAG app built using [OpenAI Assistants](https://platform.openai.com/docs/assistants/overview).\n",
    "\n",
    "This is our recommended integration strategy for developers using OpenAI Assistants. The integration is **only a few lines of code** and offers more control than integrating Codex [as-a-Tool](/codex/concepts/integrations/).\n",
    "\n",
    "When integrating Codex as-a-Backup, you can automatically detect problematic RAG responses - see the advanced usage section of our [validator tutorial](/codex/tutorials/other_rag_frameworks/validator/#advanced-usage) for an in-depth look at these detection methods.\n",
    "\n",
    "![RAG Workflow](../assets/codexasbackup.png)\n",
    "\n",
    "Let's first install packages required for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai  # we used package-version 1.59.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade cleanlab_codex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()  # API key is read from the OPENAI_API_KEY environment variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example RAG App: Customer Service for a New Product\n",
    "\n",
    "Consider a customer support use-case, where the RAG application is built on a Knowledge Base with product pages such as the following:\n",
    "\n",
    "![Image of a beautiful simple water bottle that is definitely worth more than the asking price](../assets/simple_water_bottle.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG with OpenAI Assistants\n",
    "\n",
    "Let's set up our Assistant! To keep this example simple, our Assistant's Knowledge Base only has a single document containing the description of the product listed above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af709986-4586-4822-a765-3eb38f20e3cf",
   "metadata": {},
   "source": [
    "**Optional: Helper functions to set up an OpenAI Assistant**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from io import BytesIO\n",
    "\n",
    "from openai.types.beta.threads import Run\n",
    "from openai.types.beta.assistant import Assistant\n",
    "\n",
    "\n",
    "def create_rag_assistant(client: OpenAI, instructions: str) -> Assistant:\n",
    "    \"\"\"Create and configure a RAG-enabled Assistant.\"\"\"\n",
    "    return client.beta.assistants.create(\n",
    "        name=\"RAG Assistant\",\n",
    "        instructions=instructions,  # System prompt that governs the Assistant\n",
    "        model=\"gpt-4o-mini\",\n",
    "        tools=[{\"type\": \"file_search\"}],  # OpenAI Assistants is an agentic RAG framework that treats retrieval as a Tool called file_search \n",
    "    )\n",
    "\n",
    "\n",
    "def load_documents(client: OpenAI):\n",
    "    \"\"\"A highly simplified way to populate our Assistant's Knowledge Base. You can replace this toy example with many heterogeneous document files (PDFs, web pages, ...).\"\"\"\n",
    "    vector_store = client.beta.vector_stores.create(name=\"Simple Context\")\n",
    "\n",
    "    documents = {\n",
    "        \"simple_water_bottle.txt\": \"Simple Water Bottle - Amber (limited edition launched Jan 1st 2025)\\n\\nA water bottle designed with a perfect blend of functionality and aesthetics in mind. Crafted from high-quality, durable plastic with a sleek honey-colored finish.\\n\\nPrice: $24.99 \\nDimensions: 10 inches height x 4 inches width\",\n",
    "    }  # our toy example only has one short document\n",
    "\n",
    "    # Upload documents to OpenAI\n",
    "    file_objects = []\n",
    "    for name, content in documents.items():\n",
    "        file_object = BytesIO(content.encode(\"utf-8\"))\n",
    "        file_object.name = name\n",
    "        file_objects.append(file_object)\n",
    "\n",
    "    client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "        vector_store_id=vector_store.id,\n",
    "        files=file_objects\n",
    "    )\n",
    "    return vector_store\n",
    "\n",
    "\n",
    "def add_vector_store_to_assistant(client: OpenAI, assistant, vector_store):\n",
    "    assistant = client.beta.assistants.update(\n",
    "        assistant_id=assistant.id,\n",
    "        tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    "    )\n",
    "    return assistant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have defined basic functionality to setup our Assistant, let's implement a standard RAG app using the OpenAI Assistants API.\n",
    "Our application will be conversational, supporting multi-turn dialogues. A new dialogue (i.e. *thread*) is instantiated as a `RAGChat` object defined below.\n",
    "To have the Assistant respond to each user message in the dialogue, simply call this object's `chat()` method. The `RAGChat` class properly manages conversation history, retrieval, and LLM response-generation via the OpenAI Assistants API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06559af-0eea-4284-8bf0-4ec933bb43ad",
   "metadata": {},
   "source": [
    "**Optional: RAGChat class to orchestrate each conversation with our Assistant**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RAGChat:\n",
    "    def __init__(self, client: OpenAI, assistant_id: str):\n",
    "        self.client = client\n",
    "        self.assistant_id = assistant_id\n",
    "        self.thread_id = self.client.beta.threads.create().id\n",
    "\n",
    "\n",
    "    def chat(self, user_message: str) -> str:\n",
    "        \"\"\"Process a user message and return the assistant's response.\"\"\"\n",
    "        # Add the user message to the thread\n",
    "        self.client.beta.threads.messages.create(\n",
    "            thread_id=self.thread_id,\n",
    "            role=\"user\",\n",
    "            content=user_message\n",
    "        )\n",
    "\n",
    "        # Invoke the assistant on the current thread\n",
    "        run: Run = self.client.beta.threads.runs.create_and_poll(\n",
    "            thread_id=self.thread_id,\n",
    "            assistant_id=self.assistant_id,\n",
    "        )\n",
    "        \n",
    "        # Display the assistant's response (basic example, modify it as necessary for settings like token streaming)\n",
    "        messages = list(self.client.beta.threads.messages.list(thread_id=self.thread_id, run_id=run.id))\n",
    "\n",
    "        message_content = messages[0].content[0].text\n",
    "        annotations = message_content.annotations\n",
    "        for index, annotation in enumerate(annotations):\n",
    "            message_content.value = message_content.value.replace(annotation.text, f\"[{index}]\")\n",
    "        \n",
    "        return message_content.value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use these helper methods to instantiate an Assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest files and load them into a Knowledge Base\n",
    "vector_store = load_documents(client)\n",
    "\n",
    "# Define instructions the Assistant should generally follow\n",
    "fallback_answer = \"Based on the available information, I cannot provide a complete answer to this question.\"\n",
    "system_message = f\"\"\"Do not make up answers to questions if you cannot find the necessary information.\n",
    "If you remain unsure how to accurately respond to the user after considering the available information and tools, then only respond with: \"{fallback_answer}\".\n",
    "\"\"\"\n",
    "\n",
    "# Create assistant and connect our vector store for file search (i.e. retrieval)\n",
    "assistant = create_rag_assistant(client, system_message)\n",
    "assistant = add_vector_store_to_assistant(client, assistant, vector_store)\n",
    "\n",
    "# Create RAG app to chat with this Assistant\n",
    "rag = RAGChat(client, assistant.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you can chat with the Assistant via: `rag.chat(your_query)` as shown below. Before we demonstrate that, let's first see how easy it is to integrate Codex.\n",
    "\n",
    "\n",
    "## Create Codex Project\n",
    "\n",
    "To use Codex, first [create a Project](/codex/web_tutorials/create_project/).\n",
    "\n",
    "Here we assume some common (question, answer) pairs about the *Simple Water Bottle* have already been added to a Codex Project.\n",
    "To learn how that was done, see our tutorial: [Populating Codex](/codex/web_tutorials/populating_codex/).\n",
    "\n",
    "Our existing Codex Project contains the following entries:\n",
    "\n",
    "![Codex Knowledge Base Example](../assets/codex_kb.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Programmatically populate the Codex Project with above (question, answer) pairs. Note: The recommended flow is to do this manually in the Web App.\n",
    "from cleanlab_codex.client import Client\n",
    "import os\n",
    "\n",
    "os.environ[\"CODEX_API_KEY\"] = \"<YOUR-KEY-HERE>\"  # Replace with your Codex API key\n",
    "codex_client = Client()\n",
    "\n",
    "# Create a project\n",
    "project = codex_client.create_project(\n",
    "    name=\"Product FAQs\",\n",
    "    description=\"Questions about product pages\",\n",
    ")\n",
    "\n",
    "# Add entries to the project\n",
    "project.add_entries(\n",
    "    entries=[\n",
    "        {\"question\": \"How much water can the Simple Water Bottle hold?\", \"answer\": \"32oz\"},\n",
    "        {\"question\": \"Can I return my Simple Water Bottle?\", \"answer\": \"Return it within 30 days for a full refund-- no questions asked. Contact our support team to initiate your return!\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "os.environ[\"CODEX_ACCESS_KEY\"] = project.create_access_key(\"test access key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate Codex as-a-Backup\n",
    "\n",
    "RAG apps unfortunately sometimes produce bad/unhelpful responses.\n",
    "Instead of providing these to your users, add Codex as a backup system that can automatically detect these cases and provide better answers.\n",
    "\n",
    "Integrating [Codex as-a-Backup](/codex/concepts/integrations/) just requires two steps:\n",
    "1. Configure the Codex backup system with your Codex Project credentials and settings that control what sort of responses are detected to be bad.\n",
    "2. Enhance your RAG app to:\n",
    "   - Use Codex to monitor whether each Assistant response is bad.\n",
    "   - Query Codex for a better answer when needed.\n",
    "   - Update the conversation with Codex's answer when needed.\n",
    "\n",
    "After that, call your enhanced RAG app just like the original app - Codex works automatically in the background.\n",
    "\n",
    "Below is all the code needed to integrate Codex.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1199c402-025f-420f-9f47-2c180dcf954d",
   "metadata": {},
   "source": [
    "**Optional: RAGChat subclass that integates Codex as-a-Backup (RAGChatWithCodexBackup)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Any, Dict, Optional\n",
    "from cleanlab_codex import Project\n",
    "from cleanlab_codex.response_validation import is_bad_response\n",
    "\n",
    "class RAGChatWithCodexBackup(RAGChat):\n",
    "    \"\"\"Determines when to rely on Codex as-a-Backup based on `cleanlab_codex.response_validation.is_bad_response()`. Keyword arguments for this method can be provided when instantiating this object via: `is_bad_response_config`.\"\"\" \n",
    "    def __init__(\n",
    "        self,\n",
    "        client: OpenAI,\n",
    "        assistant_id: str,\n",
    "        codex_access_key: str,\n",
    "        is_bad_response_config: Optional[Dict[str, Any]] = None,\n",
    "    ):\n",
    "        super().__init__(client, assistant_id)\n",
    "        self._codex_project = Project.from_access_key(codex_access_key)\n",
    "        self._is_bad_response_config = is_bad_response_config\n",
    "\n",
    "    def _replace_latest_message(self, new_message: str) -> None:\n",
    "        \"\"\"Updates the latest assistant message in the thread with the backup response from Codex\"\"\"\n",
    "        client: OpenAI = self.client\n",
    "        thread_id: str = self.thread_id\n",
    "        \n",
    "        messages = client.beta.threads.messages.list(\n",
    "            thread_id=thread_id\n",
    "        ).data\n",
    "        latest_message = messages[0]\n",
    "        \n",
    "        client.beta.threads.messages.delete(\n",
    "            thread_id=thread_id,\n",
    "            message_id=latest_message.id,\n",
    "        )\n",
    "        client.beta.threads.messages.create(\n",
    "            thread_id=thread_id,\n",
    "            content=new_message,\n",
    "            role=\"assistant\",\n",
    "        )\n",
    "\n",
    "    def chat(self, user_message: str) -> str:\n",
    "        response = super().chat(user_message)\n",
    "\n",
    "        kwargs = {\"response\": response, \"query\": user_message}\n",
    "        if self._is_bad_response_config is not None:\n",
    "            kwargs[\"config\"] = self._is_bad_response_config\n",
    "\n",
    "        if is_bad_response(**kwargs):\n",
    "            codex_response: str | None = self._codex_project.query(user_message)[0]\n",
    "\n",
    "            if codex_response is not None:\n",
    "                # You may prefer to utilize Codex answers differently in your app than done here\n",
    "                self._replace_latest_message(codex_response)\n",
    "                response = codex_response\n",
    "\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codex **automatically detects** when your RAG app needs backup.\n",
    "Here we provide a basic configuration for this detection that relies on the fallback answer that we instructed the Assistant to output whenever it doesn't know how to respond accurately. With this configuration, Codex will be consulted as-a-Backup whenever your Assistant's response is estimated to be unhelpful.\n",
    "\n",
    "Learn more about available detection methods and configurations via our tutorial: [Validator - Advanced Usage](/codex/tutorials/other_rag_frameworks/validator/#advanced-usage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CODEX_ACCESS_KEY\"] = \"<YOUR_PROJECT_ACCESS_KEY>\"  # Available from your Project's settings page at: https://codex.cleanlab.ai/\n",
    "\n",
    "is_bad_response_config = {\n",
    "    \"fallback_answer\": fallback_answer,\n",
    "}\n",
    "\n",
    "# Instantiate RAG app enhanced with Codex as-a-Backup\n",
    "rag_with_codex = RAGChatWithCodexBackup(\n",
    "    client=client,\n",
    "    assistant_id=assistant.id,\n",
    "    codex_access_key=os.environ[\"CODEX_ACCESS_KEY\"],\n",
    "    is_bad_response_config=is_bad_response_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG with Codex in action\n",
    "\n",
    "We can now ask user queries to our original RAG app (`rag`), as well as another version of this RAG app enhanced with Codex (`rag_with_codex`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n",
    "Let's ask a question to our **original** RAG app (before Codex was integrated).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the available information, I cannot provide a complete answer to this question.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_question = \"Can I return my simple water bottle?\"\n",
    "rag.chat(user_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **original RAG app is unable to answer**, in this case because the required information is not in its Knowledge Base.\n",
    "\n",
    "Let's ask the same question to the RAG app enhanced with Codex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return it within 30 days for a full refund-- no questions asked. Contact our support team to initiate your return!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_with_codex.chat(user_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, **integrating Codex enables your RAG app to answer questions it originally strugged with**, as long as a similar question was already answered in the corresponding Codex Project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "Let's ask another question to our RAG app with Codex integrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the available information, I cannot provide a complete answer to this question.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_question = \"How can I order the Simple Water Bottle in bulk?\"\n",
    "rag.chat(user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the available information, I cannot provide a complete answer to this question.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_with_codex.chat(user_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our RAG app is unable to answer this question because there is no relevant information in its Knowledge Base, nor has a similar question been answered in the Codex Project (see the contents of the Codex Project above).\n",
    "\n",
    "**Codex automatically recognizes this question could not be answered and logs it into the Project where it awaits an answer from a SME.**\n",
    "Navigate to your Codex Project in the [Web App](https://codex.cleanlab.ai/) where you (or a SME at your company) can enter the desired answer for this query.\n",
    "\n",
    "As soon as an answer is provided in Codex, our RAG app will be able to answer all similar questions going forward (as seen for the previous query)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3\n",
    "\n",
    "Let's ask another query to our two RAG apps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Simple Water Bottle has dimensions of 10 inches in height and 4 inches in width[0].'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_question = \"How big is the water bottle?\"\n",
    "rag.chat(user_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original RAG app was able to correctly answer without Codex (since the relevant information exists in the Knowledge Base)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Simple Water Bottle measures 10 inches in height and 4 inches in width[0].'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_with_codex.chat(user_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the RAG app with Codex integrated is still able to correctly answer this query. **Integrating Codex has no negative effect on questions your original RAG app could answer.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that Codex is integrated with your RAG app, you and SMEs can [open the Codex Project and answer questions](/codex/web_tutorials/codex_as_sme/) logged there to continuously improve your AI.\n",
    "\n",
    "This tutorial demonstrated how to easily integrate Codex as a backup system into any OpenAI Assistants application. **Unlike tool calls which are harder to control**, you can choose when to call Codex as a backup. For instance, you can use Codex to automatically detect whenever the Assistant produces hallucinations or unhelpful responses such as \"I don't know\".\n",
    "\n",
    "**Adding Codex only improves your RAG app.** Once integrated, Codex automatically logs all user queries that your original RAG app handles poorly. Using a [simple web interface](/codex/web_tutorials/codex_as_sme/), SMEs at your company can answer the highest priority questions in the Codex Project. As soon as an answer is entered in Codex, your RAG app will be able to properly handle all similar questions encountered in the future.\n",
    "\n",
    "Codex is **the fastest way for nontechnical SMEs to directly improve your AI Assistant**. As the Developer, you simply integrate Codex once, and from then on, SMEs can continuously improve how your Assistant handles common user queries without needing your help.\n",
    "\n",
    "Need help, more capabilities, or other deployment options?  \n",
    "Check the [FAQ](/codex/FAQ/) or email us at: support@cleanlab.ai"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}