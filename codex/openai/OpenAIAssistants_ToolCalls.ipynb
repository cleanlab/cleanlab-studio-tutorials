{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an agentic RAG app with OpenAI Assistants\n",
    "\n",
    "This tutorial demonstrates how to build an [OpenAI Assistants](https://platform.openai.com/docs/assistants/) application that can call tools, converse with users, and answer their questions based on a Knowledge Base of files you provide. The OpenAI Assistants framework implements agentic RAG, which treats Retrieval as a tool that can be called (rather than as a step hardcoded into every user interaction, as is done in standard RAG).\n",
    "\n",
    "Building such an Assistant is a prerequisite for our tutorial: [Integrate Codex-as-a-tool into OpenAI Assistants](/codex/tutorials/openai/OpenAIAssistants_AddingCodexAsTool/), which shows how to greatly improve any existing Assistant.\n",
    "\n",
    "![RAG Workflow](../assets/codexastool_retrievalastool_simple.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first install and setup the OpenAI client library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai  # we used package-version 1.59.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR-KEY-HERE>\"  # Replace with your OpenAI API key\n",
    "model = \"gpt-4o\"  # which LLM to use\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example RAG App: Product Customer Support\n",
    "\n",
    "Let's revisit our OpenAI Assistants application built in the tutorial: [Agentic RAG with OpenAI Assistants](/codex/tutorials/openai/OpenAIAssistants_ToolCalls/), which has the option to call a `get_todays_date()` tool. This example represents a customer support / e-commerce use-case where the Knowledge Base contains product listings like the following:\n",
    "\n",
    "![Simple water bottle product listing](../assets/simple_water_bottle.png)\n",
    "\n",
    "For simplicity, our Assistant's Knowledge Base here only contains a single document featuring this one product description.\n",
    "To build a RAG app with OpenAI Assistants: we load documents/files into a Knowledge Base (vector store), and then connect the Assistant to this Knowledge Base."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44bfd63-d4e9-49fe-ab14-fe81e1ba3a37",
   "metadata": {},
   "source": [
    "**Optional: Define helper methods for Knowledge Base creation and retreival**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import json\n",
    "\n",
    "from openai.types.beta.threads import Run\n",
    "from openai.types.beta.assistant import Assistant\n",
    "from openai.types.beta.assistant_tool_param import AssistantToolParam\n",
    "from openai.types.beta.thread import Thread\n",
    "from openai.types.beta.threads.run import Run as RunObject\n",
    "from openai.types.beta.threads.message_content import MessageContent\n",
    "from openai.types.beta.threads.run_submit_tool_outputs_params import ToolOutput\n",
    "\n",
    "DEFAULT_FILE_SEARCH: AssistantToolParam = {\"type\": \"file_search\"}\n",
    "\n",
    "def create_rag_assistant(client: OpenAI, instructions: str, tools: list[AssistantToolParam]) -> Assistant:\n",
    "    \"\"\"Create and configure a RAG-enabled assistant.\"\"\"\n",
    "\n",
    "    assert any(tool[\"type\"] == \"file_search\" for tool in tools), \"File search tool is required\"\n",
    "    \n",
    "    return client.beta.assistants.create(\n",
    "        name=\"RAG Assistant\",\n",
    "        instructions=instructions,\n",
    "        model=\"gpt-4o\",\n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_documents(client: OpenAI):\n",
    "    # Create a vector store\n",
    "    vector_store = client.beta.vector_stores.create(name=\"Simple Context\")\n",
    "\n",
    "    # This is a highly simplified way to provide document content\n",
    "    # In a real application, you would likely:\n",
    "    # - Read documents from files on disk\n",
    "    # - Download documents from a database or cloud storage\n",
    "    # - Process documents from various sources (PDFs, web pages, etc.)\n",
    "    \n",
    "    documents = {\n",
    "        \"simple_water_bottle.txt\": \"Simple Water Bottle - Amber (limited edition launched Jan 1st 2025)\\n\\nA water bottle designed with a perfect blend of functionality and aesthetics in mind. Crafted from high-quality, durable plastic with a sleek honey-colored finish.\\n\\nPrice: $24.99 \\nDimensions: 10 inches height x 4 inches width\",\n",
    "    }\n",
    "\n",
    "    # Ready the files for upload to OpenAI\n",
    "    file_objects = []\n",
    "    for doc_name, doc_content in documents.items():\n",
    "        # Create BytesIO object from document content\n",
    "        file_object = BytesIO(doc_content.encode(\"utf-8\"))\n",
    "        file_object.name = doc_name\n",
    "        file_objects.append(file_object)\n",
    "\n",
    "    # Upload files to vector store\n",
    "    client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "        vector_store_id=vector_store.id,\n",
    "        files=file_objects\n",
    "    )\n",
    "    \n",
    "    return vector_store\n",
    "\n",
    "def add_vector_store_to_assistant(client: OpenAI, assistant, vector_store):\n",
    "    assistant = client.beta.assistants.update(\n",
    "        assistant_id=assistant.id,\n",
    "        tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    "    )\n",
    "    return assistant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Chat App that supports Tool Calls\n",
    "\n",
    "We instantiate a typical chat application to interact with the Assistant.\n",
    "Each instance of our `RAGChat` class defined below manages a conversation thread (multi-turn user interaction), responding to each user message through its `chat` method.\n",
    "Our app handles tool calls for any tools registered via a `ToolRegistry` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a51e731-fc05-49c0-9b74-f907141096dd",
   "metadata": {},
   "source": [
    "**Optional: Define class for RAG chat with tools**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ToolRegistry:\n",
    "    \"\"\"Registry for tool implementations\"\"\"\n",
    "    def __init__(self):\n",
    "        self._tools = {}\n",
    "    \n",
    "    def register_tool(self, tool_name: str, handler):\n",
    "        \"\"\"Register a tool handler function\"\"\"\n",
    "        self._tools[tool_name] = handler\n",
    "        \n",
    "    def get_handler(self, tool_name: str):\n",
    "        \"\"\"Get the handler for a tool\"\"\"\n",
    "        return self._tools.get(tool_name)\n",
    "        \n",
    "    def __contains__(self, tool_name: str) -> bool:\n",
    "        \"\"\"Allow using 'in' operator to check if tool exists\"\"\"\n",
    "        return tool_name in self._tools\n",
    "\n",
    "class RAGChat:\n",
    "    def __init__(self, client: OpenAI, assistant_id: str, tool_registry: ToolRegistry):\n",
    "        self.client = client\n",
    "        self.assistant_id = assistant_id\n",
    "        self.tool_registry = tool_registry\n",
    "\n",
    "        # Create a thread for the conversation\n",
    "        self.thread: Thread = self.client.beta.threads.create()\n",
    "\n",
    "    def _handle_tool_calls(self, run: RunObject) -> list[ToolOutput]:\n",
    "        \"\"\"Handle tool calls from the assistant.\"\"\"\n",
    "        if not run.required_action or not run.required_action.submit_tool_outputs:\n",
    "            return []\n",
    "            \n",
    "        tool_outputs: list[ToolOutput] = []\n",
    "        for tool_call in run.required_action.submit_tool_outputs.tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            \n",
    "            if function_name in self.tool_registry:\n",
    "                print(f\"[internal log] Calling tool: {function_name} with args: {function_args}\")\n",
    "                handler = self.tool_registry.get_handler(function_name)\n",
    "                if handler is None:\n",
    "                    raise ValueError(f\"No handler found for called tool: {function_name}\")\n",
    "                output = handler(**function_args)\n",
    "            else:\n",
    "                output = f\"Unknown tool: {function_name}\"\n",
    "                \n",
    "            tool_outputs.append({\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"output\": output\n",
    "            })\n",
    "        \n",
    "        return tool_outputs\n",
    "\n",
    "    def _get_message_text(self, content: MessageContent) -> str:\n",
    "        \"\"\"Extract text from message content.\"\"\"\n",
    "        if hasattr(content, 'text'):\n",
    "            return content.text.value\n",
    "        return \"Error: Message content is not text\"\n",
    "\n",
    "    def chat(self, user_message: str) -> str:\n",
    "        \"\"\"Process a user message and return the assistant's response.\"\"\"\n",
    "        # Add the user message to the thread\n",
    "        self.client.beta.threads.messages.create(\n",
    "            thread_id=self.thread.id,\n",
    "            role=\"user\",\n",
    "            content=user_message\n",
    "        )\n",
    "\n",
    "        # Create a run\n",
    "        run: Run = self.client.beta.threads.runs.create(\n",
    "            thread_id=self.thread.id,\n",
    "            assistant_id=self.assistant_id\n",
    "        )\n",
    "\n",
    "        # Wait for run to complete and handle any tool calls\n",
    "        while True:\n",
    "            run = self.client.beta.threads.runs.retrieve(\n",
    "                thread_id=self.thread.id,\n",
    "                run_id=run.id\n",
    "            )\n",
    "            \n",
    "            if run.status == \"requires_action\":\n",
    "                # Handle tool calls\n",
    "                tool_outputs = self._handle_tool_calls(run)\n",
    "                \n",
    "                # Submit tool outputs\n",
    "                run = self.client.beta.threads.runs.submit_tool_outputs(\n",
    "                    thread_id=self.thread.id,\n",
    "                    run_id=run.id,\n",
    "                    tool_outputs=tool_outputs\n",
    "                )\n",
    "                \n",
    "            elif run.status == \"completed\":\n",
    "                # Get the latest message\n",
    "                messages = self.client.beta.threads.messages.list(\n",
    "                    thread_id=self.thread.id\n",
    "                )\n",
    "                if messages.data:\n",
    "                    return self._get_message_text(messages.data[0].content[0])\n",
    "                return \"Error: No messages found\"\n",
    "                \n",
    "            elif run.status in [\"failed\", \"expired\"]:\n",
    "                return f\"Error: Run {run.status}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example tool: get_todays_date\n",
    "\n",
    "Let's define an example tool `get_todays_date()` that our Assistant can rely on. Here we follow [OpenAI's JSON format](https://platform.openai.com/docs/guides/function-calling) for representing the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_todays_date(date_format: str) -> str:\n",
    "  \"A tool that returns today's date in the date format requested. Options are: 'YYYY-MM-DD', 'DD', 'MM', 'YYYY'.\"\n",
    "  datetime_str = datetime.now().strftime(date_format)\n",
    "  return datetime_str\n",
    "\n",
    "todays_date_tool_json = {\n",
    "  \"type\": \"function\",\n",
    "  \"function\": {\n",
    "    \"name\": \"get_todays_date\",\n",
    "    \"description\": \"A tool that returns today's date in the date format requested. Options are: 'YYYY-MM-DD', 'DD', 'MM', 'YYYY'.\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"date_format\": {\n",
    "          \"type\": \"string\",\n",
    "          \"enum\": [\"%Y-%m-%d\", \"%d\", \"%m\", \"%Y\"],\n",
    "          \"default\": \"%Y-%m-%d\",\n",
    "          \"description\": \"The date format to return today's date in.\"\n",
    "        }\n",
    "      },\n",
    "      \"required\": [\"date_format\"],\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update our system prompt with tool call instructions\n",
    "\n",
    "For the best performance, **add instructions on when to use the tool into the system prompt** that governs your LLM. Below we simply added Step **3.** in our list of instructions, which otherwise represent a typical RAG system prompt. In most RAG apps, one instructs the LLM what fallback answer to respond with when it does not know how to answer a user's query. Such fallback instructions help you reduce hallucinations and more precisely control the AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fallback_answer = \"Based on the available information, I cannot provide a complete answer to this question.\"\n",
    "\n",
    "system_prompt = f\"\"\"For each question:\n",
    "    1. Start with file_search tool\n",
    "    2. If file_search results are incomplete/empty:\n",
    "        - Inform the user about insufficient file results\n",
    "        - Use get_todays_date for additional information if the answer to the question requires today's date\n",
    "        - Present get_todays_date findings without citations\n",
    "       \n",
    "    Only use citations (\u3010source\u3011) for information found directly in files via file_search.\n",
    "    Do not abstain from answering without trying both tools. When you do, say: \"{fallback_answer}\", nothing else.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize OpenAI Assistant\n",
    "\n",
    "We now use the `system_prompt`, vector store helper methods and RAG classes defined above to initialize our RAG App. We add the `get_todays_date` tool into the `tool_registry`. File search (Retrieval) is another tool OpenAI Assistants can invoke during generation, so we want to add it to the function here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc14a924-ab68-41b2-9c8f-3506c6d3c068",
   "metadata": {},
   "source": [
    "**Optional: Code to create the RAG assistant and add the vector store**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = load_documents(client)\n",
    "\n",
    "# Initialize default file search as a tool\n",
    "DEFAULT_FILE_SEARCH: AssistantToolParam = {\"type\": \"file_search\"}\n",
    "\n",
    "# Create an empty tool registry (as we're not using any additional tools yet)\n",
    "tool_registry = ToolRegistry()\n",
    "tool_registry.register_tool('get_todays_date', get_todays_date)\n",
    "\n",
    "# Create assistant and configure RAP App with it, tools and the vector store.\n",
    "assistant = create_rag_assistant(client, system_prompt, [DEFAULT_FILE_SEARCH, todays_date_tool_json])\n",
    "assistant = add_vector_store_to_assistant(client, assistant, vector_store)\n",
    "rag = RAGChat(client, assistant.id, tool_registry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG in action\n",
    "Let's ask our Assistant common questions from users about the *Simple Water Bottle* in our example.\n",
    "\n",
    "\n",
    "\n",
    "### Scenario 1: RAG can answer the question using its Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The water bottle is 10 inches in height and 4 inches in width\u30104:0\u2020simple_water_bottle.txt\u3011.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_question = \"How big is the water bottle?\"\n",
    "rag.chat(user_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the Assistant was able to provide a good answer because its Knowledge Base contains the necessary information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2: RAG can answer the question using other tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[internal log] Calling tool: get_todays_date with args: {'date_format': '%Y-%m-%d'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The limited edition Amber water bottle was launched on January 1st, 2025\u30108:0\u2020simple_water_bottle.txt\u3011. Since today's date is February 27th, 2025, it has already been launched.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_question = \"Check today's date. Has the limited edition Amber water bottle already launched?\"\n",
    "rag.chat(\"Check today's date. Has the limited edition Amber water bottle already launched?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the assistant chose to call our `get_todays_date` tool to obtain information necessary for properly answering the user's query. Note that a proper answer to this question also requires considering information from the Knowledge Base as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Scenario 3: RAG can't answer the question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the available information, I cannot provide a complete answer to this question.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_question = \"Can I return my simple water bottle?\"\n",
    "rag.chat(user_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Assistant's Knowledge Base does not contain information about the return policy, and the `get_todays_date` tool would not help here either. In this case, the best our Assistant can do is to return our fallback response to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Once you have an OpenAI Assistant that can call tools, adding **Codex as a Tool** takes only a few lines of code.\n",
    "Codex enables your RAG app to answer questions it previously could not (like Scenario 3 above). Learn how via our tutorial: [Integrate Codex-as-a-tool into OpenAI Assistants](/codex/tutorials/openai/OpenAIAssistants_AddingCodexAsTool/)\n",
    "\n",
    "Need help? Check the [FAQ](/codex/FAQ/) or email us at: support@cleanlab.ai"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}