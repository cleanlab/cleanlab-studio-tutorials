{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate Codex as-a-Tool into any RAG framework\n",
    "\n",
    "To demonstrate how to integrate Codex with any RAG framework, we'll consider a toy example RAG app built from scratch using OpenAI LLMs.\n",
    "You can translate the same ideas to *any* RAG framework, assuming basic familiarity with RAG and LLMs.\n",
    "\n",
    "This tutorial presumes your RAG app already can perform tool calls. If unsure how to do RAG with tool calls, follow our tutorial: [Adding Tool Calls to RAG](/codex/tutorials/other_rag_frameworks/OtherRAG_ToolCalls/).\n",
    "\n",
    "Once you have a RAG app that supports tool calling, **adding Codex as an additional Tool takes minimal effort but guarantees better responses from your AI application**.\n",
    "\n",
    "![RAG Workflow](../assets/codexastool_retrievalfirst.png)\n",
    "\n",
    "Let's first install packages required for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "%pip install openai  # we used version 1.59.7\n",
    "%pip install --upgrade cleanlab_codex\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483b7d53-15be-43af-b2d7-c41bd6b0a371",
   "metadata": {},
   "source": [
    "**Optional: Helper methods for basic RAG from prior tutorial (Adding Tool Calls to RAG)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "fallback_answer = \"Based on the available information, I cannot provide a complete answer to this question.\"  # desired RAG response when query cannot be answered\n",
    "\n",
    "system_prompt_without_codex = f\"\"\"\n",
    "    Answer the user's Question based on the following possibly relevant Context. Follow these rules:\n",
    "    1. Never use phrases like \"according to the context,\" \"as the context states,\" etc. Treat the Context as your own knowledge, not something you are referencing.\n",
    "    2. Give a clear, short, and accurate answer. Explain complex terms if needed.\n",
    "    3. If the answer to the question requires today's date, use the following tool: get_todays_date.\n",
    "    4. If the Context doesn't adequately address the Question, say: \"{fallback_answer}\" only, nothing else.\n",
    "\n",
    "    Remember, your purpose is to provide information based on the Context, not to offer original advice.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_todays_date(date_format: str) -> str:\n",
    "  \"\"\"A tool that returns today's date in the date format requested.\"\"\"\n",
    "  datetime_str = datetime.now().strftime(date_format)\n",
    "  return datetime_str\n",
    "\n",
    "todays_date_tool_json = {\n",
    "  \"type\": \"function\",\n",
    "  \"function\": {\n",
    "    \"name\": \"get_todays_date\",\n",
    "    \"description\": \"A tool that returns today's date in the date format requested. Options for date_format parameter are: '%Y-%m-%d', '%d', '%m', '%Y'.\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"date_format\": {\n",
    "          \"type\": \"string\",\n",
    "          \"enum\": [\"%Y-%m-%d\", \"%d\", \"%m\", \"%Y\"],\n",
    "          \"default\": \"%Y-%m-%d\",\n",
    "          \"description\": \"The date format to return today's date in.\"\n",
    "        }\n",
    "      },\n",
    "      \"required\": [\"date_format\"],\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "tools_without_codex = [todays_date_tool_json]\n",
    "\n",
    "def retrieve_context(user_question: str) -> str:\n",
    "  \"\"\"Toy retrieval that returns same context for any user question. Replace this with actual retrieval in your RAG system.\"\"\"\n",
    "  contexts = \"\"\"Simple Water Bottle - Amber (limited edition launched Jan 1st 2025)\n",
    "A water bottle designed with a perfect blend of functionality and aesthetics in mind. Crafted from high-quality, durable plastic with a sleek honey-colored finish.\n",
    "Price: $24.99 \\nDimensions: 10 inches height x 4 inches width\"\"\"\n",
    "  return contexts\n",
    "\n",
    "def form_prompt(user_question: str, retrieved_context: str) -> str:\n",
    "  question_with_context = f\"Context:\\n{retrieved_context}\\n\\nUser Question:\\n{user_question}\"\n",
    "  indented_question_with_context = \"\\n\".join(f\"  {line}\" for line in question_with_context.splitlines())    # line is just formatting the final prompt for readability in the tutorial\n",
    "  return indented_question_with_context\n",
    "\n",
    "def simulate_response_as_message(response: str) -> list[dict]:\n",
    "  \"\"\"Commits the response to a conversation history to return back to the model.\"\"\"\n",
    "  return {\"role\": \"assistant\", \"content\": response}\n",
    "\n",
    "def simulate_tool_call_as_message(tool_call_id: str, function_name: str, function_arguments: str) -> dict:\n",
    "  \"\"\"Commits the tool call to a conversation history to return back to the model.\"\"\"\n",
    "  tool_call_message = {\n",
    "    \"role\": \"assistant\",\n",
    "    \"tool_calls\": [{\n",
    "            \"id\": tool_call_id,\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"arguments\": function_arguments,\n",
    "                \"name\": function_name\n",
    "            }\n",
    "  }]}\n",
    "  return tool_call_message\n",
    "\n",
    "def simulate_tool_call_response_as_message(tool_call_id: str, function_response: str) -> dict:\n",
    "  \"\"\"Commits the result of the function call to a conversation history to return back to the model.\"\"\"\n",
    "  function_call_result_message = {\n",
    "    \"role\": \"tool\",\n",
    "    \"content\": function_response,\n",
    "    \"tool_call_id\": tool_call_id,\n",
    "  }\n",
    "  return function_call_result_message\n",
    "\n",
    "def stream_response(client, messages: list[dict], model: str, tools: list[dict]) -> str:\n",
    "    \"\"\"Processes a streaming model response dynamically, handling any tool calls that were made.\n",
    "    Params:\n",
    "        messages: message history list in openai format\n",
    "        model: model name\n",
    "        tools: list of tools model can call\n",
    "    Returns:\n",
    "        response: final response in openai format\n",
    "    \"\"\"\n",
    "\n",
    "    response_stream = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "        tools=tools,\n",
    "        parallel_tool_calls=False,  # prevents OpenAI from making multiple tool calls in a single response\n",
    "    )\n",
    "\n",
    "    function_arguments = \"\"\n",
    "    function_name = \"\"\n",
    "    function_call_id = \"\"\n",
    "    is_collecting_function_args = False\n",
    "    collected_messages = []\n",
    "\n",
    "    for part in response_stream:\n",
    "        delta = part.choices[0].delta\n",
    "        finish_reason = part.choices[0].finish_reason\n",
    "\n",
    "        if not delta.tool_calls:  # Regular response\n",
    "            chunk_message = part.choices[0].delta.content\n",
    "            collected_messages.append(chunk_message)\n",
    "        else:  # Tool call logic\n",
    "            is_collecting_function_args = True\n",
    "            tool_call = delta.tool_calls[0]\n",
    "\n",
    "            if tool_call.id:\n",
    "                function_call_id = tool_call.id\n",
    "            if tool_call.function.name:\n",
    "                function_name = tool_call.function.name\n",
    "            if tool_call.function.arguments:\n",
    "                function_arguments += tool_call.function.arguments\n",
    "\n",
    "        # Process tool call when all arguments are collected\n",
    "        if finish_reason == \"tool_calls\" and is_collecting_function_args:\n",
    "            args = json.loads(function_arguments)\n",
    "            function_response = _handle_any_tool_call_for_stream_response(function_name, args)\n",
    "            print(f'[internal log] Called {function_name} tool, with arguments: {args}')\n",
    "            print(f'[internal log] Tool response: {str(function_response)}')\n",
    "            is_collecting_function_args = False\n",
    "\n",
    "    # Finalize response\n",
    "    if finish_reason == \"tool_calls\":\n",
    "        tool_call_response_message = simulate_tool_call_response_as_message(function_call_id, function_response)\n",
    "\n",
    "        # If the tool call resulted in an error, return the message instead of continuing the conversation\n",
    "        if \"error\" in tool_call_response_message[\"content\"]:\n",
    "            return tool_call_response_message\n",
    "\n",
    "        response = [\n",
    "            simulate_tool_call_as_message(function_call_id, function_name, function_arguments),\n",
    "            tool_call_response_message,\n",
    "        ]\n",
    "        \n",
    "        # If needed, extend messages and re-call the stream response\n",
    "        messages.extend(response)\n",
    "        response = stream_response(client=client, messages=messages, model=model, tools=tools)  # This recursive call handles the case when a tool calls another tool until all tools are resolved and a final response is returned\n",
    "    else:\n",
    "        collected_messages = [m for m in collected_messages if m is not None]\n",
    "        full_str_response = \"\".join(collected_messages)\n",
    "        response = simulate_response_as_message(full_str_response)\n",
    "\n",
    "    return response\n",
    "\n",
    "def _handle_any_tool_call_for_stream_response(function_name: str, arguments: dict) -> str:\n",
    "    \"\"\"Handles any tool dynamically by calling the function by name and passing in collected arguments.\n",
    "       Returns a dictionary of the tool output.\n",
    "       Returns error message if the tool is not found, not callable or called incorrectly.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        tool_function = globals().get(function_name) or locals().get(function_name)\n",
    "        if callable(tool_function):\n",
    "            # Dynamically call the tool function with arguments\n",
    "            tool_output = tool_function(**arguments)\n",
    "            return json.dumps(tool_output)\n",
    "        else:\n",
    "            return json.dumps({\n",
    "                \"error\": f\"Tool '{function_name}' not found or not callable.\",\n",
    "                \"arguments\": arguments,\n",
    "            })\n",
    "    except Exception as e:\n",
    "        return json.dumps({\n",
    "            \"error\": f\"Exception in handling tool '{function_name}': {str(e)}\",\n",
    "            \"arguments\": arguments,\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example RAG App: Product Customer Support\n",
    "\n",
    "Let's revisit our RAG app built in the [RAG With Tool Calls](/codex/tutorials/other_rag_frameworks/OtherRAG_ToolCalls/) tutorial, which has the option to call a `get_todays_date()` tool. This example represents a customer support / e-commerce use-case where the Knowledge Base contains product listings like the following:\n",
    "\n",
    "![Simple water bottle product listing](../assets/simple_water_bottle.png)\n",
    "\n",
    "The details of this toy RAG app are unimportant if you are already familiar with RAG and Tool Calling, otherwise refer to the [RAG With Tool Calls](/codex/tutorials/other_rag_frameworks/OtherRAG_ToolCalls/) tutorial. That tutorial walks through the RAG method defined below, which uses the OpenAI LLM API for single-turn Q&A with token-streaming. To run this method, we instantiate our OpenAI client. Subsequently, we integrate Codex-as-a-Tool and demonstrate its benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783daf0d-527f-42ce-82f7-4d8a06a7d9cd",
   "metadata": {},
   "source": [
    "**Optional: Helper RAG method from prior tutorial (Adding Tool Calls to RAG)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rag(client, model: str, user_question: str, system_prompt: str, tools: list[dict]) -> str:\n",
    "  retrieved_context = retrieve_context(user_question)\n",
    "  question_with_context = form_prompt(user_question, retrieved_context)\n",
    "  print(f\"[internal log] Invoking LLM with prompt\\n{question_with_context}\\n\\n\")\n",
    "\n",
    "  messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": question_with_context},\n",
    "  ]\n",
    "  \n",
    "  response_messages = stream_response(client=client, messages=messages, model=model, tools=tools)\n",
    "  return f\"[RAG response] {response_messages.get('content')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR-KEY-HERE>\"  # Replace with your OpenAI API key\n",
    "model = \"gpt-4o\"  # which LLM to use\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5rqMr-6PDHt"
   },
   "source": [
    "## Create Codex Project\n",
    "\n",
    "To use Codex, first [create a Project](/codex/sme_tutorials/getting_started/).\n",
    "\n",
    "Here we assume some common (question, answer) pairs about the *Simple Water Bottle* have already been added to a Codex Project.\n",
    "Learn how that was done via our tutorial: [Populating Codex](/codex/sme_tutorials/populating_codex/).\n",
    "\n",
    "Our existing Codex Project contains the following entries:\n",
    "\n",
    "![Codex Knowledge Base Example](../assets/codex_kb.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_key = \"<YOUR-PROJECT-ACCESS-KEY>\"  # Obtain from your Project's settings page: https://codex.cleanlab.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Programmatically populate the Codex Project with above (question, answer) pairs. Note: The recommended flow is to do this manually in the Web App.\n",
    "from cleanlab_codex.client import Client\n",
    "\n",
    "os.environ[\"CODEX_API_KEY\"] = \"<YOUR-KEY-HERE>\"  # Replace with your Codex API key\n",
    "codex_client = Client()\n",
    "\n",
    "# Create a project\n",
    "project = codex_client.create_project(\n",
    "    name=\"Product FAQs\",\n",
    "    description=\"Questions about product pages\",\n",
    ")\n",
    "\n",
    "# Add entries to the project\n",
    "project.add_entries(\n",
    "    entries=[\n",
    "        {\"question\": \"How much water can the Simple Water Bottle hold?\", \"answer\": \"32oz\"},\n",
    "        {\"question\": \"Can I return my Simple Water Bottle?\", \"answer\": \"Return it within 30 days for a full refund-- no questions asked. Contact our support team to initiate your return!\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "access_key = project.create_access_key(\"test access key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate Codex as an additional tool\n",
    "\n",
    "Integrating Codex into a RAG app that supports tool calling requires minimal code changes:\n",
    "\n",
    "1. Import Codex and add it into your list of `tools`.\n",
    "2. Update your system prompt to include instructions for calling Codex, as demonstrated below in: `system_prompt_with_codex`.\n",
    "\n",
    "After that, call your original RAG pipeline with these updated variables to start experiencing the benefits of Codex!\n",
    "\n",
    "**Note:** This tutorial uses a Codex tool description in OpenAI format, provided via the `to_openai_tool()` function. For certain non-OpenAI LLMs, you can import the Codex tool description in other provided formats as well, or manually write it yourself if no provided format is available. Check the [Codex API Docs](/reference/python/codex_tool/) for other formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab_codex import CodexTool\n",
    "\n",
    "codex_tool = CodexTool.from_access_key(access_key=access_key, fallback_answer=fallback_answer)\n",
    "codex_tool_openai = codex_tool.to_openai_tool()\n",
    "\n",
    "globals()[codex_tool.tool_name] = codex_tool.query  # Optional step for convenience: make function to call the tool globally accessible\n",
    "\n",
    "tools_with_codex = tools_without_codex + [codex_tool_openai]  # Add Codex to the list of tools\n",
    "\n",
    "# Update the RAG system prompt with instructions for handling Codex (adjust based on your needs)\n",
    "system_prompt_with_codex = f\"\"\"\n",
    "    You are a helpful assistant designed to help users navigate a complex set of documents for question-answering tasks. Answer the user's Question based on the following possibly relevant Context and previous chat history using the tools provided if necessary. Follow these rules in order:\n",
    "    1. NEVER use phrases like 'according to the context,' 'as the context states,' etc. Treat the Context as your own knowledge, not something you are referencing.\n",
    "    2. Use only information from the provided Context. Your purpose is to provide information based on the Context, not to offer original advice.\n",
    "    3. Give a clear, short, and accurate answer. Explain complex terms if needed.\n",
    "    4. If the answer to the question requires today's date, use the following tool: todays_date_tool. Return the date in the exact format the tool provides it.\n",
    "    5. If you remain unsure how to answer the user query, then use the {codex_tool.tool_name} tool to search for the answer.  Always call {codex_tool.tool_name} whenever the provided Context does not answer the user query. Do not call {codex_tool.tool_name} if you already know the right answer or the necessary information is in the provided Context. Your query to {codex_tool.tool_name} should match the user's original query, unless minor clarification is needed to form a self-contained query. After you have called {codex_tool.tool_name}, determine whether its answer seems helpful, and if so, respond with this answer to the user. If the answer from {codex_tool.tool_name} does not seem helpful, then simply ignore it.\n",
    "    6. If you remain unsure how to answer the Question (even after using the {codex_tool.tool_name} tool and considering the provided Context), then only respond with: \"{fallback_answer}\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG with Codex in action \n",
    "\n",
    "Integrating Codex as-a-Tool allows your RAG app to answer more questions than it was originally capable of.\n",
    "\n",
    "### Example 1\n",
    "\n",
    "Let's ask a question to our **original** RAG app (before Codex was integrated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[internal log] Invoking LLM with prompt + context\n",
      "  Context:\n",
      "  Simple Water Bottle - Amber (limited edition launched Jan 1st 2025)\n",
      "  A water bottle designed with a perfect blend of functionality and aesthetics in mind. Crafted from high-quality, durable plastic with a sleek honey-colored finish.\n",
      "  Price: $24.99 \n",
      "  Dimensions: 10 inches height x 4 inches width\n",
      "  \n",
      "  User Question:\n",
      "  Can I return my simple water bottle?\n",
      "\n",
      "\n",
      "[RAG response] Based on the available information, I cannot provide a complete answer to this question.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"Can I return my simple water bottle?\"\n",
    "\n",
    "response = rag(client, model=model, user_question=user_question,\n",
    "               system_prompt=system_prompt_without_codex, tools=tools_without_codex\n",
    "              )\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **original RAG app is unable to answer**, in this case because the required information is not in its Knowledge Base.\n",
    "\n",
    "Let's ask the same question to our RAG app with Codex added as an additional tool. \n",
    "Note that we use the updated system prompt and tool list when Codex is integrated in the RAG app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[internal log] Invoking LLM with prompt + context\n",
      "  Context:\n",
      "  Simple Water Bottle - Amber (limited edition launched Jan 1st 2025)\n",
      "  A water bottle designed with a perfect blend of functionality and aesthetics in mind. Crafted from high-quality, durable plastic with a sleek honey-colored finish.\n",
      "  Price: $24.99 \n",
      "  Dimensions: 10 inches height x 4 inches width\n",
      "  \n",
      "  User Question:\n",
      "  Can I return my simple water bottle?\n",
      "\n",
      "\n",
      "[internal log] Called ask_advisor tool, with arguments: {'question': 'Can I return my simple water bottle?'}\n",
      "[internal log] Tool response: \"Return it within 30 days for a full refund-- no questions asked. Contact our support team to initiate your return!\"\n",
      "[RAG response] You can return your Simple Water Bottle within 30 days for a full refund. To initiate the return, contact the support team.\n"
     ]
    }
   ],
   "source": [
    "response = rag(client, model=model, user_question=user_question,\n",
    "               system_prompt=system_prompt_with_codex, tools=tools_with_codex\n",
    "              )\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, **integrating Codex enables your RAG app to answer questions it originally strugged with**, as long as a similar question was already answered in the corresponding Codex Project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "Let's ask another question to our RAG app with Codex integrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[internal log] Invoking LLM with prompt + context\n",
      "  Context:\n",
      "  Simple Water Bottle - Amber (limited edition launched Jan 1st 2025)\n",
      "  A water bottle designed with a perfect blend of functionality and aesthetics in mind. Crafted from high-quality, durable plastic with a sleek honey-colored finish.\n",
      "  Price: $24.99 \n",
      "  Dimensions: 10 inches height x 4 inches width\n",
      "  \n",
      "  User Question:\n",
      "  How can I order the Simple Water Bottle in bulk?\n",
      "\n",
      "\n",
      "[internal log] Called ask_advisor tool, with arguments: {'question': 'How can I order the Simple Water Bottle in bulk?'}\n",
      "[internal log] Tool response: \"Based on the available information, I cannot provide a complete answer to this question.\"\n",
      "[RAG response] Based on the available information, I cannot provide a complete answer to this question.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"How can I order the Simple Water Bottle in bulk?\"\n",
    "\n",
    "response = rag(client, model=model, user_question=user_question,\n",
    "               system_prompt=system_prompt_with_codex, tools=tools_with_codex\n",
    "              )\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our RAG app is unable to answer this question because there is no relevant information in its Knowledge Base, nor has a similar question been answered in the Codex Project (see the contents of the Codex Project above).\n",
    "\n",
    "**Codex automatically recognizes this question could not be answered and logs it into the Project where it awaits an answer from a SME**.\n",
    "Navigate to your Codex Project in the [Web App](https://codex.cleanlab.ai/) where you (or a SME at your company) can enter the desired answer for this query.\n",
    "\n",
    "![Codex Project with asked question that has not been answered yet](../assets/codex_kb_unanswered.png)\n",
    "\n",
    "As soon as an answer is provided in Codex, our RAG app will be able to answer all similar questions going forward (as seen for the previous query)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3\n",
    "\n",
    "Let's ask another query to our RAG app with Codex integrated. This is a query the  original RAG app was able to correctly answer without Codex (since the relevant information exists in the Knowledge Base)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[internal log] Invoking LLM with prompt + context\n",
      "  Context:\n",
      "  Simple Water Bottle - Amber (limited edition launched Jan 1st 2025)\n",
      "  A water bottle designed with a perfect blend of functionality and aesthetics in mind. Crafted from high-quality, durable plastic with a sleek honey-colored finish.\n",
      "  Price: $24.99 \n",
      "  Dimensions: 10 inches height x 4 inches width\n",
      "  \n",
      "  User Question:\n",
      "  How big is the water bottle?\n",
      "\n",
      "\n",
      "[RAG response] The water bottle is 10 inches in height and 4 inches in width.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"How big is the water bottle?\"\n",
    "\n",
    "response = rag(client, model=model, user_question=user_question,\n",
    "               system_prompt=system_prompt_with_codex, tools=tools_with_codex\n",
    "              )\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the RAG app with Codex integrated is still able to correctly answer this query. **Integrating Codex has no negative effect on questions your original RAG app could answer**.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that Codex is integrated with your RAG app, you and SMEs can [open the Codex Project and answer questions](/codex/sme_tutorials/codex_as_sme/) logged there to continuously improve your AI.\n",
    "\n",
    "**Adding Codex only improves your RAG app.** As seen here, integrating Codex into your RAG app requires minimal extra code. Once integrated, the Codex Project automatically logs all user queries that your original RAG app handles poorly. Using a [simple web interface](/codex/sme_tutorials/codex_as_sme/), SMEs at your company can answer the highest priority questions in the Codex Project. As soon as an answer is entered in Codex, your RAG app will be able to properly handle all similar questions encountered in the future\n",
    "\n",
    "Codex is **the fastest way for nontechnical SMEs to directly improve your RAG app**. As the Developer, you simply integrate Codex once, and from then on, SMEs can continuously improve how your AI handles common user queries without needing your help. Codex works with *any* RAG architecture, so Developers can independently improve the RAG system in other ways with their new free time. \n",
    "\n",
    "This tutorial demonstrated a single-turn Q&A app, but you can easily [extend this code into a conversational app](/codex/tutorials/other_rag_frameworks/OtherRAG_ToolCalls/#conversational-rag-multi-turn-dialogues) (multi-turn chat).\n",
    "\n",
    "Need help, more capabilities, or other deployment options? \n",
    "Check the [FAQ](/codex/FAQ/) or email us at: support@cleanlab.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "3.10.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}