{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Tool calls to RAG\n",
    "\n",
    "This tutorial covers the basics of building a RAG app that supports [tool calls](https://platform.openai.com/docs/guides/function-calling). Here we demonstrate how to build the specific RAG app\u00a0used in our [Integrate Codex as-a-Tool into any RAG framework](/codex/tutorials/other_rag_frameworks/OtherRAG_CodexAsTool/) tutorial, a minimal example just using OpenAI LLMs. Remember that Codex works with *any* RAG app, you can easily translate these ideas to more complex RAG pipelines and other LLMs.\n",
    "\n",
    "Here's a typical architecture for RAG apps with tool calling:\n",
    "\n",
    "![RAG Workflow](../assets/codexastool_retrievalfirst.png)\n",
    "\n",
    "Let's first install and setup the OpenAI client library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade openai  # we used package-version 1.63.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NgaXfc7SO20r"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR-KEY-HERE>\"  # Replace with your OpenAI API key\n",
    "model = \"gpt-4o\"  # which LLM to use\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example RAG App: Product Customer Support\n",
    "\n",
    "Consider a customer support / e-commerce RAG use-case where the Knowledge Base contains product listings like the following:\n",
    "\n",
    "![Simple water bottle product listing](../assets/simple_water_bottle.png)\n",
    "\n",
    "To keep our example minimal, we mock the retrieval step (you can easily replace our mock retrieval with actual search over a complex Knowledge Base or Vector Database). In our mock retrieval, the same `context` (product information) will always be returned for every user query. After retrieval, the next step in RAG is to combine the retrieved context with the user query into a LLM prompt that is used to generate a response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1766879-1af8-42ca-9676-c461d3a551b5",
   "metadata": {},
   "source": [
    "**Optional: Helper methods for a toy RAG application**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_context(user_question: str) -> str:\n",
    "  \"\"\"Mock retrieval function returns same context for any user_question. Replace with actual retrieval step in your RAG system.\"\"\"\n",
    "  contexts = \"\"\"Simple Water Bottle - Amber (limited edition launched Jan 1st 2025)\n",
    "A water bottle designed with a perfect blend of functionality and aesthetics in mind. Crafted from high-quality, durable plastic with a sleek honey-colored finish.\n",
    "Price: $24.99 \\nDimensions: 10 inches height x 4 inches width\"\"\"\n",
    "  return contexts\n",
    "\n",
    "def form_prompt(user_question: str, retrieved_context: str) -> str:\n",
    "  question_with_context = f\"Context:\\n{retrieved_context}\\n\\nUser Question:\\n{user_question}\"\n",
    "  indented_question_with_context = \"\\n\".join(f\"  {line}\" for line in question_with_context.splitlines())    # line is just formatting the final prompt for readability in the tutorial\n",
    "  return indented_question_with_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log tool calls in the message history\n",
    "\n",
    "Conversational AI applications rely on message history to track a dialogue between the user and AI assistant. When the AI can choose to call tools, we must update the message history to reflect when a tool was called and what it returned. The required formats for the message history differ between: a regular LLM response, a tool call request from the LLM, and a response from the tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a667daaf-004c-410c-816e-f390b200ba94",
   "metadata": {},
   "source": [
    "**Optional: Helper methods to handle message history with tool calls**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Functions for appropriately formatting model responses, tool calls, and tool outputs for the message history.\n",
    "# The format varies by LLM provider. If you are not using OpenAI, adjust these functions for your required format.\n",
    "\n",
    "def simulate_response_as_message(response: str) -> list[dict]:\n",
    "  \"\"\"Commits the response to a conversation history to return back to the model.\"\"\"\n",
    "  return {\"role\": \"assistant\", \"content\": response}\n",
    "\n",
    "def simulate_tool_call_as_message(tool_call_id: str, function_name: str, function_arguments: str) -> dict:\n",
    "  \"\"\"Commits the tool call to a conversation history to return back to the model.\"\"\"\n",
    "  tool_call_message = {\n",
    "    \"role\": \"assistant\",\n",
    "    \"tool_calls\": [{\n",
    "            \"id\": tool_call_id,\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"arguments\": function_arguments,\n",
    "                \"name\": function_name\n",
    "            }\n",
    "  }]}\n",
    "  return tool_call_message\n",
    "\n",
    "def simulate_tool_call_response_as_message(tool_call_id: str, function_response: str) -> dict:\n",
    "  \"\"\"Commits the result of the function call to a conversation history to return back to the model.\"\"\"\n",
    "  function_call_result_message = {\n",
    "    \"role\": \"tool\",\n",
    "    \"content\": function_response,\n",
    "    \"tool_call_id\": tool_call_id,\n",
    "  }\n",
    "  return function_call_result_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run LLM and tool calls\n",
    "\n",
    "In this example, our LLM will use token streaming to produce its responses in real-time. \n",
    "When prompting the LLM, we provide a list of tools that the LLM can optionally choose to call instead of generating a response. If the LLM chooses to use a tool instead of returning a response right away, then you need to execute the function corresponding to this tool yourself using the argument values generated by the LLM. After calling the function, give the return value to the LLM in a new message. The LLM can then decide how to respond to the user, or may choose to call yet another tool.\n",
    "\n",
    "All of this is handled  for *any* tool in the below helper methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bf94e6-6a5a-4d41-845f-ff5a02346a0c",
   "metadata": {},
   "source": [
    "**Optional: Helper methods to generate responses and call tools**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If you are not using OpenAI LLMs, adjust these functions for your required format for tool calling and prompting.\n",
    "\n",
    "def stream_response(client, messages: list[dict], model: str, tools: list[dict]) -> str:\n",
    "    \"\"\"Processes a streaming response dynamically handling any tool.\n",
    "    Params:\n",
    "        messages: message history list in openai format\n",
    "        model: model name\n",
    "        tools: list of tools model can call\n",
    "    Returns:\n",
    "        response: final response in openai format\n",
    "    \"\"\"\n",
    "\n",
    "    response_stream = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "        tools=tools,\n",
    "        parallel_tool_calls=False,  # prevents OpenAI from making multiple tool calls in a single response\n",
    "    )\n",
    "\n",
    "    collected_messages = []\n",
    "    final_tool_calls = {}\n",
    "\n",
    "    for chunk in response_stream:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            collected_messages.append(chunk.choices[0].delta.content)\n",
    "        for tool_call in chunk.choices[0].delta.tool_calls or []:\n",
    "            index = tool_call.index\n",
    "\n",
    "            if index not in final_tool_calls:\n",
    "                final_tool_calls[index] = tool_call\n",
    "\n",
    "            final_tool_calls[index].function.arguments += tool_call.function.arguments\n",
    "        \n",
    "        if chunk.choices[0].finish_reason == \"tool_calls\":\n",
    "            for tool_call in final_tool_calls.values():\n",
    "                function_response = _handle_any_tool_call_for_stream_response(tool_call.function.name, json.loads(tool_call.function.arguments))\n",
    "                print(f'[internal log] Called {tool_call.function.name} tool, with arguments: {tool_call.function.arguments}')\n",
    "                print(f'[internal log] Tool response: {str(function_response)}')\n",
    "                tool_call_response_message = simulate_tool_call_response_as_message(tool_call.id, function_response)\n",
    "\n",
    "                # If the tool call resulted in an error, return the message instead of continuing the conversation\n",
    "                if \"error\" in tool_call_response_message[\"content\"]:\n",
    "                    return tool_call_response_message\n",
    "\n",
    "                response = [\n",
    "                    simulate_tool_call_as_message(tool_call.id, tool_call.function.name, tool_call.function.arguments),\n",
    "                    tool_call_response_message,\n",
    "                ]\n",
    "                \n",
    "                # If needed, extend messages and re-call the stream response\n",
    "                messages.extend(response)\n",
    "                response = stream_response(client=client, messages=messages, model=model, tools=tools)  # This recursive call handles the case when a tool calls another tool until all tools are resolved and a final response is returned\n",
    "        else:\n",
    "            collected_messages = [m for m in collected_messages if m is not None]\n",
    "            full_str_response = \"\".join(collected_messages)\n",
    "            response = simulate_response_as_message(full_str_response)\n",
    "    return response\n",
    "    \n",
    "\n",
    "def _handle_any_tool_call_for_stream_response(function_name: str, arguments: dict) -> str:\n",
    "    \"\"\"Handles any tool dynamically by calling the function by name and passing in collected arguments.\n",
    "       Returns a dictionary of the tool output.\n",
    "       Returns error message if the tool is not found, not callable or called incorrectly.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        tool_function = globals().get(function_name) or locals().get(function_name)\n",
    "        if callable(tool_function):\n",
    "            # Dynamically call the tool function with arguments\n",
    "            tool_output = tool_function(**arguments)\n",
    "            return json.dumps(tool_output)\n",
    "        else:\n",
    "            return json.dumps({\n",
    "                \"error\": f\"Tool '{function_name}' not found or not callable.\",\n",
    "                \"arguments\": arguments,\n",
    "            })\n",
    "    except Exception as e:\n",
    "        return json.dumps({\n",
    "            \"error\": f\"Exception in handling tool '{function_name}': {str(e)}\",\n",
    "            \"arguments\": arguments,\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define single-turn RAG application (Q&A)\n",
    "\n",
    "We integrate the above helper methods into a standard RAG app that can respond to any user query, calling tools as the LLM deems necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(client, model: str, user_question: str, system_prompt: str, tools: list[dict]) -> str:\n",
    "  retrieved_context = retrieve_context(user_question)\n",
    "  question_with_context = form_prompt(user_question, retrieved_context)\n",
    "  print(f\"[internal log] Invoking LLM text\\n{question_with_context}\\n\\n\")\n",
    "\n",
    "  messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": question_with_context},\n",
    "  ]\n",
    "  \n",
    "  response_messages = stream_response(client=client, messages=messages, model=model, tools=tools)\n",
    "  return f\"\\n[RAG response] {response_messages.get('content')}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Tool: get_todays_date\n",
    "\n",
    "Let's define an example tool `get_todays_date()` that our RAG app can rely on. Here we follow OpenAI's format for representing the tool, but other LLM providers are similar. We describe the actual function to the LLM in a JSON format, listing the arguments and their properties as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_todays_date(date_format: str) -> str:\n",
    "  \"\"\"A tool that returns today's date in the date format requested.\"\"\"\n",
    "  datetime_str = datetime.now().strftime(date_format)\n",
    "  return datetime_str\n",
    "\n",
    "todays_date_tool_json = {\n",
    "  \"type\": \"function\",\n",
    "  \"function\": {\n",
    "    \"name\": \"get_todays_date\",\n",
    "    \"description\": \"A tool that returns today's date in the date format requested. Options for date_format parameter are: '%Y-%m-%d', '%d', '%m', '%Y'.\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"date_format\": {\n",
    "          \"type\": \"string\",\n",
    "          \"enum\": [\"%Y-%m-%d\", \"%d\", \"%m\", \"%Y\"],\n",
    "          \"default\": \"%Y-%m-%d\",\n",
    "          \"description\": \"The date format to return today's date in.\"\n",
    "        }\n",
    "      },\n",
    "      \"required\": [\"date_format\"],  # indicates this is a required argument whose value must be specified when calling this tool.\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update our LLM system prompt with tool call instructions\n",
    "\n",
    "For the best performance, **add instructions on when to use the tool into the system prompt** that governs your LLM. Below we simply added Step **3.** in our list of instructions, which otherwise represent a typical RAG system prompt. In most RAG apps, one instructs the LLM what fallback answer to respond with when it does not know how to answer a user's query. Such fallback instructions help you reduce hallucinations and more precisely control the AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fallback_answer = \"Based on the available information, I cannot provide a complete answer to this question.\"\n",
    "\n",
    "system_prompt = f\"\"\"You are a helpful assistant designed to help users navigate a complex set of documents for question-answering tasks. Answer the user's Question based on the following possibly relevant Context and previous chat history using the tools provided if necessary. Follow these rules in order:\n",
    "    1. NEVER use phrases like \"according to the context\", \"as the context states\", etc. Treat the Context as your own knowledge, not something you are referencing.\n",
    "    2. Use only information from the provided Context.\n",
    "    3. Give a clear, short, and accurate Answer. Explain complex terms if needed.\n",
    "    4. If the answer to the question requires today's date, use the following tool: get_todays_date. Return the date in the exact format the tool provides it.\n",
    "    5. If the Context doesn't adequately address the Question or you are unsure how to answer the Question, say: \"{fallback_answer}\" only, nothing else.\n",
    "\n",
    "    Remember, your purpose is to provide information based on the Context, not to offer original advice.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG in action\n",
    "\n",
    "Let's run our RAG application over different questions commonly asked by users about the *Simple Water Bottle* in our example.\n",
    "\n",
    "### Scenario 1: RAG can answer the question without tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[internal log] Invoking LLM text\n",
      "  Context:\n",
      "  Simple Water Bottle - Amber (limited edition launched Jan 1st 2025)\n",
      "  A water bottle designed with a perfect blend of functionality and aesthetics in mind. Crafted from high-quality, durable plastic with a sleek honey-colored finish.\n",
      "  Price: $24.99 \n",
      "  Dimensions: 10 inches height x 4 inches width\n",
      "  \n",
      "  User Question:\n",
      "  How big is the water bottle?\n",
      "\n",
      "\n",
      "\n",
      "[RAG response] The water bottle is 10 inches in height and 4 inches in width.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"How big is the water bottle?\"\n",
    "\n",
    "response = rag(client, model=model, user_question=user_question, system_prompt=system_prompt, tools=[todays_date_tool_json])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the LLM was able to provide a good answer because the retrieved context contains the necessary information.\n",
    "\n",
    "### Scenario 2: RAG can answer the question using tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[internal log] Invoking LLM text\n",
      "  Context:\n",
      "  Simple Water Bottle - Amber (limited edition launched Jan 1st 2025)\n",
      "  A water bottle designed with a perfect blend of functionality and aesthetics in mind. Crafted from high-quality, durable plastic with a sleek honey-colored finish.\n",
      "  Price: $24.99 \n",
      "  Dimensions: 10 inches height x 4 inches width\n",
      "  \n",
      "  User Question:\n",
      "  Has the limited edition Amber water bottle already launched?\n",
      "\n",
      "\n",
      "[internal log] Called get_todays_date tool, with arguments: {\"date_format\":\"%Y-%m-%d\"}\n",
      "[internal log] Tool response: \"2025-02-25\"\n",
      "\n",
      "[RAG response] Yes, the limited edition Amber water bottle launched on January 1st, 2025, so it has already launched.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"Has the limited edition Amber water bottle already launched?\"\n",
    "\n",
    "response = rag(client, model=model, user_question=user_question, system_prompt=system_prompt, tools=[todays_date_tool_json])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the LLM chose to call our `get_todays_date` tool to obtain information necessary for properly answering the user's query. Note that a proper answer to this question also requires considering information from the retrieved context as well.\n",
    "\n",
    "### Scenario 3: RAG cannot answer the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "id": "93Vm-SMaO28z",
    "outputId": "a7766b6e-7eda-4f99-b346-a00f55adc8c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[internal log] Invoking LLM text\n",
      "  Context:\n",
      "  Simple Water Bottle - Amber (limited edition launched Jan 1st 2025)\n",
      "  A water bottle designed with a perfect blend of functionality and aesthetics in mind. Crafted from high-quality, durable plastic with a sleek honey-colored finish.\n",
      "  Price: $24.99 \n",
      "  Dimensions: 10 inches height x 4 inches width\n",
      "  \n",
      "  User Question:\n",
      "  Can I return my simple water bottle?\n",
      "\n",
      "\n",
      "\n",
      "[RAG response] Based on the available information, I cannot provide a complete answer to this question.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"Can I return my simple water bottle?\"\n",
    "\n",
    "response = rag(client, model=model, user_question=user_question, system_prompt=system_prompt, tools=[todays_date_tool_json])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Context does not contain information about the return policy, and the `get_todays_date` tool would not help either.\n",
    "In this case, we want to return our fallback response to the user.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Adding tool calls to your RAG system expands the capabilities of what your AI can do and the types of questions it can answer.\n",
    "\n",
    "Once you have a RAG app with tools set up, adding **Codex as-a-Tool** takes only a few lines of code (regardless what RAG framework you are using).\n",
    "Codex enables your RAG app to answer questions it previously could not (like Scenario 3 above). Learn how via our tutorial: [Integrate Codex as-a-Tool into any RAG framework](/codex/tutorials/other_rag_frameworks/OtherRAG_CodexAsTool/).\n",
    "\n",
    "Need help? Check the [FAQ](/codex/FAQ/) or email us at: support@cleanlab.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversational RAG (multi-turn dialogues) \n",
    "\n",
    "Extending our single-turn RAG function above into a conversational chat application (with tool calling) is easy:\n",
    "\n",
    "Update the helper method `stream_response` defined above: add a `messages` argument that tracks conversation history, and append generated responses to this history.\n",
    "\n",
    "(Note: to make the code in this section runnable, you'll have to actually update the above `rag()` helper methods according to the descriptions here.)\n",
    "\n",
    "```python\n",
    "def stream_response(..., messages):\n",
    "    # same code as stream_response() function defined in the above helper method, we only show how to update the final Else clause here\n",
    "    else:\n",
    "        collected_messages = [m for m in collected_messages if m is not None]\n",
    "        full_str_response = \"\".join(collected_messages)\n",
    "        response = simulate_response_as_message(full_str_response)\n",
    "        messages.append(response)\n",
    "    return messages\n",
    "```\n",
    "\n",
    "Define a global `message_history` variable to pass into RAG function called at each conversation turn in a dialogue. Each time you start a new dialogue (user interaction), simply reset `message_history`.\n",
    "\n",
    "```python\n",
    "message_history = [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": (\n",
    "          system_prompt\n",
    "      ),\n",
    "  },\n",
    "]\n",
    "```\n",
    "\n",
    "For each turn in a conversation, call `rag()` with `message_history`. The last message in this history corresponds to the final response for a specific query that you can give the user.\n",
    "\n",
    "Here's an example conversation (first update the `rag()` helper methods above before running this code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The water bottle is Amber in color.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_query = \"What color is the Simple Water Bottle?\"\n",
    "message_history = rag(model=model, user_query=user_query, message_history=message_history, tools=tools)\n",
    "message_history[-1].get(\"content\")  # Print final response to return to user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The water bottle has dimensions of 10 inches in height and 4 inches in width.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_query = \"How big is it?\"\n",
    "message_history = rag(model=model, user_query=user_query, message_history=message_history, tools=tools)\n",
    "message_history[-1].get(\"content\")  # Print final response to return to user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "3.10.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}