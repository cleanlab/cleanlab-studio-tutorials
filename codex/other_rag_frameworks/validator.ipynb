{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect and remediate bad responses from any RAG application\n",
    "\n",
    "This tutorial demonstrates how to automatically improve any RAG application by integrating Cleanlab. The Cleanlab API takes in the AI-generated response from your RAG app, and the same inputs provided to the LLM that generated it: user query, retrieved context, and any parts of the LLM prompt. Cleanlab will automatically detect if your AI response is bad (e.g., untrustworthy, unhelpful, or unsafe).  The Cleanlab API returns these real-time evaluation scores which you can use to guardrail your AI. If your AI response is flagged as bad, the Cleanlab API will also return an expert response whenever a similar query has been answered in the connected Cleanlab Project, or otherwise log this query into the Cleanlab Project for SMEs to answer.\n",
    "\n",
    "!['Cleanlab AI Platform'](../assets/codexasbackup.png)\n",
    "\n",
    "## Overview\n",
    "\n",
    "Here's all the code needed for using Cleanlab with your RAG system.\n",
    "\n",
    "```python\n",
    "from cleanlab_codex import Project\n",
    "project = Project.from_access_key(access_key)\n",
    "\n",
    "# Your existing RAG code:\n",
    "context = rag_retrieve_context(user_query)\n",
    "messages = rag_form_prompt(user_query, context)\n",
    "response = rag_generate_response(messages)\n",
    "\n",
    "# Detect bad responses and remediate with Cleanlab\n",
    "results = project.validate(messages=messages, query=user_query, context=context, response=response)\n",
    "\n",
    "final_response = (\n",
    "    results.expert_answer if results.expert_answer and results.escalated_to_sme\n",
    "    else fallback_response if results.should_guardrail\n",
    "    else response\n",
    ")\n",
    "```\n",
    "\n",
    "**Note:** This tutorial is for *Single-turn Q&A Apps*. If you have a *Multi-turn Chat app*, a similar workflow is covered in the [Detect and Remediate bad responses in Conversational Apps](/codex/tutorials/other_rag_frameworks/validator/) tutorial.\n",
    "\n",
    "## Setup\n",
    "\n",
    "This tutorial requires a Cleanlab API key. Get one [here](https://codex.cleanlab.ai/account)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade cleanlab-codex pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your Codex API key\n",
    "import os\n",
    "os.environ[\"CODEX_API_KEY\"] = \"<API key>\" # Get your free API key from: https://codex.cleanlab.ai/account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from cleanlab_codex.project import Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example RAG App: Product Customer Support\n",
    "\n",
    "Consider a customer support / e-commerce RAG use-case where the Knowledge Base contains product listings like the following:\n",
    "\n",
    "![Simple water bottle product listing](../assets/simple_water_bottle.png)\n",
    "\n",
    "Here, the inner workings of the RAG app are not important for this tutorial. What is important is that the RAG app generates a response based on a user query and a context, which are all made available for evaluation.\n",
    "\n",
    "\n",
    "For simplicity, our context is hardcoded as the product listing below. You should replace these with the outputs of your RAG system, noting that Cleanlab can detect issues in these outputs in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_listing = \"\"\"Simple Water Bottle - Amber (limited edition launched Jan 1st 2025)\n",
    "A water bottle designed with a perfect blend of functionality and aesthetics in mind. Crafted from high-quality, durable plastic with a sleek honey-colored finish.\n",
    "Price: $24.99\n",
    "Dimensions: 10 inches height x 4 inches width\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97b5298-5cea-42e5-b075-576cd6df1fca",
   "metadata": {},
   "source": [
    "**Optional: Example queries and retrieved context + generated response from RAG system dataframe**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How much water can the Simple Water Bottle hold?</td>\n",
       "      <td>Simple Water Bottle - Amber (limited edition l...</td>\n",
       "      <td>The Simple Water Bottle can hold 16 oz of Water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How can I order the Simple Water Bottle in bulk?</td>\n",
       "      <td>Simple Water Bottle - Amber (limited edition l...</td>\n",
       "      <td>Based on the available information, I cannot p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How much does the Simple Water Bottle cost?</td>\n",
       "      <td>Simple Water Bottle - Amber (limited edition l...</td>\n",
       "      <td>The Simple Water Bottle costs $10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              query  \\\n",
       "0  How much water can the Simple Water Bottle hold?   \n",
       "1  How can I order the Simple Water Bottle in bulk?   \n",
       "2       How much does the Simple Water Bottle cost?   \n",
       "\n",
       "                                             context  \\\n",
       "0  Simple Water Bottle - Amber (limited edition l...   \n",
       "1  Simple Water Bottle - Amber (limited edition l...   \n",
       "2  Simple Water Bottle - Amber (limited edition l...   \n",
       "\n",
       "                                            response  \n",
       "0    The Simple Water Bottle can hold 16 oz of Water  \n",
       "1  Based on the available information, I cannot p...  \n",
       "2                  The Simple Water Bottle costs $10  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"query\": \"How much water can the Simple Water Bottle hold?\",\n",
    "        \"context\": product_listing,\n",
    "        \"response\": \"The Simple Water Bottle can hold 16 oz of Water\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How can I order the Simple Water Bottle in bulk?\",\n",
    "        \"context\": product_listing,\n",
    "        \"response\": \"Based on the available information, I cannot provide a complete answer to this question.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How much does the Simple Water Bottle cost?\",\n",
    "        \"context\": product_listing,\n",
    "        \"response\": \"The Simple Water Bottle costs $10\"\n",
    "    },\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, your RAG system should already have functions to retrieve context, generate responses, and build a messages object to prompt the LLM with. For this tutorial, we'll simulate these functions using the above fields as well as define a simple `fallback_response` and `prompt_template`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e1d9ec-cacc-414d-9542-efac50723600",
   "metadata": {},
   "source": [
    "**Optional: Toy RAG methods you should replace with existing methods from your RAG system**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fallback_response = \"I'm sorry, I couldn't find an answer for that \u2014 can I help with something else?\"\n",
    "\n",
    "prompt_template = \"\"\"You are a customer service agent. Your task is to answer the following customer questions based on the product listing.\n",
    "\n",
    "Product Listing: {context}\n",
    "\n",
    "Customer Question: {query}\n",
    "\"\"\"\n",
    "\n",
    "def rag_retrieve_context(query):\n",
    "    \"\"\"Simulate retrieval from a knowledge base\"\"\"\n",
    "    # In a real system, this would search the knowledge base\n",
    "    for item in data:\n",
    "        if item[\"query\"] == query:\n",
    "            return item[\"context\"]\n",
    "    return \"\"\n",
    "\n",
    "def rag_generate_response(messages):\n",
    "    \"\"\"Simulate LLM response generation\"\"\"\n",
    "    # In a real system, this would call an LLM\n",
    "    user_prompt = messages[0][\"content\"]\n",
    "    query = user_prompt.split(\"Customer Question: \")[1].split(\"\\n\")[0]\n",
    "    for item in data:\n",
    "        if item[\"query\"] == query:\n",
    "            return item[\"response\"]\n",
    "    \n",
    "    # Return a fallback response if the LLM is unable to answer the question\n",
    "    return \"Based on the available information, I cannot provide a complete answer to this question.\"\n",
    "\n",
    "def rag_form_prompt(query, context):\n",
    "    \"\"\"\n",
    "    Form a prompt for your LLM response-generation step (from the user query, retrieved context, system instructions, etc). We represent the `prompt` using OpenAI's `messages` format, which matches the input to Cleanlab's `validate()` method.\n",
    "    \"\"\"\n",
    "    \n",
    "    user_prompt = prompt_template.format(query=query, context=context)\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt,\n",
    "    }]\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Cleanlab Project\n",
    "\n",
    "To later use the Cleanlab AI Platform, we must first [create a Project](/codex/web_tutorials/create_project/).\n",
    "Here we assume some (question, answer) pairs have already been added to the Cleanlab Project.\n",
    "\n",
    "Our existing Cleanlab Project contains the following entries:\n",
    "\n",
    "![Codex Project Example](../assets/codex_kb.png)\n",
    "\n",
    "User queries where Cleanlab detected a bad response from your RAG app will be logged in this Project for SMEs to later answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Programmatically populate the Codex Project with above (question, answer) pairs. Note: The recommended flow is to do this manually in the Web App.\n",
    "from cleanlab_codex.client import Client\n",
    "\n",
    "codex_client = Client()\n",
    "\n",
    "# Create a project\n",
    "project = codex_client.create_project(\n",
    "    name=\"Product FAQs\",\n",
    "    description=\"Questions about product pages\",\n",
    ")\n",
    "# Add entries to the project\n",
    "project.add_remediation(\n",
    "    question=\"How much water can the Simple Water Bottle hold?\",\n",
    "    answer=\"32oz\",\n",
    ")\n",
    "project.add_remediation(\n",
    "    question=\"Can I return my Simple Water Bottle?\",\n",
    "    answer=\"Return it within 30 days for a full refund-- no questions asked. Contact our support team to initiate your return!\",\n",
    ")\n",
    "\n",
    "access_key = project.create_access_key(\"test access key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running detection and remediation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our Cleanlab Project is configured, we can use the [`Project.validate()`](/codex/api/python/project/#method-validate) method to detect bad responses from our RAG application. A single call runs many real-time [Evals](/tlm/api/python/utils.rag/#class-eval) to score each AI response, and when scores fall below certain thresholds, the response is flagged for guardrailing or for SME review.\n",
    "\n",
    "When your AI response is flagged for SME review, the `Project.validate()` call will simultaneously query Cleanlab for an expert answer that can remediate your bad AI response. If no suitable expert answer is found, this query will be logged as `Unaddressed` in the Cleanlab Project for SMEs to answer\n",
    "\n",
    "When a response is flagged for guardrailing, the `should_guardrail` return value will be marked as `True`.  You can choose to return a safer fallback response in place of the original AI response, or escalate to a human employee rather than letting your AI handle this case. \n",
    "\n",
    "Here's some logic to determine the `final_response` to return to your user.\n",
    "\n",
    "```python\n",
    "final_response = (\n",
    "    results.expert_answer if results.expert_answer and results.escalated_to_sme  # you can optionally omit the 2nd part of the AND statement to always use expert answers when available\n",
    "    else fallback_response if results.should_guardrail\n",
    "    else initial_response\n",
    ")\n",
    "```\n",
    "\n",
    "Let's initialize the Project using our access key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_key = \"<YOUR-PROJECT-ACCESS-KEY>\"  # Obtain from your Project's settings page: https://codex.cleanlab.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = Project.from_access_key(access_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the `validate()` method to a RAG system is straightfoward. Here we do this using a helper function that applies the Validator to one row from our example dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_row_validation(df, row_index, project, verbosity=0):\n",
    "    \"\"\"\n",
    "    Detect and remediate bad responses in a specific row from the dataframe\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The dataframe containing the query, context, and response to validate.\n",
    "        row_index (int): The index of the row in the dataframe to validate.\n",
    "        project (Project): The Codex Project object used to detect bad responses and remediate them.        verbosity (int): Whether to print verbose output. Defaults to 0.\n",
    "            At verbosity level 0, only the query and final response are printed.\n",
    "            At verbosity level 1, the initial RAG response and the validation results are printed as well.\n",
    "            At verbosity level 2, the retrieved context is also printed.\n",
    "    \"\"\"\n",
    "   # 1. Get user query\n",
    "    user_query = df.iloc[row_index][\"query\"]\n",
    "    print(f\"Query: {user_query}\\n\")\n",
    "    \n",
    "    # 2. Standard RAG pipeline\n",
    "    # a. retrieve the context\n",
    "    retrieved_context = rag_retrieve_context(user_query)\n",
    "    if verbosity >= 2:\n",
    "        print(f\"Retrieved context:\\n{retrieved_context}\\n\")\n",
    "    \n",
    "    # b. build prompt for RAG system\n",
    "    messages = rag_form_prompt(\n",
    "        query=user_query,\n",
    "        context=retrieved_context,\n",
    "    )\n",
    "\n",
    "    # c. simulate LLM response generation\n",
    "    initial_response = rag_generate_response(messages)\n",
    "    if verbosity >= 1:\n",
    "        print(f\"Initial RAG response: {initial_response}\\n\")\n",
    "        \n",
    "    # 3. Detect and remediate bad responses\n",
    "    results = project.validate(\n",
    "        messages=messages,\n",
    "        response=initial_response,\n",
    "        query=user_query,\n",
    "        context=retrieved_context,\n",
    "    )\n",
    "    \n",
    "    # 4. Get the final response: \n",
    "    #    - Use the fallback_response if the response was flagged as requiring guardrails\n",
    "    #    - Use an expert answer if available and response was flagged as escalated to SME\n",
    "    #    - Otherwise, use the initial response\n",
    "\n",
    "    final_response = (\n",
    "        results.expert_answer if results.expert_answer and results.escalated_to_sme\n",
    "        else fallback_response if results.should_guardrail\n",
    "        else initial_response\n",
    "    )\n",
    "    print(f\"Final Response: {final_response}\\n\")\n",
    "    \n",
    "    # For tutorial purposes, show validation results\n",
    "    if verbosity >= 1:\n",
    "        print(\"Validation Results:\")\n",
    "        for key, value in results.model_dump().items():\n",
    "            print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's validate the RAG response to our first example query. The final dictionary printed by our helper functions are the results of `Project.validate()`, which we'll break down below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How much water can the Simple Water Bottle hold?\n",
      "\n",
      "Initial RAG response: The Simple Water Bottle can hold 16 oz of Water\n",
      "\n",
      "Final Response: 32oz\n",
      "\n",
      "Validation Results:\n",
      "    deterministic_guardrails_results: {}\n",
      "    escalated_to_sme: True\n",
      "    eval_scores: {'trustworthiness': {'score': 0.1828326551090038, 'triggered': True, 'triggered_escalation': True, 'triggered_guardrail': True, 'failed': True, 'log': None}, 'context_sufficiency': {'score': 0.0024944120492266216, 'triggered': True, 'triggered_escalation': False, 'triggered_guardrail': False, 'failed': True, 'log': None}, 'response_helpfulness': {'score': 0.9975122307805526, 'triggered': False, 'triggered_escalation': False, 'triggered_guardrail': False, 'failed': False, 'log': None}, 'query_ease': {'score': 0.815418802947351, 'triggered': False, 'triggered_escalation': False, 'triggered_guardrail': False, 'failed': False, 'log': None}, 'response_groundedness': {'score': 0.0024875794784465807, 'triggered': False, 'triggered_escalation': False, 'triggered_guardrail': False, 'failed': False, 'log': None}}\n",
      "    expert_answer: 32oz\n",
      "    is_bad_response: True\n",
      "    should_guardrail: True\n"
     ]
    }
   ],
   "source": [
    "df_row_validation(df, 0, project, verbosity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Project.validate()` method returns a comprehensive dictionary containing multiple evaluation metrics and remediation options. Let's examine the key components of these results:\n",
    "\n",
    "### Core Validation Results\n",
    "\n",
    "1. **`expert_answer` (String | None)**\n",
    "   - Contains the remediation response retrieved from the Cleanlab Project.\n",
    "   - Returns `None` in two scenarios:\n",
    "     - When `escalated_to_sme` is `False` (indicating no remediation needed, so Cleanlab is not queried).\n",
    "     - When no suitable expert answer exists in the Cleanlab Project for similar queries.\n",
    "   - Returns a string containing the expert-provided answer when:\n",
    "     - The response is flagged as requiring remediation (`escalated_to_sme=True`).\n",
    "     - A semantically similar query exists in the Cleanlab Project with an expert answer.\n",
    "\n",
    "2. **`escalated_to_sme` (Boolean)**\n",
    "   - This is the primary validation indicator that determines if a response requires remediation (i.e. for `expert_answer` to contain a string value).\n",
    "   - Will be `True` if any eval fails with `should_escalate=True`, meaning the score for that specific eval falls below a configured threshold.\n",
    "   - Controls whether the system will attempt to fetch an expert answer from Cleanlab. When `escalated_to_sme=True` the system will lookup an expert answer from Cleanlab (which logs the corresponding query into the Cleanlab Project).\n",
    "\n",
    "4. **`should_guardrail` (Boolean)**\n",
    "   - Will be `True` when any configured guardrails are triggered with `should_guardrail=True`, meaning the score for that specific guardrail falls below a configured threshold.\n",
    "   - Does not trigger checking Cleanlab for an expert answer and flagging the query for review.\n",
    "   \n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "Each evaluation metric has a `triggered_guardrail` and `triggered_escalation` boolean flag that indicates whether the metric's score falls below its configured threshold, which determines if a response needs remediation or guardrailing.\n",
    "\n",
    "By default, the `Project.validate()` method uses the following metrics as **Evaluations** for escalation:\n",
    "- `trustworthiness`: overall confidence that your RAG system's response is correct.\n",
    "- `response_helpfulness`: evaluates whether the response attempts to helpfully address the user query vs. abstaining or saying 'I don't know'.\n",
    "\n",
    "By default, the `Project.validate()` method uses the following metrics as **Guardrails**:\n",
    "- `trustworthiness`: overall confidence that your RAG system's response is correct (used for guardrailing and escalation).\n",
    "\n",
    "You can modify these metrics or add your own by defining a custom list of *Evaluations* and/or *Guardrails* for a Project in the Cleanlab Web App.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's validate another example from our RAG system.  For this example, the response is flagged as bad, but no expert answer is available in the Cleanlab Project. The corresponding query will be logged there for SMEs to answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How can I order the Simple Water Bottle in bulk?\n",
      "\n",
      "Initial RAG response: Based on the available information, I cannot provide a complete answer to this question.\n",
      "\n",
      "Final Response: I'm sorry, I couldn't find an answer for that \u2014 can I help with something else?\n",
      "\n",
      "Validation Results:\n",
      "    deterministic_guardrails_results: {}\n",
      "    escalated_to_sme: True\n",
      "    eval_scores: {'trustworthiness': {'score': 0.5637340040550474, 'triggered': True, 'triggered_escalation': True, 'triggered_guardrail': True, 'failed': True, 'log': None}, 'context_sufficiency': {'score': 0.0024907979417248738, 'triggered': True, 'triggered_escalation': False, 'triggered_guardrail': False, 'failed': True, 'log': None}, 'response_helpfulness': {'score': 0.003276786800052492, 'triggered': True, 'triggered_escalation': True, 'triggered_guardrail': False, 'failed': True, 'log': None}, 'query_ease': {'score': 0.9975124059276235, 'triggered': False, 'triggered_escalation': False, 'triggered_guardrail': False, 'failed': False, 'log': None}, 'response_groundedness': {'score': 0.9975124079370519, 'triggered': False, 'triggered_escalation': False, 'triggered_guardrail': False, 'failed': False, 'log': None}}\n",
      "    expert_answer: None\n",
      "    is_bad_response: True\n",
      "    should_guardrail: True\n"
     ]
    }
   ],
   "source": [
    "df_row_validation(df, 1, project, verbosity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RAG system is unable to answer this question because there is no relevant information in the retrieved context, nor has a similar question been answered in the Cleanlab Project (see the contents of the Cleanlab Project above).\n",
    "\n",
    "**Cleanlab automatically recognizes this question could not be answered and logs it into the Project where it awaits an answer from a SME (notice that `escalated_to_sme` is True).**\n",
    "Navigate to your Cleanlab Project in the [Web App](https://codex.cleanlab.ai/) where you (or a SME at your company) can enter the desired answer for this query.\n",
    "\n",
    "As soon as an answer is provided in Cleanlab, our RAG system will be able to answer all similar questions going forward (as seen for the previous query)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Usage\n",
    "\n",
    "You can configure many aspects of the bad response detection like what score thresholds to use inside the Cleanlab Web App.\n",
    "\n",
    "### Response Quality Thresholds\n",
    "\n",
    "Thresholds determine when a response needs intervention:\n",
    "- A metric (trustworthiness, pii_detection, etc.) can be defined as an Evaluation, Guardrail , or both.\n",
    "- Each metric has its own threshold (0-1) and a defined threshold direction (below/above).\n",
    "- If any Evaluation's score violates the threshold condition, the response is escalated to an SME and an expert answer is attempted to be returned.\n",
    "- If any Guardrail's score violates the threshold condition, `Project.validate()` returns with the `should_guardrail=True` flag and no other action is taken.\n",
    "\n",
    "- Example: With Evaluation metric *Hallucination (trustworthiness)* threshold set to \"below 0.85\"\n",
    "  - Score 0.80 -> Escalated to an SME\n",
    "  - Score 0.90 -> Passes validation\n",
    "\n",
    "Setting thresholds affects your validation strategy:\n",
    "- Higher thresholds (e.g. 0.9) = Stricter validation\n",
    "  - More responses marked as \"should_escalate\"\n",
    "  - More queries logged for SMEs to answer\n",
    "  - Better response quality but higher SME workload\n",
    "   \n",
    "- Lower thresholds (e.g. 0.7) = More lenient validation\n",
    "  - Fewer responses marked as \"should_escalate\"\n",
    "  - Fewer queries logged for SMEs to answer\n",
    "  - Lower SME workload, but may allow lower quality responses from your RAG app to be returned unremediated.\n",
    "\n",
    "### Configure Custom Evaluations and Guardrails\n",
    "\n",
    "You can configure these directly in the Cleanlab Web UI. For a detailed walkthrough, see the [Adding custom guardrails](/codex/tutorials/azure/Azure_Guardrails_CodexAsBackup/#adding-custom-guardrails-to-your-cleanlab-project) section of our other tutorial.\n",
    "\n",
    "\n",
    "### Logging Additional Information\n",
    "\n",
    "When `project.validate()` returns results indicating a response should be escalated to an SME, it logs the query into your Cleanlab Project. By default, this log automatically includes the evaluation scores (like trustworthiness), the context and LLM response.\n",
    "\n",
    "You can include additional information that would be helpful for Subject Matter Experts (SMEs) when they review the logged queries in the Cleanlab Project later.\n",
    "\n",
    "To add any extra information, simply pass in any key-value pairs into the `metadata` parameter in the `validate()` method. For example, you can add the location the Query came from like so:\n",
    "\n",
    "```python\n",
    "metadata = {\"location\": \"USA\"}\n",
    "results = project.validate(\n",
    "  messages=messages, \n",
    "  response=response, \n",
    "  query=query, \n",
    "  context=context,\n",
    "  metadata=metadata,\n",
    ")\n",
    "```\n",
    "\n",
    "Run the example below to add an entry into Cleanlab that contains this additional metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_row_log_metadata(df, row_index, project, verbosity=0):\n",
    "    \"\"\"\n",
    "    Detect and remediate bad responses in a specific row from the dataframe\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The dataframe containing the query, context, and response to validate.\n",
    "        row_index (int): The index of the row in the dataframe to validate.\n",
    "        project (Project): The Codex Project object used to detect bad responses and remediate them.\n",
    "        verbosity (int): Whether to print verbose output. Defaults to 0.\n",
    "            At verbosity level 0, only the query and final response are printed.\n",
    "            At verbosity level 1, the initial RAG response and the validation results are printed as well.\n",
    "            At verbosity level 2, the retrieved context is also printed.\n",
    "    \"\"\"\n",
    "   # 1. Get user query\n",
    "    user_query = df.iloc[row_index][\"query\"]\n",
    "    print(f\"Query: {user_query}\\n\")\n",
    "    \n",
    "    # 2. Standard RAG pipeline\n",
    "    # a. retrieve the context\n",
    "    retrieved_context = rag_retrieve_context(user_query)\n",
    "    if verbosity >= 2:\n",
    "        print(f\"Retrieved context:\\n{retrieved_context}\\n\")\n",
    "    \n",
    "    # b. build prompt for RAG system\n",
    "    messages = rag_form_prompt(\n",
    "        query=user_query,\n",
    "        context=retrieved_context,\n",
    "    )\n",
    "\n",
    "    # c. simulate LLM response generation\n",
    "    initial_response = rag_generate_response(messages)\n",
    "    if verbosity >= 1:\n",
    "        print(f\"Initial RAG response: {initial_response}\\n\")\n",
    "        \n",
    "    # 3. Detect and remediate bad responses\n",
    "    results = project.validate(\n",
    "        messages=messages,\n",
    "        response=initial_response,\n",
    "        query=user_query,\n",
    "        context=retrieved_context,\n",
    "        metadata={\"location\": \"USA\"},\n",
    "    )\n",
    "    \n",
    "    # 4. Get the final response: \n",
    "    #    - Use the fallback_response if the response was flagged as requiring guardrails\n",
    "    #    - Use an expert answer if available and response was flagged as escalated to SME\n",
    "    #    - Otherwise, use the initial response\n",
    "\n",
    "    final_response = (\n",
    "        results.expert_answer if results.expert_answer and results.escalated_to_sme\n",
    "        else fallback_response if results.should_guardrail\n",
    "        else initial_response\n",
    "    )\n",
    "    print(f\"Final Response: {final_response}\\n\")\n",
    "    \n",
    "    # For tutorial purposes, show validation results\n",
    "    if verbosity >= 1:\n",
    "        print(\"Validation Results:\")\n",
    "        for key, value in results.model_dump().items():\n",
    "            print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How much does the Simple Water Bottle cost?\n",
      "\n",
      "Initial RAG response: The Simple Water Bottle costs $10\n",
      "\n",
      "Final Response: I'm sorry, I couldn't find an answer for that \u2014 can I help with something else?\n",
      "\n",
      "Validation Results:\n",
      "    deterministic_guardrails_results: {}\n",
      "    escalated_to_sme: True\n",
      "    eval_scores: {'trustworthiness': {'score': 0.08583690987124462, 'triggered': True, 'triggered_escalation': True, 'triggered_guardrail': True, 'failed': True, 'log': None}, 'context_sufficiency': {'score': 0.9975124378114657, 'triggered': False, 'triggered_escalation': False, 'triggered_guardrail': False, 'failed': False, 'log': None}, 'response_helpfulness': {'score': 0.9975124367477299, 'triggered': False, 'triggered_escalation': False, 'triggered_guardrail': False, 'failed': False, 'log': None}, 'query_ease': {'score': 0.9968973517119587, 'triggered': False, 'triggered_escalation': False, 'triggered_guardrail': False, 'failed': False, 'log': None}, 'response_groundedness': {'score': 0.002487575280819864, 'triggered': False, 'triggered_escalation': False, 'triggered_guardrail': False, 'failed': False, 'log': None}}\n",
      "    expert_answer: None\n",
      "    is_bad_response: True\n",
      "    should_guardrail: True\n"
     ]
    }
   ],
   "source": [
    "df_row_log_metadata(df, 2, project, verbosity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that Cleanlab is integrated with your *Single-turn Q&A App*, you and SMEs can [open the Cleanlab Project and answer questions](/codex/web_tutorials/codex_as_sme/) logged there to continuously improve your AI.\n",
    "\n",
    "This tutorial demonstrated how to use Cleanlab to automatically detect and remediate bad responses in any Single-turn Q&A application. Cleanlab provides a robust way to evaluate response quality and automatically fetch expert answers when needed. For responses that don't meet quality thresholds, Cleanlab automatically logs the queries for SME review.\n",
    "\n",
    "**Note:** Automatic detection and remediation of bad responses for a *Multi-turn Conversational Chat app* is covered in the [Detect and Remediate bad responses in Conversational Apps](/codex/tutorials/other_rag_frameworks/validator/) tutorial.\n",
    "\n",
    "**Adding Cleanlab only improves your RAG app.** Once integrated, it automatically identifies problematic responses and either remediates them with expert answers or logs them for review. Using a [simple web interface](/codex/web_tutorials/codex_as_sme/), SMEs at your company can answer the highest priority questions in the Cleanlab Project. As soon as an answer is entered in Cleanlab, your RAG app will be able to properly handle all similar questions encountered in the future.\n",
    "\n",
    "Cleanlab is **the fastest way for nontechnical SMEs to directly improve your RAG system**. As the Developer, you simply integrate Cleanlab once, and from then on, SMEs can continuously improve how your system handles common user queries without needing your help.\n",
    "\n",
    "Need help, more capabilities, or other deployment options?  \n",
    "Check the [FAQ](/codex/FAQ/) or email us at: support@cleanlab.ai"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}