{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect and remediate bad responses from conversational RAG applications\n",
    "\n",
    "This tutorial demonstrates how to automatically improve any *multi-turn chat* RAG application by integrating Codex as-a-backup. The Codex API takes in the AI-generated response from your RAG app, and the same inputs provided to the LLM that generated it: user query, retrieved context, and any parts of the LLM prompt (including the chat history). Cleanlab will automatically detect if your AI response is bad (e.g., untrustworthy, unhelpful, or unsafe).  The Codex API returns these real-time evaluation scores which you can use to guardrail your AI. If your AI response is flagged as bad, the Codex API will also return an expert response whenever a similar query has been answered in the connected Codex Project, or otherwise log this query into the Codex Project for SMEs to answer.\n",
    "\n",
    "!['Codex as a backup'](../assets/codexasbackup.png)\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "Here's all the code needed for using Codex as-a-backup with your RAG system:\n",
    "```python\n",
    "from cleanlab_codex import Project\n",
    "project = Project.from_access_key(access_key)\n",
    "\n",
    "# Your existing RAG code:\n",
    "context = rag_retrieve_context(user_query)\n",
    "messages = rag_form_prompt(user_query, context, conversation_history)\n",
    "response = rag_generate_response(messages)\n",
    "\n",
    "# Detect bad responses and remediate with Cleanlab\n",
    "results = project.validate(messages=messages, query=user_query, context=context, response=response)\n",
    "\n",
    "final_response = (\n",
    "    results.expert_answer if results.expert_answer and results.escalated_to_sme\n",
    "    else fallback_response if results.should_guardrail\n",
    "    else response\n",
    ")\n",
    "\n",
    "# Update the conversation history\n",
    "conversation_history.append({\"role\": \"user\", \"content\": user_query})\n",
    "conversation_history.append({\"role\": \"assistant\", \"content\": final_response})\n",
    "```\n",
    "\n",
    "**Note:** This tutorial is for *Multi-turn Chat Apps*. If you have a *Single-turn Q&A app*, a similar workflow is covered in the [Detect and Remediate bad responses in Single-turn Q&A Apps](/codex/tutorials/other_rag_frameworks/validator/) tutorial.\n",
    "\n",
    "## Setup\n",
    "\n",
    "This tutorial requires a Codex API key. Get one [here](https://codex.cleanlab.ai/account)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade cleanlab-codex pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your Codex API key\n",
    "import os\n",
    "os.environ[\"CODEX_API_KEY\"] = \"<API key>\" # Get your free API key from: https://codex.cleanlab.ai/account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from cleanlab_codex.project import Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example RAG App: Product Customer Support\n",
    "\n",
    "Consider a customer support / e-commerce RAG use-case where the Knowledge Base contains product listings like the following:\n",
    "\n",
    "![Simple water bottle product listing](../assets/simple_water_bottle.png)\n",
    "\n",
    "Here, the inner workings of the RAG app are not important for this tutorial. What is important is that the RAG app generates a *response* based on a *user query*, a *context*, and a prior *conversation history*, which are all made available for evaluation.\n",
    "\n",
    "For simplicity, our context is hardcoded as the product listing below. Similarly, the current step of the conversation history is hardcoded as well. You should replace these with the outputs of your RAG system, noting that Cleanlab can detect issues in these outputs in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_listing = \"\"\"Simple Water Bottle - Amber (limited edition launched Jan 1st 2025)\n",
    "A water bottle designed with a perfect blend of functionality and aesthetics in mind. Crafted from high-quality, durable plastic with a sleek honey-colored finish.\n",
    "Price: $24.99\n",
    "Dimensions: 10 inches height x 4 inches width\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f43c705-2726-46ea-87ae-295c98e79d8f",
   "metadata": {},
   "source": [
    "**Optional: Example queries, retrieved context + generated response from RAG system dataframe**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How much does the Simple Water Bottle cost?</td>\n",
       "      <td>Simple Water Bottle - Amber (limited edition l...</td>\n",
       "      <td>The Simple Water Bottle costs $24.99.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can I ask a question about the Simple Water Bo...</td>\n",
       "      <td>Simple Water Bottle - Amber (limited edition l...</td>\n",
       "      <td>Sure! What would you like to know?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How much water can it hold?</td>\n",
       "      <td>Simple Water Bottle - Amber (limited edition l...</td>\n",
       "      <td>The Simple Water Bottle can hold 16 oz of Water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can I bulk order the bottles?</td>\n",
       "      <td>Simple Water Bottle - Amber (limited edition l...</td>\n",
       "      <td>I am sorry, I do not have information about bu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0        How much does the Simple Water Bottle cost?   \n",
       "1  Can I ask a question about the Simple Water Bo...   \n",
       "2                        How much water can it hold?   \n",
       "3                      Can I bulk order the bottles?   \n",
       "\n",
       "                                             context  \\\n",
       "0  Simple Water Bottle - Amber (limited edition l...   \n",
       "1  Simple Water Bottle - Amber (limited edition l...   \n",
       "2  Simple Water Bottle - Amber (limited edition l...   \n",
       "3  Simple Water Bottle - Amber (limited edition l...   \n",
       "\n",
       "                                            response  \n",
       "0              The Simple Water Bottle costs $24.99.  \n",
       "1                 Sure! What would you like to know?  \n",
       "2    The Simple Water Bottle can hold 16 oz of Water  \n",
       "3  I am sorry, I do not have information about bu...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"query\": \"How much does the Simple Water Bottle cost?\",\n",
    "        \"context\": product_listing,\n",
    "        \"response\": \"The Simple Water Bottle costs $24.99.\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Can I ask a question about the Simple Water Bottle?\",\n",
    "        \"context\": product_listing,\n",
    "        \"response\": \"Sure! What would you like to know?\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How much water can it hold?\",\n",
    "        \"context\": product_listing,\n",
    "        \"response\": \"The Simple Water Bottle can hold 16 oz of Water\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Can I bulk order the bottles?\",\n",
    "        \"context\": product_listing,\n",
    "        \"response\": \"I am sorry, I do not have information about bulk orders for the Simple Water Bottle.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, your RAG system should already have functions to retrieve context, generate responses, and build a messages object to prompt the LLM with. \n",
    "\n",
    "For this tutorial, we'll simulate these functions using the above fields as well as define a simple `fallback_response`, `system_prompt`, and `prompt_template`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9906f34f-939c-4c0b-9cf8-e76dcdc7042d",
   "metadata": {},
   "source": [
    "**Optional: Toy RAG methods you should replace with existing methods from your RAG system**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fallback_response = \"I'm sorry, I couldn't find an answer for that \u2014 can I help with something else?\"\n",
    "\n",
    "system_prompt = \"You are a customer service agent. Be polite and concise in your responses.\"\n",
    "\n",
    "prompt_template = \"\"\"Answer the following customer question based on the product listing.\n",
    "\n",
    "Customer Question: {query}\n",
    "\"\"\"\n",
    "\n",
    "def rag_retrieve_context(query):\n",
    "    \"\"\"Simulate retrieval from a knowledge base\"\"\"\n",
    "    # In a real system, this would search the knowledge base\n",
    "    for item in data:\n",
    "        if item[\"query\"] == query:\n",
    "            return item[\"context\"]\n",
    "    return \"\"\n",
    "\n",
    "def rag_generate_response(messages):\n",
    "    \"\"\"Simulate LLM response generation\"\"\"\n",
    "    # In a real system, this would call an LLM\n",
    "    for item in data:\n",
    "        if item[\"query\"] in messages[-1][\"content\"]:\n",
    "            return item[\"response\"]\n",
    "    \n",
    "    # Return a fallback response if the LLM is unable to answer the question\n",
    "    return fallback_response\n",
    "\n",
    "def rag_form_prompt(query, context, conversation_history):\n",
    "    \"\"\"Form a prompt for your LLM response-generation step (from the user query, retrieved context, conversation history, system instructions, etc). We represent the `prompt` in OpenAI's `messages` format, which matches the input to Cleanlab's `validate()` method.\n",
    "    \n",
    "    **Note:** In `messages`, it is recommended to inject retrieved context into the system prompt rather than each user message.\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt_with_context = {\"role\": \"system\", \"content\": system_prompt + f\"\\n\\nContext\\n{context}\\n\"}\n",
    "    user_query_prompt = {\"role\": \"user\", \"content\": prompt_template.format(query=query)}\n",
    "    messages = [system_prompt_with_context] + conversation_history + [user_query_prompt]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Codex Project\n",
    "\n",
    "To later use Codex, we must first [create a Project](/codex/web_tutorials/create_project/).\n",
    "Here we assume some (question, answer) pairs have already been added to the Codex Project.\n",
    "\n",
    "Our existing Codex Project contains the following entries:\n",
    "\n",
    "![Codex Project Example](../assets/codex_kb.png)\n",
    "\n",
    "User queries where Codex detected a bad response from your RAG app will be logged in this Project for SMEs to later answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Programmatically populate the Codex Project with above (question, answer) pairs. Note: The recommended flow is to do this manually in the Web App.\n",
    "from cleanlab_codex.client import Client\n",
    "\n",
    "codex_client = Client()\n",
    "\n",
    "# Create a project\n",
    "project = codex_client.create_project(\n",
    "    name=\"Product FAQs\",\n",
    "    description=\"Questions about product pages\",\n",
    ")\n",
    "# Add entries to the project\n",
    "project.add_remediation(\n",
    "    question=\"How much water can the Simple Water Bottle hold?\",\n",
    "    answer=\"32oz\",\n",
    ")\n",
    "project.add_remediation(\n",
    "    question=\"Can I return my Simple Water Bottle?\",\n",
    "    answer=\"Return it within 30 days for a full refund-- no questions asked. Contact our support team to initiate your return!\",\n",
    ")\n",
    "\n",
    "access_key = project.create_access_key(\"test access key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running detection and remediation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our Codex Project is configured, we can use the `Project.validate()` method to detect bad responses from our RAG application. A single call runs many real-time [Evals](/tlm/api/python/utils.rag/#class-eval) to score each AI response, and when scores fall below certain thresholds, the response is flagged for guardrailing or for SME review.\n",
    "\n",
    "When your AI response is flagged for SME review, the `Project.validate()` call will simultaneously query Codex for an expert answer that can remediate your bad AI response. If no suitable expert answer is found, this query will be logged as `Unaddressed` in the Codex Project for SMEs to answer\n",
    "\n",
    "When a response is flagged for guardrailing, the `should_guardrail` return value will be marked as `True`.  You can choose to return a safer fallback response in place of the original AI response, or escalate to a human employee rather than letting your AI handle this case. \n",
    "\n",
    "Here's some logic to determine the `final_response` to return to your user.\n",
    "\n",
    "```python\n",
    "final_response = (\n",
    "    results.expert_answer if results.expert_answer and results.escalated_to_sme  # you can optionally omit the 2nd part of the AND statement to always use expert answers when available\n",
    "    else fallback_response if results.should_guardrail\n",
    "    else initial_response\n",
    ")\n",
    "```\n",
    "\n",
    "Let's initialize the Project using our access key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_key = \"<YOUR-PROJECT-ACCESS-KEY>\"  # Obtain from your Project's settings page: https://codex.cleanlab.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = Project.from_access_key(access_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the `Project.validate()` method to a RAG system is straightfoward. Here we do this using a mock RAG chat application and a helper function that validates a single chat turn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929b0660-3b9b-46aa-b080-9d808c0e7f45",
   "metadata": {},
   "source": [
    "**Optional: Toy RAG.chat() method for a single turn of a conversation.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rag_chat(query, conversation_history):\n",
    "    \"\"\"Ask a question to the RAG pipeline and return the response.\"\"\"\n",
    "    # Add user query to the conversation history\n",
    "    updated_conversation_history = conversation_history + [{\"role\": \"user\", \"content\": query}]\n",
    "\n",
    "    context = rag_retrieve_context(query)\n",
    "    messages = rag_form_prompt(\n",
    "        query=query,\n",
    "        context=context,\n",
    "        conversation_history=updated_conversation_history,\n",
    "    )\n",
    "\n",
    "    response = rag_generate_response(messages)\n",
    "    \n",
    "    # Add response to the updated conversation history\n",
    "    updated_conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "    \n",
    "    rag_response = {\n",
    "        \"query\": query,\n",
    "        \"context\": context,\n",
    "        \"response\": response,\n",
    "        \"messages\": messages,\n",
    "        \"conversation_history\": updated_conversation_history,\n",
    "    }\n",
    "    return rag_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(user_query, conversation_history, project, verbosity=0):\n",
    "    \"\"\"\n",
    "    Detect and remediate bad responses in a chat turn of a Conversational RAG system.\n",
    "    \n",
    "    Args:\n",
    "        user_query (str): The user query to validate.\n",
    "        conversation_history (list): The conversation history to validate.\n",
    "        project (Project): The Codex Project object used to detect bad responses and remediate them.        verbosity (int): Whether to print verbose output. Defaults to 0.\n",
    "            At verbosity level 0, only the query and final response are printed.\n",
    "            At verbosity level 1, the initial RAG response and the validation results are printed as well.\n",
    "            At verbosity level 2, the retrieved context is also printed.\n",
    "\n",
    "    Returns:\n",
    "        list: Updated conversation history with the final response.\n",
    "    \"\"\"\n",
    "    print(f\"User Query: {user_query}\\n\")\n",
    "\n",
    "    # 1. Run standard RAG pipeline\n",
    "    rag_response = rag_chat(user_query, conversation_history)\n",
    "    if verbosity >= 2:\n",
    "        print(f\"Retrieved context:\\n{rag_response['context']}\\n\")\n",
    "    if verbosity >= 1:\n",
    "        print(f\"Initial RAG response: {rag_response['response']}\\n\")\n",
    "        \n",
    "    # 3. Detect and remediate bad responses\n",
    "    results = project.validate(\n",
    "        messages=rag_response[\"messages\"],\n",
    "        response=rag_response[\"response\"],\n",
    "        query=user_query,\n",
    "        context=rag_response[\"context\"],\n",
    "    )\n",
    "    \n",
    "    # 4. Get the final response: \n",
    "    #    - Use the fallback_response if the response was flagged as requiring guardrails\n",
    "    #    - Use an expert answer if available and response was flagged as escalated to SME\n",
    "    #    - Otherwise, use the initial response\n",
    "\n",
    "    final_response = (\n",
    "        results.expert_answer if results.expert_answer and results.escalated_to_sme\n",
    "        else fallback_response if results.should_guardrail\n",
    "        else rag_response[\"response\"]  # initial response from the RAG system\n",
    "    )\n",
    "    print(f\"Final Response: {final_response}\\n\")\n",
    "    \n",
    "    # 5. Update the conversation history with the final response (instead of the initial response)\n",
    "    updated_conversation_history = rag_response[\"conversation_history\"]\n",
    "    updated_conversation_history[-1] = {\"role\": \"assistant\", \"content\": final_response}\n",
    "\n",
    "    # For tutorial purposes, show validation results and conversation history\n",
    "    if verbosity >= 2:\n",
    "        print(\"Conversation History:\")\n",
    "        for message in rag_response[\"conversation_history\"]:\n",
    "            print(f\"    {message['role'].capitalize()}: {message['content']}\")\n",
    "        print()\n",
    "    if verbosity >= 1:\n",
    "        print(\"Validation Results:\")\n",
    "        for key, value in results.model_dump().items():\n",
    "            print(f\"    {key}: {value}\")\n",
    "        print()\n",
    "    return updated_conversation_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's validate the RAG response to our first example conversation. Notice the conversational opening statement and the LLM response to it was not escalated to the SME or guardrailed and the \"Final Response\" stayed the same as the \"Initial RAG response\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: Can I ask a question about the Simple Water Bottle?\n",
      "\n",
      "Initial RAG response: Sure! What would you like to know?\n",
      "\n",
      "Final Response: Sure! What would you like to know?\n",
      "\n",
      "Validation Results:\n",
      "    deterministic_guardrails_results: {}\n",
      "    escalated_to_sme: False\n",
      "    eval_scores: {'trustworthiness': {'score': 0.9999969547409946, 'triggered': False, 'triggered_escalation': False, 'triggered_guardrail': False, 'failed': False, 'log': None}, 'context_sufficiency': {'score': 0.6435045579137594, 'triggered': False, 'triggered_escalation': False, 'triggered_guardrail': False, 'failed': False, 'log': None}, 'response_helpfulness': {'score': 0.9867708789410339, 'triggered': False, 'triggered_escalation': False, 'triggered_guardrail': False, 'failed': False, 'log': None}, 'query_ease': {'score': 0.9975124364322093, 'triggered': False, 'triggered_escalation': False, 'triggered_guardrail': False, 'failed': False, 'log': None}, 'response_groundedness': {'score': 0.9975124310861848, 'triggered': False, 'triggered_escalation': False, 'triggered_guardrail': False, 'failed': False, 'log': None}}\n",
      "    expert_answer: None\n",
      "    is_bad_response: False\n",
      "    should_guardrail: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conversation_history = []\n",
    "query = \"Can I ask a question about the Simple Water Bottle?\"\n",
    "conversation_history = run_validation(query, conversation_history, project, verbosity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets ask a followup question and further break down the returned keys below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: How much does the Simple Water Bottle cost?\n",
      "\n",
      "Retrieved context:\n",
      "Simple Water Bottle - Amber (limited edition launched Jan 1st 2025)\n",
      "A water bottle designed with a perfect blend of functionality and aesthetics in mind. Crafted from high-quality, durable plastic with a sleek honey-colored finish.\n",
      "Price: $24.99\n",
      "Dimensions: 10 inches height x 4 inches width\n",
      "\n",
      "Initial RAG response: The Simple Water Bottle costs $24.99.\n",
      "\n",
      "Final Response: The Simple Water Bottle costs $24.99.\n",
      "\n",
      "Conversation History:\n",
      "    User: Can I ask a question about the Simple Water Bottle?\n",
      "    Assistant: Sure! What would you like to know?\n",
      "    User: How much does the Simple Water Bottle cost?\n",
      "    Assistant: The Simple Water Bottle costs $24.99.\n",
      "\n",
      "Validation Results:\n",
      "    deterministic_guardrails_results: {}\n",
      "    escalated_to_sme: False\n",
      "    eval_scores: {'trustworthiness': {'score': 0.984763948497854, 'triggered': False, 'triggered_escalation': False, 'triggered_guardrail': False, 'failed': False, 'log': None}, 'context_sufficiency': {'score': 0.9975124378114657, 'triggered': False, 'triggered_escalation': False, 'triggered_guardrail': False, 'failed': False, 'log': None}, 'response_helpfulness': {'score': 0.9975124375104937, 'triggered': False, 'triggered_escalation': False, 'triggered_guardrail': False, 'failed': False, 'log': None}, 'query_ease': {'score': 0.9971390047732863, 'triggered': False, 'triggered_escalation': False, 'triggered_guardrail': False, 'failed': False, 'log': None}, 'response_groundedness': {'score': 0.9975124378113617, 'triggered': False, 'triggered_escalation': False, 'triggered_guardrail': False, 'failed': False, 'log': None}}\n",
      "    expert_answer: None\n",
      "    is_bad_response: False\n",
      "    should_guardrail: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"How much does the Simple Water Bottle cost?\"\n",
    "conversation_history = run_validation(query, conversation_history, project, verbosity=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Project.validate()` method returns a comprehensive dictionary containing multiple evaluation metrics and remediation options. Let's examine the key components of these results:\n",
    "\n",
    "### Core Validation Results\n",
    "\n",
    "1. **`expert_answer` (String | None)**\n",
    "   - Contains the remediation response retrieved from the Codex Project.\n",
    "   - Returns `None` in two scenarios:\n",
    "     - When `escalated_to_sme` is `False` (indicating no remediation needed, so Codex is not queried).\n",
    "     - When no suitable expert answer exists in the Codex Project for similar queries.\n",
    "   - Returns a string containing the expert-provided answer when:\n",
    "     - The response is flagged as requiring remediation (`escalated_to_sme=True`).\n",
    "     - A semantically similar query exists in the Codex Project with an expert answer.\n",
    "\n",
    "2. **`escalated_to_sme` (Boolean)**\n",
    "   - This is the primary validation indicator that determines if a response requires remediation (i.e. for `expert_answer` to contain a string value).\n",
    "   - Will be `True` if any eval fails with `should_escalate=True`, meaning the score for that specific eval falls below a configured threshold.\n",
    "   - Controls whether the system will attempt to fetch an expert answer from Codex. When `escalated_to_sme=True` the system will lookup an expert answer from Codex (which logs the corresponding query into the Codex Project).\n",
    "\n",
    "4. **`should_guardrail` (Boolean)**\n",
    "   - Will be `True` when any configured guardrails are triggered with `should_guardrail=True`, meaning the score for that specific guardrail falls below a configured threshold.\n",
    "   - Does not trigger checking Codex for an expert answer and flagging the query for review.\n",
    "\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "Each evaluation metric has a `triggered_guardrail` and `triggered_escalation` boolean flag that indicates whether the metric's score falls below its configured threshold, which determines if a response needs remediation or guardrailing.\n",
    "\n",
    "By default, the `Project.validate()` method uses the following metrics as **Evaluations** for escalation:\n",
    "- `trustworthiness`: overall confidence that your RAG system's response is correct.\n",
    "- `response_helpfulness`: evaluates whether the response attempts to helpfully address the user query vs. abstaining or saying 'I don't know'.\n",
    "\n",
    "By default, the `Project.validate()` method uses the following metrics as **Guardrails**:\n",
    "- `trustworthiness`: overall confidence that your RAG system's response is correct (used for guardrailing and escalation).\n",
    "\n",
    "You can modify these metrics or add your own by defining a custom list of *Evaluations* and/or *Guardrails* for a Project in the Codex Web App."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating an unhelpful response\n",
    "\n",
    "Let's continue our conversation and validate another example from our RAG system. \n",
    "\n",
    "The response is flagged as bad, but no expert answer is available in the Codex Project. The corresponding query will be logged there for SMEs to answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: Can I bulk order the bottles?\n",
      "\n",
      "Retrieved context:\n",
      "Simple Water Bottle - Amber (limited edition launched Jan 1st 2025)\n",
      "A water bottle designed with a perfect blend of functionality and aesthetics in mind. Crafted from high-quality, durable plastic with a sleek honey-colored finish.\n",
      "Price: $24.99\n",
      "Dimensions: 10 inches height x 4 inches width\n",
      "\n",
      "Initial RAG response: I am sorry, I do not have information about bulk orders for the Simple Water Bottle.\n",
      "\n",
      "Final Response: I am sorry, I do not have information about bulk orders for the Simple Water Bottle.\n",
      "\n",
      "Conversation History:\n",
      "    User: Can I ask a question about the Simple Water Bottle?\n",
      "    Assistant: Sure! What would you like to know?\n",
      "    User: How much does the Simple Water Bottle cost?\n",
      "    Assistant: The Simple Water Bottle costs $24.99.\n",
      "    User: Can I bulk order the bottles?\n",
      "    Assistant: I am sorry, I do not have information about bulk orders for the Simple Water Bottle.\n",
      "    User: Can I bulk order the bottles?\n",
      "    Assistant: I am sorry, I do not have information about bulk orders for the Simple Water Bottle.\n",
      "\n",
      "Validation Results:\n",
      "    deterministic_guardrails_results: {}\n",
      "    escalated_to_sme: True\n",
      "    eval_scores: {'trustworthiness': {'score': 0.8075889311812013, 'triggered': False, 'triggered_escalation': False, 'triggered_guardrail': False, 'failed': False, 'log': None}, 'context_sufficiency': {'score': 0.002487621636582364, 'triggered': True, 'triggered_escalation': False, 'triggered_guardrail': False, 'failed': True, 'log': None}, 'response_helpfulness': {'score': 0.2058643027089209, 'triggered': True, 'triggered_escalation': True, 'triggered_guardrail': False, 'failed': True, 'log': None}, 'query_ease': {'score': 0.9975124247214743, 'triggered': False, 'triggered_escalation': False, 'triggered_guardrail': False, 'failed': False, 'log': None}, 'response_groundedness': {'score': 0.9973973081739447, 'triggered': False, 'triggered_escalation': False, 'triggered_guardrail': False, 'failed': False, 'log': None}}\n",
      "    expert_answer: None\n",
      "    is_bad_response: True\n",
      "    should_guardrail: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Can I bulk order the bottles?\"\n",
    "conversation_history = run_validation(query, conversation_history, project, verbosity=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RAG system is unable to answer this question because there is no relevant information in the retrieved context, nor has a similar question been answered in the Codex Project (see the contents of the Codex Project above).\n",
    "\n",
    "**Codex automatically recognizes this question could not be answered and logs it into the Project where it awaits an answer from a SME (notice that `escalated_to_sme` is True).**\n",
    "Navigate to your Codex Project in the [Web App](https://codex.cleanlab.ai/) where you (or a SME at your company) can enter the desired answer for this query.\n",
    "\n",
    "As soon as an answer is provided in Codex, our RAG system will be able to answer all similar questions going forward (as seen for the previous query).\n",
    "\n",
    "### Validating an incorrect response where the expert answer is in Codex\n",
    "\n",
    "Let's restart our conversation and validate another example from our RAG system.  For this example, initial RAG response is a hallucination as the volume of the water bottle is not in the `Retrieved context`. It is *incorrect*.  \n",
    "\n",
    "Cleanlab auto-flags this response as bad (escalated to SME), and searches for an expert answer in the Codex Project. Since one is available for a similar query in the Codex Project, this expert answer is returned to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: Can I ask a question about the Simple Water Bottle?\n",
      "\n",
      "Final Response: Sure! What would you like to know?\n",
      "\n",
      "User Query: How much water can it hold?\n",
      "\n",
      "Initial RAG response: The Simple Water Bottle can hold 16 oz of Water\n",
      "\n",
      "Final Response: 32oz\n",
      "\n",
      "Validation Results:\n",
      "    deterministic_guardrails_results: {}\n",
      "    escalated_to_sme: True\n",
      "    eval_scores: {'trustworthiness': {'score': 0.15236053164056915, 'triggered': True, 'triggered_escalation': True, 'triggered_guardrail': True, 'failed': True, 'log': None}, 'context_sufficiency': {'score': 0.0025709827483007225, 'triggered': True, 'triggered_escalation': False, 'triggered_guardrail': False, 'failed': True, 'log': None}, 'response_helpfulness': {'score': 0.9973749219279046, 'triggered': False, 'triggered_escalation': False, 'triggered_guardrail': False, 'failed': False, 'log': None}, 'query_ease': {'score': 0.2701081658621518, 'triggered': False, 'triggered_escalation': False, 'triggered_guardrail': False, 'failed': False, 'log': None}, 'response_groundedness': {'score': 0.0024890909097237614, 'triggered': False, 'triggered_escalation': False, 'triggered_guardrail': False, 'failed': False, 'log': None}}\n",
      "    expert_answer: 32oz\n",
      "    is_bad_response: True\n",
      "    should_guardrail: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conversation_history = []\n",
    "query = \"Can I ask a question about the Simple Water Bottle?\"\n",
    "conversation_history = run_validation(query, conversation_history, project, verbosity=0)\n",
    "query = \"How much water can it hold?\"\n",
    "conversation_history = run_validation(query, conversation_history, project, verbosity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating assistant responses with expert answers\n",
    "In a conversational setting, we want to update the `conversation_history` with the `final_response` provided by Codex instead of the initial RAG response to track only the highest quality conversation. \n",
    "\n",
    "Notice how the (incorrect) initial RAG response *\"The Simple Water Bottle can hold 16 oz of Water\"* is replaced with the expert answer from Codex of \"32oz\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Usage\n",
    "\n",
    "You can configure many aspects of the bad response detection like what score thresholds to use inside the Codex Web App.\n",
    "\n",
    "### Response Quality Thresholds\n",
    "\n",
    "Thresholds determine when a response needs intervention:\n",
    "- A metric (trustworthiness, pii_detection, etc.) can be defined as an Evaluation, Guardrail , or both.\n",
    "- Each metric has its own threshold (0-1) and a defined threshold direction (below/above).\n",
    "- If any Evaluation's score violates the threshold condition, the response is escalated to an SME and an expert answer is attempted to be returned.\n",
    "- If any Guardrail's score violates the threshold condition, `Project.validate()` returns with the `should_guardrail=True` flag and no other action is taken.\n",
    "\n",
    "- Example: With Evaluation metric *Hallucination (trustworthiness)* threshold set to \"below 0.85\"\n",
    "  - Score 0.80 -> Escalated to an SME\n",
    "  - Score 0.90 -> Passes validation\n",
    "\n",
    "Setting thresholds affects your validation strategy:\n",
    "- Higher thresholds (e.g. 0.9) = Stricter validation\n",
    "  - More responses marked as \"should_escalate\"\n",
    "  - More queries logged for SMEs to answer\n",
    "  - Better response quality but higher SME workload\n",
    "   \n",
    "- Lower thresholds (e.g. 0.7) = More lenient validation\n",
    "  - Fewer responses marked as \"should_escalate\"\n",
    "  - Fewer queries logged for SMEs to answer\n",
    "  - Lower SME workload, but may allow lower quality responses from your RAG app to be returned unremediated.\n",
    "\n",
    "### Configure Custom Evaluations and Guardrails\n",
    "\n",
    "You can configure these directly in the Codex Web UI.  For details, see the [Adding custom guardrails](/codex/tutorials/azure/Azure_Guardrails_CodexAsBackup/#adding-custom-guardrails-to-your-codex-project) section in our other tutorial.\n",
    "\n",
    "### Rewritten Queries\n",
    "\n",
    "If your Conversational RAG pipeline already includes a query rewriter, you can save compute by passing the rewritten query directly to the `Project.validate()` method. A `rewritten_query` should be a reformulation of the original `query` made self-contained with respect to multi-turn conversations to improve retrieval quality. \n",
    "\n",
    "Simply provide it as the `rewritten_query` argument like so:\n",
    "\n",
    "```python\n",
    "# After discussing France in previous conversation turns...\n",
    "# Current query lacks context from the conversation\n",
    "query = \"What is its capital?\"\n",
    "# Rewritten query adds necessary context for better retrieval\n",
    "rewritten_query = \"What is the capital of France?\"\n",
    "results = project.validate(\n",
    "  messages=messages,\n",
    "  response=response,\n",
    "  query=query,\n",
    "  context=context,\n",
    "  rewritten_query=rewritten_query\n",
    ")\n",
    "```\n",
    "\n",
    "### Logging Additional Information\n",
    "\n",
    "When `project.validate()` returns results indicating a response should be escalated to an SME, it logs the query into your Codex Project. By default, this log automatically includes the evaluation scores (like trustworthiness), the context and LLM response.\n",
    "\n",
    "You can include additional information that would be helpful for Subject Matter Experts (SMEs) when they review the logged queries in the Codex Project later.\n",
    "\n",
    "To add any extra information, simply pass in any key-value pairs into the `metadata` parameter in the `validate()` method. For example, you can add the location the Query came from like so:\n",
    "\n",
    "```python\n",
    "metadata = {\"location\": \"USA\"}\n",
    "results = project.validate(\n",
    "  messages=messages, \n",
    "  response=response, \n",
    "  query=query, \n",
    "  context=context,\n",
    "  metadata=metadata,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that Cleanlab is integrated with your *Multi-turn Chat App*, you and SMEs can [open the Codex Project and answer questions](/codex/web_tutorials/codex_as_sme/) logged there to continuously improve your AI.\n",
    "\n",
    "This tutorial demonstrated how to use Cleanlab to automatically detect and remediate bad responses in any RAG application. Cleanlab provides a robust way to evaluate response quality and automatically fetch expert answers when needed. For responses that don't meet quality thresholds, Codex automatically logs the queries for SME review.\n",
    "\n",
    "**Note:** Automatic detection and remediation of bad responses for a *Single-turn Q&A app* is covered in the [Detect and Remediate bad responses in Single-turn Q&A App Tutorial](/codex/tutorials/other_rag_frameworks/validator/)\n",
    "\n",
    "**Adding Cleanlab only improves your RAG app.** Once integrated, it automatically identifies problematic responses and either remediates them with expert answers or logs them for review. Using a [simple web interface](/codex/web_tutorials/codex_as_sme/), SMEs at your company can answer the highest priority questions in the Codex Project. As soon as an answer is entered in Codex, your RAG app will be able to properly handle all similar questions encountered in the future.\n",
    "\n",
    "Codex is **the fastest way for nontechnical SMEs to directly improve your RAG system**. As the Developer, you simply integrate Cleanlab once, and from then on, SMEs can continuously improve how your system handles common user queries without needing your help.\n",
    "\n",
    "Need help, more capabilities, or other deployment options?  \n",
    "Check the [FAQ](/codex/FAQ/) or email us at: support@cleanlab.ai"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}