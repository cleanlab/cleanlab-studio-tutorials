{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf38a9d7",
   "metadata": {},
   "source": [
    "# Detect and remediate bad responses from RAG applications with Tool Calls\n",
    "\n",
    "This notebok demonstrates how to call Cleanlab validation on any response generated from an LLM with tool calling capabilities. \n",
    "\n",
    "Cleanlab will automatically detect if your AI response is bad (e.g., untrustworthy, unhelpful, or unsafe).  The Codex API returns these real-time evaluation scores which you can use to guardrail your AI. \n",
    "\n",
    "!['Codex as a backup'](../assets/codexasbackup.png)\n",
    "\n",
    "**Note:** This tutorial uses OpenAI as an example AI Agent, however, any responses from any Agent can be used as long as they are translated into OpenAI format.\n",
    "\n",
    "**Note:** We recommend keeping your tool-calling AI connected to a separate Codex project from your Q&A / Chat AI.\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook shows how Cleanlab can detect and prevent wrong Tool Calls or bad AI responses from happening, and also serve expert answers in scenarios where your AI previously responded incorrectly or output a wrong Tool Call.\n",
    "\n",
    "**Note:** This tutorial is for *Multi-turn Chat Apps*. If you have a *Single-turn Q&A app*, a similar workflow is covered in the [Detect and Remediate bad responses in Single-turn Q&A Apps](/codex/tutorials/other_rag_frameworks/validator/) tutorial.\n",
    "\n",
    "## Setup\n",
    "\n",
    "This tutorial requires a Codex API key. Get one [here](https://codex.cleanlab.ai/account)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0533299",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade cleanlab-codex pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32b5186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your Codex API key\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<API key>\"  # Get your free API key from: https://platform.openai.com/account/api-keys\n",
    "os.environ[\"CODEX_API_KEY\"] = \"<API key>\" # Get your free API key from: https://codex.cleanlab.ai/account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afb0cac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from cleanlab_codex.project import Project\n",
    "from openai import OpenAI\n",
    "import time\n",
    "import uuid\n",
    "from openai.types.chat import ChatCompletion, ChatCompletionMessage\n",
    "import json\n",
    "from cleanlab_tlm.utils.chat import form_response_string_chat_completions_api\n",
    "import pandas as pd\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a249a3c",
   "metadata": {},
   "source": [
    "## Example RAG App: Morgage Lender\n",
    "\n",
    "Consider a customer support / banking RAG use-case where the Knowledge Base contains information on loans like the following:\n",
    "\n",
    "```bash\n",
    "**Knowledge Base Article: Application Review Process**\n",
    "\n",
    "- Once a customer submits their application, it enters the **Review Stage**.  \n",
    "- **Review Stage Timeline:** Typically 3\u20135 business days.  \n",
    "- **What Happens During Review:**  \n",
    "  - Verification of identity and personal details  \n",
    "  - Credit report evaluation  \n",
    "  - Fraud checks and risk assessment  \n",
    "```\n",
    "\n",
    "Here, the inner workings of the RAG app are not important for this tutorial. What is important is that the RAG app generates a *response* based on a set of provided *tools*, *user query*, a *context*, and a prior *conversation history*, which are all made available for evaluation.\n",
    "\n",
    "For simplicity, our context and tool responses are hardcoded below. You should replace these with the outputs of your RAG system, noting that Cleanlab can detect issues in these outputs in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34a1e841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "\n",
    "CONTEXT = \"Knowledge Base Article: Application Review Process\\nOnce a customer submits their application, it enters the Review Stage.\\nReview Stage Timeline: Typically 3\u20135 business days.\\nWhat Happens During Review:\\n- Verification of identity and personal details\\n- Credit report evaluation\\n- Fraud checks and risk assessment\"\n",
    "\n",
    "def get_application_status():\n",
    "    \"\"\"A tool that simulates fetching the application status for a customer.\n",
    "    **Note:** This tool returns a hardcoded *realistic* application status for demonstration purposes.\"\"\"\n",
    "    return {\n",
    "        \"status\": \"RATE_ACCEPTED\",\n",
    "        \"bank_accounts\": [\n",
    "            {\"is_verified\": True, \"added_via\": \"PLAID\"}\n",
    "        ],\n",
    "    }\n",
    "\n",
    "\n",
    "def get_payment_schedule():\n",
    "    \"\"\"A tool that simulates fetching a payment schedule for a customer.\n",
    "    **Note:** This tool returns a hardcoded *unrealistic* payment schedule for demonstration purposes.\"\"\"\n",
    "    return {\n",
    "        \"currency\": \"USD\",\n",
    "        \"payments\": [\n",
    "            {\n",
    "                \"due_date\": str(date.today() + timedelta(days=30)),\n",
    "                \"amount_due\": 350000000000000.00,\n",
    "                \"status\": \"UPCOMING\"\n",
    "            },\n",
    "            {\n",
    "                \"due_date\": str(date.today() + timedelta(days=60)),\n",
    "                \"amount_due\": 350000000000000.00,\n",
    "                \"status\": \"UPCOMING\"\n",
    "            }\n",
    "        ],\n",
    "        \"next_payment_due\": str(date.today() + timedelta(days=30))\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd63028",
   "metadata": {},
   "source": [
    "In practice, your RAG system should already have functions to process tool calls, retrieve context, generate responses, and build a messages object to prompt the LLM with. \n",
    "\n",
    "For this tutorial, we'll simulate these functions using the above fields as well as define a simple `fallback_response`, `system_prompt`, and `prompt_template`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6ed771-bec8-4126-acdb-75e05dae550b",
   "metadata": {},
   "source": [
    "**Optional: Toy RAG methods you should replace with existing methods from your RAG system**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c36e0790",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()  # Replace with your RAG system's client\n",
    "SYSTEM_PROMPT = \"You are a customer service agent. Be polite and concise in your responses.\"\n",
    "FALLBACK_RESPONSE = \"I'm sorry, but I need to direct you to our customer service team for assistance with this inquiry. Please reach out to example_lenders@money.com for help.\"\n",
    "PROMPT_TEMPLATE = \"\"\"Answer the following customer question.\n",
    "\n",
    "Customer Question: {question}\n",
    "\"\"\"\n",
    "CONVERSATION_HISTORY = []\n",
    "\n",
    "mock_tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_application_status\",\n",
    "            \"description\": \"Returns the current loan application status.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": []\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_payment_schedule\",\n",
    "            \"description\": \"Retrieves the upcoming payment schedule for the active loan application.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "#### RAG helper methods\n",
    "\n",
    "def rag_form_prompt(conversation_history, user_query=None, context=None):\n",
    "    \"\"\"Form a prompt for your LLM response-generation step (from the user query, retrieved context, conversation history, system instructions, etc). We represent the `prompt` in OpenAI's `messages` format, which matches the input to Cleanlab's `validate()` method.\n",
    "    \n",
    "    **Note:** In `messages`, it is recommended to inject retrieved context into the system prompt rather than each user message.\n",
    "    \"\"\"\n",
    "\n",
    "    system_message = f\"System message: {SYSTEM_PROMPT}\\n\\nContext: {context}\\n\\n\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        *conversation_history,  # Include previous messages\n",
    "    ]\n",
    "    if user_query:\n",
    "        user_message = PROMPT_TEMPLATE.format(context=context, question=user_query)\n",
    "        messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    return messages\n",
    "\n",
    "def rag_retreive_context(query):\n",
    "    \"\"\"Retrieve relevant context for the given query. In practice, this would involve querying a vector database or similar system.\"\"\"\n",
    "    # For this tutorial, we return the hardcoded context\n",
    "    return CONTEXT\n",
    "\n",
    "#### Tool calling helper methods\n",
    "mock_tool_registry = {\n",
    "    \"get_application_status\": get_application_status,\n",
    "    \"get_payment_schedule\": get_payment_schedule,\n",
    "}\n",
    "\n",
    "def mock_tool_handler(tool_name, arguments):\n",
    "    if tool_name in mock_tool_registry:\n",
    "        return json.dumps(mock_tool_registry[tool_name]())\n",
    "    return json.dumps({\"error\": \"Unknown tool\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917ae7b0-231b-41ca-b9c0-6db6bc8df560",
   "metadata": {},
   "source": [
    "**Optional: Cleanlab helper methods for validation and managing conversation history**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84d2583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_response_with_cleanlab(results, initial_response: ChatCompletion, FALLBACK_RESPONSE: str) -> ChatCompletion:\n",
    "    \"\"\"\n",
    "    Extracts the final response from the initial response and validation results using the following logic:\n",
    "    \n",
    "    - If the expert answer is provided and the query was escalated to an SME, return that.\n",
    "    - If a guardrail was triggered, return the fallback response.\n",
    "    - Otherwise, return the initial response from the RAG system.\n",
    "\n",
    "    Returns response in ChatCompletion as the initial response.\n",
    "    \"\"\"\n",
    "    \n",
    "    def make_cleanlab_response_into_minimal_chatcompletion(\n",
    "        content: str,\n",
    "    ) -> ChatCompletion:\n",
    "        \"\"\"\n",
    "        Create the smallest valid ChatCompletion object per schema with content.\n",
    "        \n",
    "        Args:\n",
    "            content: The text to set as the message content.\n",
    "\n",
    "        Returns:\n",
    "            ChatCompletion: Minimal valid ChatCompletion object.\n",
    "        \"\"\"\n",
    "        return ChatCompletion(\n",
    "            id=f\"chatcmpl-{uuid.uuid4().hex[:8]}\",\n",
    "            object=\"chat.completion\",\n",
    "            created=int(time.time()),\n",
    "            model=\"cleanlab\",\n",
    "            choices=[\n",
    "                {\n",
    "                    \"index\": 0,\n",
    "                    \"finish_reason\": \"stop\",\n",
    "                    \"message\": ChatCompletionMessage(\n",
    "                        role=\"assistant\",\n",
    "                        content=content,\n",
    "                    ),\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    if results.expert_answer and results.escalated_to_sme:\n",
    "        return make_cleanlab_response_into_minimal_chatcompletion(\n",
    "            results.expert_answer\n",
    "        )\n",
    "    elif results.should_guardrail:\n",
    "        return make_cleanlab_response_into_minimal_chatcompletion(\n",
    "            FALLBACK_RESPONSE\n",
    "        )\n",
    "    else:\n",
    "        return initial_response\n",
    "\n",
    "from openai.types.chat import ChatCompletionMessage\n",
    "\n",
    "def clean_conversation_history(conversation):\n",
    "    \"\"\"Removes bad tool calls in the current chat turn if final assistant has no tool calls.\"\"\"\n",
    "    \n",
    "    if len(conversation) == 0:\n",
    "        return conversation\n",
    "    \n",
    "    # Find start of current turn (last user message)\n",
    "    start_index = next(\n",
    "        i for i in range(len(conversation) - 1, -1, -1)\n",
    "        if (isinstance(conversation[i], ChatCompletionMessage) and conversation[i].role == \"user\") or (isinstance(conversation[i], dict) and conversation[i].get(\"role\") == \"user\")\n",
    "    )\n",
    "    chat_turn = conversation[start_index:]\n",
    "    final_assistant = chat_turn[-1]\n",
    "    \n",
    "    # Remove tool calls if final assistant has no tool_calls\n",
    "    if getattr(final_assistant, \"tool_calls\", None) is None:\n",
    "        skip_tool_ids = {tc.id for m in chat_turn if getattr(m, \"tool_calls\", None)\n",
    "                         for tc in getattr(m, \"tool_calls\", [])}\n",
    "        \n",
    "        # Replace slice in place\n",
    "        conversation[start_index:] = [\n",
    "            m for m in chat_turn if not (\n",
    "                (getattr(m, \"tool_calls\", None) and any(tc.id in skip_tool_ids for tc in m.tool_calls)) or\n",
    "                (isinstance(m, dict) and m.get(\"role\") == \"tool\" and m.get(\"tool_call_id\") in skip_tool_ids)\n",
    "            )\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0867ffcc",
   "metadata": {},
   "source": [
    "## Create Codex Project\n",
    "\n",
    "To later use Codex, we must first [create a Project](/codex/web_tutorials/create_project/).\n",
    "Here we assume no (question, answer) pairs have already been added to the Codex Project.\n",
    "\n",
    "User queries where Codex detected a bad response from your RAG app will be logged in this Project for SMEs to later answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a28cef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab_codex.client import Client\n",
    "\n",
    "codex_client = Client()\n",
    "\n",
    "# Create a project\n",
    "project = codex_client.create_project(\n",
    "    name=\"Morgage lending AI Chatbot (with tools)\",\n",
    "    description=\"Customer facing chatbot for a mortgage lending company.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55b7847",
   "metadata": {},
   "source": [
    "## Run detection and remediation\n",
    "\n",
    "Now that our Codex Project is configured, we can use the `Project.validate()` method to detect bad responses from our RAG application for each chat turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbeb54ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "# This is used to show consecutive messages from the same conversation in the Codex UI\n",
    "# but it is not used in the code logic.\n",
    "thread_id = str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e857e09e",
   "metadata": {},
   "source": [
    "Applying the `Project.validate()` method to a RAG system is straightfoward. Here we do this using live prompting with OpenAI in a mock RAG chat application. Let's define a helper method that does this logic below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b27a486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag_with_cleanlab(\n",
    "    user_query: str,\n",
    "    context: str,\n",
    "    conversation_history: list[ChatCompletionMessage],\n",
    "    thread_id: str = None,\n",
    ") -> ChatCompletion:\n",
    "    \"\"\"Validate a RAG system response using Cleanlab's `Project.validate()` method.\n",
    "    \n",
    "    Args:\n",
    "        user_query: The user's question.\n",
    "        context: The context retrieved for the query.\n",
    "        conversation_history: The history of the conversation as a list of ChatCompletionMessage objects.\n",
    "        thread_id: Optional thread ID for the conversation.\n",
    "\n",
    "    Returns:\n",
    "        List[ChatCompletion]: Conversation history after processing the user query and validation.\n",
    "    \"\"\"\n",
    "    conversation_history_turn = deepcopy(conversation_history)\n",
    "    messages = rag_form_prompt(conversation_history_turn, user_query, context)\n",
    "    conversation_history_turn.append({\"role\": \"user\", \"content\": user_query})\n",
    "\n",
    "    print(f'---User Question---\\n{user_query}')\n",
    "    while True:\n",
    "        initial_response = client.chat.completions.create(\n",
    "            model=\"gpt-4.1\",\n",
    "            messages=messages,\n",
    "            tools=mock_tools,\n",
    "        )\n",
    "\n",
    "        ### New code to add for Cleanlab API ###\n",
    "        print(f'---Original LLM Response---\\n{form_response_string_chat_completions_api(initial_response.choices[0].message)}')\n",
    "        validation_result = project.validate(\n",
    "            response=initial_response,\n",
    "            query=user_query,\n",
    "            context=context,\n",
    "            messages=messages,\n",
    "            tools=mock_tools,\n",
    "            metadata={\"thread_id\": thread_id},  # Add thread id to track conversation in Codex UI\n",
    "        )\n",
    "        print(f\"---Cleanlab Validation---\")\n",
    "        print(f\"Escalated to SME: {validation_result.escalated_to_sme}\")\n",
    "        print(f\"Should Guardrail: {validation_result.should_guardrail}\")\n",
    "        print(f\"Expert Answer Available: {bool(validation_result.expert_answer)}\")\n",
    "        \n",
    "        response = get_final_response_with_cleanlab(validation_result, initial_response, FALLBACK_RESPONSE)\n",
    "        print(f'---Final LLM Response (after Cleanlab validation)---\\n{form_response_string_chat_completions_api(response.choices[0].message)}')\n",
    "        ### End of new code to add for Cleanlab API ###\n",
    "        \n",
    "        conversation_history_turn.append(response.choices[0].message)\n",
    "\n",
    "        if not response.choices[0].message.tool_calls:\n",
    "            if response != initial_response:  # If Cleanlab validation stepped in to change the response, remove bad tool calls from history\n",
    "                clean_conversation_history(conversation_history_turn)\n",
    "            break\n",
    "        else:\n",
    "            tools_for_print = []\n",
    "            for tool_call in response.choices[0].message.tool_calls:\n",
    "                args = json.loads(tool_call.function.arguments)\n",
    "                tool_response = mock_tool_handler(\n",
    "                    tool_call.function.name,\n",
    "                    tool_call.function.arguments\n",
    "                )\n",
    "                tool_dict = {\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"content\": str(tool_response),\n",
    "                }\n",
    "                conversation_history_turn.append(tool_dict)\n",
    "                tools_for_print.append(tool_dict)\n",
    "            # Update the messages with the new conversation history\n",
    "            print(f'---Tool Responses---\\n{tools_for_print}')\n",
    "            messages = rag_form_prompt(conversation_history_turn, None, None)\n",
    "        print('-'*40)\n",
    "    return conversation_history_turn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9c305a",
   "metadata": {},
   "source": [
    "### Chat Turn 1: \"What's the status of my application?\"\n",
    "\n",
    "This turn requires a single tool call to answer user query\n",
    "\n",
    "**Note:** The `validation_result` object below returned by `Project.validate()` contains all sorts of other useful information. See more details about it [here](https://help.cleanlab.ai/codex/tutorials/other_rag_frameworks/validator_conversational/#evaluation-metrics). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caad83c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---User Question---\n",
      "What's the status of my application?\n",
      "---Original LLM Response---\n",
      "<tool_call>\n",
      "{\n",
      "  \"name\": \"get_application_status\",\n",
      "  \"arguments\": {}\n",
      "}\n",
      "</tool_call>\n",
      "---Cleanlab Validation---\n",
      "Escalated to SME: False\n",
      "Should Guardrail: False\n",
      "Expert Answer Available: False\n",
      "---Final LLM Response (after Cleanlab validation)---\n",
      "<tool_call>\n",
      "{\n",
      "  \"name\": \"get_application_status\",\n",
      "  \"arguments\": {}\n",
      "}\n",
      "</tool_call>\n",
      "---Tool Responses---\n",
      "[{'role': 'tool', 'tool_call_id': 'call_hQeUNGM6CiFX7J7tlscWeUGx', 'content': '{\"status\": \"RATE_ACCEPTED\", \"bank_accounts\": [{\"is_verified\": true, \"added_via\": \"PLAID\"}]}'}]\n",
      "----------------------------------------\n",
      "---Original LLM Response---\n",
      "Your application status is \"Rate Accepted,\" and your bank account has been successfully verified. If you need further details or next steps, please let me know!\n",
      "---Cleanlab Validation---\n",
      "Escalated to SME: False\n",
      "Should Guardrail: False\n",
      "Expert Answer Available: False\n",
      "---Final LLM Response (after Cleanlab validation)---\n",
      "Your application status is \"Rate Accepted,\" and your bank account has been successfully verified. If you need further details or next steps, please let me know!\n"
     ]
    }
   ],
   "source": [
    "user_query1 = \"What's the status of my application?\"\n",
    "context1 = rag_retreive_context(user_query1)\n",
    "CONVERSATION_HISTORY = run_rag_with_cleanlab(\n",
    "    user_query1,\n",
    "    context1,\n",
    "    CONVERSATION_HISTORY,\n",
    "    thread_id=thread_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d311ab7f",
   "metadata": {},
   "source": [
    "### Chat Turn 2: \"what is my payment schedule?\"\n",
    "\n",
    "This turn also requires a single tool call to answer user query, however, the `get_payment_schedule()` tool is intentionally defined to have a *inaccurate or unrealistic* output.\n",
    "\n",
    "Cleanlab's validation software steps in and prevents such an output from being returned to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd6bf2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---User Question---\n",
      "what is my payment schedule?\n",
      "---Original LLM Response---\n",
      "<tool_call>\n",
      "{\n",
      "  \"name\": \"get_payment_schedule\",\n",
      "  \"arguments\": {}\n",
      "}\n",
      "</tool_call>\n",
      "---Cleanlab Validation---\n",
      "Escalated to SME: False\n",
      "Should Guardrail: False\n",
      "Expert Answer Available: False\n",
      "---Final LLM Response (after Cleanlab validation)---\n",
      "<tool_call>\n",
      "{\n",
      "  \"name\": \"get_payment_schedule\",\n",
      "  \"arguments\": {}\n",
      "}\n",
      "</tool_call>\n",
      "---Tool Responses---\n",
      "[{'role': 'tool', 'tool_call_id': 'call_cCb4lfSuBgj2smHfTcGM6AbZ', 'content': '{\"currency\": \"USD\", \"payments\": [{\"due_date\": \"2025-09-13\", \"amount_due\": 350000000000000.0, \"status\": \"UPCOMING\"}, {\"due_date\": \"2025-10-13\", \"amount_due\": 350000000000000.0, \"status\": \"UPCOMING\"}], \"next_payment_due\": \"2025-09-13\"}'}]\n",
      "----------------------------------------\n",
      "---Original LLM Response---\n",
      "Your upcoming payments are as follows:\n",
      "\n",
      "- Next payment due: September 13, 2025, amount: $350,000,000,000,000.00\n",
      "- Following payment due: October 13, 2025, amount: $350,000,000,000,000.00\n",
      "\n",
      "If you need more details or have any questions, please let me know!\n",
      "---Cleanlab Validation---\n",
      "Escalated to SME: True\n",
      "Should Guardrail: True\n",
      "Expert Answer Available: False\n",
      "---Final LLM Response (after Cleanlab validation)---\n",
      "I'm sorry, but I need to direct you to our customer service team for assistance with this inquiry. Please reach out to example_lenders@money.com for help.\n"
     ]
    }
   ],
   "source": [
    "user_query2 = \"what is my payment schedule?\"\n",
    "context2 = rag_retreive_context(user_query2)\n",
    "CONVERSATION_HISTORY = run_rag_with_cleanlab(\n",
    "    user_query2,\n",
    "    context2,\n",
    "    CONVERSATION_HISTORY,\n",
    "    thread_id=thread_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04df37de",
   "metadata": {},
   "source": [
    "### Chat Turn 3: \"how long does it take to review an application?\"\n",
    "\n",
    "This turn does not require any tool calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "528130b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---User Question---\n",
      "how long does it take to review an application?\n",
      "---Original LLM Response---\n",
      "The application review process typically takes 3\u20135 business days after you submit your application.\n",
      "---Cleanlab Validation---\n",
      "Escalated to SME: False\n",
      "Should Guardrail: False\n",
      "Expert Answer Available: False\n",
      "---Final LLM Response (after Cleanlab validation)---\n",
      "The application review process typically takes 3\u20135 business days after you submit your application.\n"
     ]
    }
   ],
   "source": [
    "user_query3 = \"how long does it take to review an application?\"\n",
    "context3 = rag_retreive_context(user_query3)\n",
    "CONVERSATION_HISTORY = run_rag_with_cleanlab(\n",
    "    user_query3,\n",
    "    context3,\n",
    "    CONVERSATION_HISTORY,\n",
    "    thread_id=thread_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1367231b",
   "metadata": {},
   "source": [
    "#### View entire conversation history\n",
    "\n",
    "Notice how the untrustworthy Initial LLM response to the user query \"what is my payment schedule?\" is guardrailed by Cleanlab and the fallback response is safely returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "642b8603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: What's the status of my application?\n",
      "ASSISTANT: <tool_call>\n",
      "{\n",
      "  \"name\": \"get_application_status\",\n",
      "  \"arguments\": {}\n",
      "}\n",
      "</tool_call>\n",
      "TOOL: {\"status\": \"RATE_ACCEPTED\", \"bank_accounts\": [{\"is_verified\": true, \"added_via\": \"PLAID\"}]}\n",
      "ASSISTANT: Your application status is \"Rate Accepted,\" and your bank account has been successfully verified. If you need further details or next steps, please let me know!\n",
      "USER: what is my payment schedule?\n",
      "ASSISTANT: I'm sorry, but I need to direct you to our customer service team for assistance with this inquiry. Please reach out to example_lenders@money.com for help.\n",
      "USER: how long does it take to review an application?\n",
      "ASSISTANT: The application review process typically takes 3\u20135 business days after you submit your application.\n"
     ]
    }
   ],
   "source": [
    "for c in CONVERSATION_HISTORY:\n",
    "    role = c[\"role\"] if isinstance(c, dict) and \"role\" in c else c.role\n",
    "    print(f\"{role.upper()}: {form_response_string_chat_completions_api(c)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a8b278",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that Cleanlab is integrated with your *Tool Calling AI App*, you and SMEs can [open the Codex Project and answer questions](/codex/web_tutorials/codex_as_sme/) logged there to continuously improve your AI.\n",
    "\n",
    "This tutorial demonstrated the basic of how to use Cleanlab to automatically detect and remediate bad responses in any RAG application. **Advanced Useage** (such as setting thresholds, adding evals and logging information) is covered in the [Detect and remediate bad responses from conversational RAG applications](/codex/tutorials/other_rag_frameworks/validator_conversational/) tutorial.\n",
    "\n",
    "\n",
    "Cleanlab provides a robust way to evaluate response quality and automatically fetch expert answers when needed. For responses that don't meet quality thresholds, Codex automatically logs the queries for SME review.\n",
    "\n",
    "**Adding Cleanlab only improves your RAG app.** Once integrated, it automatically identifies problematic responses and either remediates them with expert answers or logs them for review. Using a [simple web interface](/codex/web_tutorials/codex_as_sme/), SMEs at your company can answer the highest priority questions in the Codex Project. As soon as an answer is entered in Codex, your RAG app will be able to properly handle all similar questions encountered in the future.\n",
    "\n",
    "Codex is **the fastest way for nontechnical SMEs to directly improve your RAG system**. As the Developer, you simply integrate Cleanlab once, and from then on, SMEs can continuously improve how your system handles common user queries without needing your help.\n",
    "\n",
    "Need help, more capabilities, or other deployment options?  \n",
    "Check the [FAQ](/codex/FAQ/) or email us at: support@cleanlab.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c01c09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}