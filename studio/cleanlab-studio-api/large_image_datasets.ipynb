{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Issues in Large-Scale Image Datasets\n",
    "\n",
    "<head>\n",
    "  <meta name=\"title\" content=\"Automated Data Quality for Large-Scale Image Datasets\"/>\n",
    "  <meta property=\"og:title\" content=\"Automated Data Quality for Large-Scale Image Datasets\"/>\n",
    "  <meta name=\"twitter:title\" content=\"Automated Data Quality for Large-Scale Image Datasets\" />\n",
    "  <meta name=\"image\" content=\"/img/imagenetissues.png\" />\n",
    "  <meta property=\"og:image\" content=\"/img/imagenetissues.png\" />\n",
    "  <meta name=\"description\" content=\"How to quickly curate 1M+ image datasets (like ImageNet) via Data-Centric AI.\"  />\n",
    "  <meta property=\"og:description\" content=\"How to quickly curate 1M+ image datasets (like ImageNet) via Data-Centric AI.\" />\n",
    "  <meta name=\"twitter:description\" content=\"How to quickly curate 1M+ image datasets (like ImageNet) via Data-Centric AI.\" />\n",
    "</head>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we'll leverage [Cleanlab Studio](https://app.cleanlab.ai/) to automate data quality improvements on a large scale image dataset. This tutorial uses the ImageNet dataset as an example, but the same methods can easily be applied to your own large scale datasets! For an in depth analysis of Cleanlab Studio's results on ImageNet, see our accompanying [blog post](https://cleanlab.ai/blog/automated-data-quality-at-scale/).\n",
    "\n",
    "**Overview of what we'll do in this tutorial:**\n",
    "- Prep a large image dataset to be analyzed with Cleanlab Studio\n",
    "- Create a Project that automatically runs various data quality checks\n",
    "- Review the results of these quality checks and accordingly make corrections to your dataset\n",
    "- Produce a cleaned version of the dataset\n",
    "\n",
    "![Issues detected in the Imagenet dataset](./assets/large-image-tutorial/imagenet-issues.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and import required dependencies\n",
    "You can use `pip` to install all packages required for this tutorial as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install boto3 pandas requests tqdm cleanlab-studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab_studio import Studio\n",
    "from tqdm import tqdm\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81625030-94d1-4daa-898c-f1a387fcab93",
   "metadata": {},
   "source": [
    "**Optional: Initialize helper methods to render url column of DataFrame as images**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "def url_to_img_html(url: str) -> str:\n",
    "    return f'<img src=\"{url}\" width=\"100\" alt=\"\" />'\n",
    "\n",
    "def display(df: pd.DataFrame) -> None:\n",
    "    return HTML(df.to_html(escape=False, formatters=dict(url=url_to_img_html)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep and Upload Dataset\n",
    "Large image datasets are often stored in data lakes like AWS S3 or Google Cloud Storage Buckets. Using Cleanlab Studio's _externally-hosted media_ format, you can directly analyze images stored in your data lake without having to manually download and upload them to Cleanlab Studio. In this tutorial, we'll show you how to take images that are hosted in a **public** S3 bucket and format a dataset that you can upload to Cleanlab Studio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep Data\n",
    "\n",
    "We want to create a file that looks like this:\n",
    "\n",
    "```csv\n",
    "id,url,class_id,class_name\n",
    "0,https://cleanlab-public.s3.amazonaws.com/StudioDemoDatasets/imagenet-1k/n01440764/n01440764_6130.JPEG,n01440764,tench\n",
    "1,https://cleanlab-public.s3.amazonaws.com/StudioDemoDatasets/imagenet-1k/n01440764/n01440764_18.JPEG,n01440764,tench\n",
    "2,https://cleanlab-public.s3.amazonaws.com/StudioDemoDatasets/imagenet-1k/n01440764/n01440764_21955.JPEG,n01440764,tench\n",
    "3,https://cleanlab-public.s3.amazonaws.com/StudioDemoDatasets/imagenet-1k/n01440764/n01440764_10698.JPEG,n01440764,tench\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll replace the `url` column with public URLs for your own images. If you're using your own dataset (rather than ImageNet), the `class_id` column is unnecessary and you can replace the `class_name` column with your own label column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Optional: Download raw ImageNet dataset and upload images to S3 <b>(click to expand)</b></summary>\n",
    "<details><summary>Step 1: Download and unzip the ImageNet dataset</summary>\n",
    "\n",
    "You can download the ImageNet dataset as a torrent from [academic torrents](https://academictorrents.com/details/943977d8c96892d24237638335e481f3ccd54cfb). To do this, you'll need to have a Bittorrent client. We recommend `aria2` which can be installed in a terminal as follows:\n",
    "\n",
    "Linux: `sudo apt update && sudo apt install -y aria2`\n",
    "\n",
    "Mac: `brew install aria2`\n",
    "\n",
    "Once you have `aria2` installed, you can download ImageNet using:\n",
    "```\n",
    "aria2c https://academictorrents.com/download/c5af268ec55cf2d3b439e7311ad43101ba8322eb.torrent\n",
    "```\n",
    "\n",
    "(Warning: this requires around 166GB of disk space)\n",
    "\n",
    "This will download the dataset as a tar.gz file which you can extract in a terminal using:\n",
    "```\n",
    "tar -xvf [path to downloaded file]\n",
    "```\n",
    "\n",
    "(Warning: this requires an additional 173GB of disk space)\n",
    "\n",
    "You should now have a folder with the following structure:\n",
    "\n",
    "```bash\n",
    "|-- ILSVRC\n",
    "|   |-- Annotations\n",
    "|   |-- Data\n",
    "|   |   |-- CLS-LOC\n",
    "|   |   |   |-- train\n",
    "|   |   |   |-- test\n",
    "|   |   |   |-- val\n",
    "|   |-- ImageSets\n",
    "```\n",
    "\n",
    "For the purposes of this demo, we'll only be working with the ImageNet training set which can be found in the `ILSVRC/Annotations/Data/CLS-LOC/train` directory\n",
    "</details>\n",
    "\n",
    "<details><summary>Step 2: Upload images to S3</summary>\n",
    "\n",
    "To upload your dataset images to S3, we recommend using `s5cmd` (click [here](https://github.com/peak/s5cmd#overview) for more info and installation instructions). Use the following command to upload your images:\n",
    "```\n",
    "s5cmd cp -f ILSVRC/Annotations/Data/CLS-LOC/train/* s3://[your bucket name]/\n",
    "```\n",
    "Make sure the images uploaded to S3 are publicly accessible (you can configure your bucket permissions through the AWS console).\n",
    "</details>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For this tutorial we assume you have images in a public S3 bucket organized in the following structure:\n",
    "```bash\n",
    "|-- <bucket_name>\n",
    "|   |-- <imagenet_class_id_0>\n",
    "|   |   |-- <class_id_0_img_0>\n",
    "|   |   |-- <class_id_0_img_1>\n",
    "...\n",
    "|   |-- <imagenet_class_id_n>\n",
    "...\n",
    "```\n",
    "\n",
    "We'll be creating a dataset with \"id\" (row index), \"url\" (S3 object URLs for your images), \"class_id\" (ImageNet class IDs), and \"class_name\" (human readable class names) columns using the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, setup your S3 client and some helper functions to iterate through your images in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator\n",
    "\n",
    "# initialize boto3 S3 client\n",
    "# you may need to provide credentials \n",
    "# (as described here https://boto3.amazonaws.com/v1/documentation/api/latest/studio/credentials.html)\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "s3_bucket = \"<insert your S3 bucket name>\"\n",
    "\n",
    "# optional prefix within your S3 bucket where your dataset images are located\n",
    "# set to empty string if your directory structure exactly matches the example \n",
    "# structure in the section above\n",
    "s3_prefix = \"<insert your prefix>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf48415-36b0-4226-ac5c-1632d191b85d",
   "metadata": {},
   "source": [
    "**Optional: Initialize helper methods to iterate through class directories**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def list_class_directories(bucket: str, prefix: str) -> Generator[str, None, None]:\n",
    "    paginator = s3_client.get_paginator(\"list_objects_v2\")\n",
    "    result = paginator.paginate(Bucket=bucket, Prefix=prefix, Delimiter=\"/\")\n",
    "    for page in result:\n",
    "        if \"CommonPrefixes\" in page:\n",
    "            for class_dir in page[\"CommonPrefixes\"]:\n",
    "                yield pathlib.Path(class_dir[\"Prefix\"]).name\n",
    "\n",
    "def list_image_filenames(bucket: str, class_prefix: str) -> Generator[str, None, None]:\n",
    "    paginator = s3_client.get_paginator(\"list_objects_v2\")\n",
    "    result = paginator.paginate(Bucket=bucket, Prefix=class_prefix)\n",
    "    for page in result:\n",
    "        if \"Contents\" in page:\n",
    "            for obj in page[\"Contents\"]:\n",
    "                yield pathlib.Path(obj[\"Key\"]).name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load the mapping we've created from ImageNet class IDs to human readable labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_id_to_label = requests.get(\n",
    "    \"https://cleanlab-public.s3.amazonaws.com/StudioDemoDatasets/imagenet-1k/class_id_to_label.json\"\n",
    ").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, iterate through your images to build your DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [08:39,  1.93it/s]\n"
     ]
    }
   ],
   "source": [
    "class_dfs = []\n",
    "for class_id in tqdm(list_class_directories(s3_bucket, s3_prefix)):\n",
    "    class_name = class_id_to_label[class_id]\n",
    "    class_df = pd.DataFrame()\n",
    "    s3_class_prefix = str(pathlib.Path(s3_prefix).joinpath(class_id))\n",
    "    class_df[\"url\"] = [\n",
    "        f\"https://{s3_bucket}.s3.amazonaws.com/{s3_class_prefix}/{img_filename}\" \n",
    "        for img_filename in list_image_filenames(s3_bucket, s3_class_prefix)\n",
    "    ]\n",
    "    class_df[\"class_id\"] = class_id\n",
    "    class_df[\"class_name\"] = class_name\n",
    "    class_dfs.append(class_df)\n",
    "\n",
    "df = pd.concat(class_dfs, ignore_index=True)\n",
    "\n",
    "# Optional: to get your dataset in the same order as ours\n",
    "df = df.sort_values(by=\"class_id\", ignore_index=True)\n",
    "\n",
    "df.index.name = \"id\"\n",
    "df = df.reset_index()\n",
    "\n",
    "# Optional: save dataframe to CSV file (not necessary if uploading via our Python API)\n",
    "df.set_index(\"id\")\n",
    "df.to_csv(\"imagenet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>class_id</th>\n",
       "      <th>class_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1138521</th>\n",
       "      <td>1138521</td>\n",
       "      <td><img src=\"https://cleanlab-public.s3.amazonaws.com/StudioDemoDatasets/imagenet-1k/n04536866/n04536866_8781.JPEG\" width=\"100\" alt=\"\" /></td>\n",
       "      <td>n04536866</td>\n",
       "      <td>violin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171438</th>\n",
       "      <td>171438</td>\n",
       "      <td><img src=\"https://cleanlab-public.s3.amazonaws.com/StudioDemoDatasets/imagenet-1k/n02009912/n02009912_9169.JPEG\" width=\"100\" alt=\"\" /></td>\n",
       "      <td>n02009912</td>\n",
       "      <td>great egret</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067267</th>\n",
       "      <td>1067267</td>\n",
       "      <td><img src=\"https://cleanlab-public.s3.amazonaws.com/StudioDemoDatasets/imagenet-1k/n04347754/n04347754_85281.JPEG\" width=\"100\" alt=\"\" /></td>\n",
       "      <td>n04347754</td>\n",
       "      <td>submarine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951605</th>\n",
       "      <td>951605</td>\n",
       "      <td><img src=\"https://cleanlab-public.s3.amazonaws.com/StudioDemoDatasets/imagenet-1k/n04005630/n04005630_61649.JPEG\" width=\"100\" alt=\"\" /></td>\n",
       "      <td>n04005630</td>\n",
       "      <td>prison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386329</th>\n",
       "      <td>386329</td>\n",
       "      <td><img src=\"https://cleanlab-public.s3.amazonaws.com/StudioDemoDatasets/imagenet-1k/n02167151/n02167151_6199.JPEG\" width=\"100\" alt=\"\" /></td>\n",
       "      <td>n02167151</td>\n",
       "      <td>ground beetle</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have a dataset that's ready to upload to Cleanlab Studio!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>This tutorial will focus on using the Python API, but you can also use our <a href=\"https://app.cleanlab.ai\">Web UI</a> for a no-code option <b>(click to expand)</b></summary>\n",
    "\n",
    "If you would like to upload your data without writing code, simply go to [https://app.cleanlab.ai/upload](https://app.cleanlab.ai/upload) and follow these steps:\n",
    "1. Click \"Upload from your computer\"\n",
    "2. Drag & Drop or select the CSV file you saved from the DataFrame you created in the [Prep Data](#prep-data) section\n",
    "3. Click \"Upload\" and wait for the file to upload\n",
    "4. Click \"Next\"\n",
    "5. Select \"id\" as the ID column for your dataset. Leave everything else on the schema editing page as default\n",
    "6. Click \"Confirm\"\n",
    "7. Wait for data ingestion to complete\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can upload your dataset to Cleanlab Studio using our Python API with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading dataset...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|\n",
      "Ingesting Dataset...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|\n"
     ]
    }
   ],
   "source": [
    "# you can find your API key by going to app.cleanlab.ai/upload, \n",
    "# clicking \"Upload via Python API\", and copying the API key there\n",
    "API_KEY = \"<insert your API key>\"\n",
    "\n",
    "# initialize studio object\n",
    "studio = Studio(API_KEY)\n",
    "\n",
    "# upload dataset\n",
    "dataset_id = studio.upload_dataset(df, dataset_name=\"ImageNet\", schema_overrides=[{\"name\": \"url\", \"column_type\": \"image_external\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Project\n",
    "\n",
    "Creating a project in Cleanlab Studio will automatically run our data quality checks on your dataset. Since ImageNet is a large dataset (with 1.2 million images), it will take a while (around 20 hours) for your project to be ready.\n",
    "\n",
    "<details><summary><a href=\"https://app.cleanlab.ai\">Web UI</a> instructions <b>(click to expand)</b></summary>\n",
    "\n",
    "To create a project for your dataset, navigate to the [Cleanlab Studio Dashboard](https://app.cleanlab.ai) and follow these steps:\n",
    "\n",
    "1. Find your dataset in the datasets grid and click the \"Create Project\" button.\n",
    "2. Enter a name for your project (you can change this later).\n",
    "3. Make sure \"Image Classification\" and \"Multi-Class\" are selected for the Machine Learning Task and Type of Classification options.\n",
    "4. Select \"class_name\" as your label column.\n",
    "5. Select \"Use a provided model\" and \"Fast\" for the model type.\n",
    "6. Click \"Clean my Data\".\n",
    "\n",
    "</details>\n",
    "\n",
    "Using your `dataset_id` from step 2, you can create a project with one line of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = studio.create_project(\n",
    "    dataset_id, \n",
    "    project_name=\"ImageNet\", \n",
    "    modality=\"image\", \n",
    "    model_type=\"fast\", \n",
    "    label_column=\"class_name\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you create your project, you'll need to wait for it to complete before proceeding with this tutorial. You can check your project's status in the [Cleanlab Studio Dashboard](https://app.cleanlab.ai) or using the Python API. You'll also receive an email when the project is ready.\n",
    "\n",
    "<details><summary><a href=\"https://app.cleanlab.ai\">Web UI</a> instructions <b>(click to expand)</b></summary>\n",
    "\n",
    "Navigate to the [Cleanlab Studio Dashboard](https://app.cleanlab.ai). Find your project in the Projects grid and view it's status.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can programmatically wait until the project has completed in the Python API using the code below. This may take a *long time* for big datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleanset Progress: \\ Step 5/5, Ready for review!\n"
     ]
    }
   ],
   "source": [
    "cleanset_id = studio.get_latest_cleanset_id(project_id)\n",
    "print(f\"cleanset_id: {cleanset_id}\")\n",
    "studio.wait_until_cleanset_ready(cleanset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will continuously poll for your project status and wait until the project is ready. You can optionally provide a `timeout` parameter after which the function will stop waiting even if the project is not ready. If your Jupyter notebook has timed out during this process, then you can resume work by re-running this cell (which should return instantly if the project has completed training; do not create a new Project)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Your Project and Make Corrections\n",
    "\n",
    "### View Your Project\n",
    "Once your project's ready, you can view the results and make corrections to your dataset! This step is best done through the Web UI. To find your project results, navigate to the [Cleanlab Studio Dashboard](https://app.cleanlab.ai) and click on your project name. You should see a page that looks like this:\n",
    "\n",
    "![Imagenet Project](./assets/large-image-tutorial/imagenet_project_page.png)\n",
    "\n",
    "Here you can browse through and view analytics on the [data issues](/studio/concepts/cleanlab_columns) Cleanlab Studio found in your project, make corrections based on Cleanlab Studio's suggestions, and eventually export your improved dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Corrections\n",
    "\n",
    "You can make corrections to your dataset by using the resolver panel on your project page (click any row in the project grid for the resolver to appear). If you would like to make correct multiple datapoints at once, you can use the \"Clean Top K\" button at the bottom of the project page or select multiple rows in the project grid and apply an action to those rows.\n",
    "\n",
    "For the purposes of this tutorial, try correcting a few labels and excluding some rows from your dataset. See the video below for an example:\n",
    "\n",
    "<Video\n",
    "  width=\"1792\"\n",
    "  height=\"1010\"\n",
    "  src=\"./assets/large-image-tutorial/imagenet_corrections.mp4\"\n",
    "  autoPlay={false}\n",
    "  loop={false}\n",
    "  muted={true}\n",
    "/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a deeper analysis of Cleanlab Studio's results on ImageNet, read our blog post [here](https://cleanlab.ai/blog/automated-data-quality-at-scale/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Your Cleaned Data\n",
    "\n",
    "Once you're happy with the corrections you've made to your dataset, you can export your cleaned data through the Web UI (100MB export limit) or Python API. This tutorial will focus on exporting through the Python API since your ImageNet project will be too big to export completely through the Web UI, but we've also included instructions for exporting a subset of your data through the Web UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><a href=\"https://app.cleanlab.ai\">Web UI</a> instructions <b>(click to expand)</b></summary>\n",
    "\n",
    "To export a subset of your data through the Web UI, follow these steps:\n",
    "1. Navigate to your project page\n",
    "2. Filter your project, so that a subset of the rows are displayed (i.e. all rows you corrected or all rows with label issues). You should be able to export around 450k rows without exceeding the 100MB Web UI export limit.\n",
    "3. Click the \"Export Cleanset\" button at the bottom of the page.\n",
    "4. Select the \"Custom\" export configuration.\n",
    "5. Click \"Export\".\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To export through the Python API, use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `df` should be the same DataFrame you created to upload your data to Cleanlab Studio\n",
    "# if you did not upload using the Python API, \n",
    "# uncomment out the following code before running `studio.apply_corrections()`\n",
    "# studio = Studio(\"<INSERT YOUR API KEY>\")\n",
    "# cleanset_id = \"<INSERT YOUR CLEANSET ID>\"\n",
    "\n",
    "cleaned_df = studio.apply_corrections(cleanset_id, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that labels you fixed are reflected in the resulting DataFrame. In our project, we corrected the label for row 1026223 from \"slot machine\" to \"vending machine\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>class_id</th>\n",
       "      <th>class_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1026223</th>\n",
       "      <td>1026223</td>\n",
       "      <td><img src=\"https://cleanlab-public.s3.amazonaws.com/StudioTestAssets/Datasets/imagenet-1k/n04243546/n04243546_21009.JPEG\" width=\"100\" alt=\"\" /></td>\n",
       "      <td>n04243546</td>\n",
       "      <td>slot machine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original dataset\n",
    "display(df[df[\"id\"] == 1026223])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>class_id</th>\n",
       "      <th>class_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1026223</th>\n",
       "      <td>1026223</td>\n",
       "      <td><img src=\"https://cleanlab-public.s3.amazonaws.com/StudioTestAssets/Datasets/imagenet-1k/n04243546/n04243546_21009.JPEG\" width=\"100\" alt=\"\" /></td>\n",
       "      <td>n04243546</td>\n",
       "      <td>vending machine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cleaned dataset\n",
    "display(cleaned_df[cleaned_df[\"id\"] == 1026223])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also corrected labels for a few other rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>class_name</th>\n",
       "      <th>class_name_corrected</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>825275</th>\n",
       "      <td><img src=\"https://cleanlab-public.s3.amazonaws.com/StudioTestAssets/Datasets/imagenet-1k/n03724870/n03724870_13435.JPEG\" width=\"100\" alt=\"\" /></td>\n",
       "      <td>mask</td>\n",
       "      <td>ski mask</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722195</th>\n",
       "      <td><img src=\"https://cleanlab-public.s3.amazonaws.com/StudioTestAssets/Datasets/imagenet-1k/n03388043/n03388043_10892.JPEG\" width=\"100\" alt=\"\" /></td>\n",
       "      <td>fountain</td>\n",
       "      <td>geyser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825015</th>\n",
       "      <td><img src=\"https://cleanlab-public.s3.amazonaws.com/StudioTestAssets/Datasets/imagenet-1k/n03724870/n03724870_3386.JPEG\" width=\"100\" alt=\"\" /></td>\n",
       "      <td>mask</td>\n",
       "      <td>ski mask</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940053</th>\n",
       "      <td><img src=\"https://cleanlab-public.s3.amazonaws.com/StudioTestAssets/Datasets/imagenet-1k/n03976657/n03976657_22445.JPEG\" width=\"100\" alt=\"\" /></td>\n",
       "      <td>pole</td>\n",
       "      <td>maypole</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original = df.set_index(\"id\").loc[\n",
    "    [825275, 722195, 825015, 940053], \n",
    "    [\"url\", \"class_name\"]\n",
    "]\n",
    "corrected = cleaned_df.set_index(\"id\").loc[\n",
    "    [825275, 722195, 825015, 940053], \n",
    "    [\"url\", \"class_name\"]\n",
    "]\n",
    "\n",
    "display(original.merge(corrected, on=[\"id\", \"url\"], suffixes=(\"\", \"_corrected\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should also see that any rows you excluded are dropped in the resulting DataFrame. In our project, we excluded row 529403 since it was a duplicate of row 1026223. We should that there's no longer a row with id 529403 in our cleaned dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "529403 in cleaned_df[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! You now have an improved version of the famous ImageNet dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Bonus!) Improve Results by Rerunning Cleanlab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional awesome thing about Cleanlab, is that you can rerun Cleanlab on your improved dataset to get even better results! To try this, click the \"Improve Results\" button at the bottom of your project page.\n",
    "\n",
    "![Improve Results](./assets/large-image-tutorial/improve_results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}