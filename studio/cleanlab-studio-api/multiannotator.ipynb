{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c7436b8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Estimate Consensus and Annotator Quality for Data\u00a0Labeled by Multiple Annotators\n",
    "\n",
    "<head>\n",
    "  <meta name=\"title\" content=\"Estimate Consensus and Annotator Quality for Data\u00a0Labeled by Multiple Annotators\"/>\n",
    "  <meta property=\"og:title\" content=\"Estimate Consensus and Annotator Quality for Data\u00a0Labeled by Multiple Annotators\"/>\n",
    "  <meta name=\"twitter:title\" content=\"Estimate Consensus and Annotator Quality for Data\u00a0Labeled by Multiple Annotators\" />\n",
    "  <meta name=\"image\" content=\"/img/multiannotator.png\" />\n",
    "  <meta property=\"og:image\" content=\"/img/multiannotator.png\" />\n",
    "</head>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b432513",
   "metadata": {},
   "source": [
    "This tutorial shows how to use Cleanlab Studio's Python API for classification data that has been labeled by *multiple* annotators (where each data point has been labeled by at least one annotator, but not every annotator has labeled every data point). Asking multiple annotators to label some of the same data is common when any one annotator may not provide perfect labels. For instance, some data points in the dataset may have received 3 labels (from 3 different annotators), while other data points have only received a single label (from one annotator). \n",
    "\n",
    "Compared to existing crowdsourcing tools, Cleanlab Studio helps you better analyze such data by leveraging a trained classifier model in addition to the raw annotations. With few lines of code, you can automatically compute:\n",
    "\n",
    "- A **consensus label** for each data point (i.e. *truth inference*) that aggregates the individual annotations (more accurately than algorithms from crowdsourcing like majority-vote, Dawid-Skene, or GLAD).\n",
    "- A **quality score for each consensus label** which measures our confidence that this label is correct (via well-calibrated estimates that account for the: number of annotators which have labeled this data point, overall quality of each annotator, and quality of our trained ML models).\n",
    "- An analogous **label quality score** for each individual label chosen by one annotator for a particular data point (to measure our confidence in alternate labels when annotators differ from the consensus).\n",
    "- An **overall quality score for each annotator** which measures our confidence in the overall correctness of labels obtained from this annotator.\n",
    "\n",
    "**Consensus labels** represent the best guess of the true label for each data point and can be used for more reliable modeling/analytics. Cleanlab automatically produces enhanced estimates of consensus through the use of machine learning.\n",
    "**Quality scores** help us determine how much trust we can place in each: consensus label, individual annotator, and particular label from a particular annotator. These quality scores can help you determine which\u00a0annotators are best/worst overall, as well as which current consensus labels are least trustworthy and should perhaps be verified via additional annotation. \n",
    "\n",
    "![Dog image multiannotator example.](./assets/multiannotator-tutorial/thumbnail.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a48d31",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Install and import required dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6e5b15",
   "metadata": {},
   "source": [
    "You can use `pip` to install all packages required for this tutorial as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a170b103-4f38-435d-a05c-79e49a57cc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install cleanlab cleanlab-studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4efd119",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from cleanlab_studio import Studio\n",
    "from cleanlab.multiannotator import get_label_quality_multiannotator, get_majority_vote_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345b6678",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Format Data (can skip these details)\n",
    "\n",
    "This tutorial uses a toy *tabular* dataset but these same steps can easily be applied to **image or text data** labeled by multiple annotators. We generate a toy dataset that has 50 annotators and 300 data points from three possible classes: `0`, `1` and `2`. \n",
    "\n",
    "Each annotator annotates approximately 10% of the data points. We purposefully made the last 5 annotators in our toy dataset provide much noisier labels than the rest of the annotators.\n",
    "\n",
    "Solely for evaluating Cleanlab Studio's consensus labels against other consensus methods, we here also generate the true labels for this example dataset. However, true labels are not required for any of Cleanlab Studio's multi-annotator functions (and they usually are not available in real applications).\n",
    "We generate our multiannotator data via the `make_data()` method (can skip these details)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6843ca-909c-4139-8971-e62f2e1b1d4c",
   "metadata": {},
   "source": [
    "**Optional: Initialize helper method to make multiannotator dataset**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c37c0a69",
   "metadata": {
    "editable": true,
    "nbsphinx": "hidden",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from cleanlab.benchmarking.noise_generation import generate_noise_matrix_from_trace\n",
    "from cleanlab.benchmarking.noise_generation import generate_noisy_labels\n",
    "\n",
    "SEED = 111 # set to None for non-reproducible randomness\n",
    "np.random.seed(seed=SEED)\n",
    "\n",
    "def make_data(\n",
    "    means=[[3, 2], [7, 7], [0, 8]],\n",
    "    covs=[[[5, -1.5], [-1.5, 1]], [[1, 0.5], [0.5, 4]], [[5, 1], [1, 5]]],\n",
    "    sizes=[150, 75, 75],\n",
    "    num_annotators=50,\n",
    "):\n",
    "    \n",
    "    m = len(means)  # number of classes\n",
    "    n = sum(sizes)\n",
    "    local_data = []\n",
    "    labels = []\n",
    "\n",
    "    for idx in range(m):\n",
    "        local_data.append(\n",
    "            np.random.multivariate_normal(mean=means[idx], cov=covs[idx], size=sizes[idx])\n",
    "        )\n",
    "        labels.append(np.array([idx for i in range(sizes[idx])]))\n",
    "    X_train = np.vstack(local_data)\n",
    "    true_labels_train = np.hstack(labels)\n",
    "\n",
    "    # Compute p(true_label=k)\n",
    "    py = np.bincount(true_labels_train) / float(len(true_labels_train))\n",
    "    \n",
    "    noise_matrix_better = generate_noise_matrix_from_trace(\n",
    "        m,\n",
    "        trace=0.8 * m,\n",
    "        py=py,\n",
    "        valid_noise_matrix=True,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    \n",
    "    noise_matrix_worse = generate_noise_matrix_from_trace(\n",
    "        m,\n",
    "        trace=0.35 * m,\n",
    "        py=py,\n",
    "        valid_noise_matrix=True,\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    # Generate our noisy labels using the noise_matrix for specified number of annotators.\n",
    "    s = pd.DataFrame(\n",
    "        np.vstack(\n",
    "            [\n",
    "                generate_noisy_labels(true_labels_train, noise_matrix_better)\n",
    "                if i < num_annotators - 5\n",
    "                else generate_noisy_labels(true_labels_train, noise_matrix_worse)\n",
    "                for i in range(num_annotators)\n",
    "            ]\n",
    "        ).transpose()\n",
    "    )\n",
    "\n",
    "    # Each annotator only labels approximately 10% of the dataset\n",
    "    # (unlabeled points represented with NaN)\n",
    "    s = s.apply(lambda x: x.mask(np.random.random(n) < 0.9)).astype(\"Int64\")\n",
    "    s.dropna(axis=1, how=\"all\", inplace=True)\n",
    "    s.columns = [\"A\" + str(i).zfill(4) for i in range(1, num_annotators+1)]\n",
    "\n",
    "    row_NA_check = pd.notna(s).any(axis=1)\n",
    "\n",
    "    return {\n",
    "        \"X_train\": X_train[row_NA_check],\n",
    "        \"true_labels_train\": true_labels_train[row_NA_check],\n",
    "        \"multiannotator_labels\": s[row_NA_check].reset_index(drop=True),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99f69523",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = make_data()\n",
    "\n",
    "X = data_dict[\"X_train\"]\n",
    "multiannotator_labels = data_dict[\"multiannotator_labels\"]\n",
    "true_labels = data_dict[\"true_labels_train\"] # used for comparing the accuracy of consensus labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a705e28",
   "metadata": {},
   "source": [
    "Let's view the first few rows of the data used for this tutorial. Here are the labels selected by each annotator for the first few examples (rows) in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f241c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A0001</th>\n",
       "      <th>A0002</th>\n",
       "      <th>A0003</th>\n",
       "      <th>A0004</th>\n",
       "      <th>A0005</th>\n",
       "      <th>A0006</th>\n",
       "      <th>A0007</th>\n",
       "      <th>A0008</th>\n",
       "      <th>A0009</th>\n",
       "      <th>A0010</th>\n",
       "      <th>...</th>\n",
       "      <th>A0041</th>\n",
       "      <th>A0042</th>\n",
       "      <th>A0043</th>\n",
       "      <th>A0044</th>\n",
       "      <th>A0045</th>\n",
       "      <th>A0046</th>\n",
       "      <th>A0047</th>\n",
       "      <th>A0048</th>\n",
       "      <th>A0049</th>\n",
       "      <th>A0050</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows \u00d7 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   A0001  A0002  A0003  A0004  A0005  A0006  A0007  A0008  A0009  A0010  ...  \\\n",
       "0   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>  ...   \n",
       "1   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>      0   <NA>   <NA>   <NA>  ...   \n",
       "2   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>  ...   \n",
       "3   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>      2   <NA>   <NA>   <NA>  ...   \n",
       "4   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>  ...   \n",
       "\n",
       "   A0041  A0042  A0043  A0044  A0045  A0046  A0047  A0048  A0049  A0050  \n",
       "0   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>  \n",
       "1   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>  \n",
       "2   <NA>      0   <NA>   <NA>   <NA>   <NA>   <NA>      2   <NA>   <NA>  \n",
       "3      0   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>  \n",
       "4   <NA>   <NA>   <NA>      2   <NA>   <NA>      0   <NA>   <NA>   <NA>  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiannotator_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a705e29",
   "metadata": {},
   "source": [
    "Here are the corresponding features for these data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f0819ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.60856743,  1.41693214],\n",
       "       [-0.40908785,  2.87147629],\n",
       "       [ 4.64941785,  1.10774851],\n",
       "       [ 3.0524466 ,  1.71853246],\n",
       "       [ 4.37169848,  0.66031048]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb8131d",
   "metadata": {},
   "source": [
    "`multiannotator_labels` contains the class label that each annotator chose for each data point, with data points that a particular annotator did not label represented using `np.nan`. \n",
    "`X` contains the features for each data point, which happen to be numeric in this tutorial but any feature modality can be used. Check out [Bringing Your Own Data](#bringing-your-own-data) to learn how to run Cleanlab Studio with multi-annotator data of other modalities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51335def",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Get Initial Consensus Labels via Majority Vote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1857cc7",
   "metadata": {},
   "source": [
    "We first obtain initial consensus labels from the data annotations representing a crude guess of the best label for each data point. The most straight forward way to obtain an initial set of consensus labels is via simple majority vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d009f347",
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_vote_label = get_majority_vote_label(multiannotator_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd2272e-ee2e-495d-85d1-055054015504",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Train model and Compute Out-of-Sample Predicted Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7287b733",
   "metadata": {},
   "source": [
    "Majority vote consensus labels may not be very reliable, particularly for data points that were only labeled by one or a few annotators. To more reliably estimate consensus, we can account for the features associated with each data point (based on which the annotations were derived in the first place). Fitting a classifier model serves as a natural way to account for these feature values, here we show how to train and utilize such a classifier to get significantly more accurate estimates of consensus labels and associated quality scores.\n",
    "\n",
    "We use the Cleanlab Studio Python API to train a model with our initial consensus labels, and then get (out-of-sample) predicted class probabilities for each data point from the trained model. These predicted probabilities help us estimate the best consensus labels and associated confidence values in a statistically optimal manner that accounts for all the available information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae2cd8f-86ff-4d69-a09a-58e3bea66ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can find your API key by going to app.cleanlab.ai/upload, \n",
    "# clicking \"Upload via Python API\", and copying the API key there\n",
    "API_KEY = \"<YOUR_API_KEY>\"\n",
    "\n",
    "# initialize studio object\n",
    "studio = Studio(API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729a6754-a007-43f2-b999-ce65c0354385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload dataset\n",
    "formatted_dataframe = pd.DataFrame(X)\n",
    "formatted_dataframe[\"label\"] = majority_vote_label\n",
    "\n",
    "# set schema for all feature columns to float\n",
    "schema_overrides = [{\"name\": str(col), \"column_type\": \"float\"} for col in formatted_dataframe.columns if col != \"label\"]\n",
    "\n",
    "dataset_id = studio.upload_dataset(formatted_dataframe, dataset_name='multiannotator_tutorial', schema_overrides=schema_overrides)\n",
    "print(f'Uploaded dataset_id: {dataset_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8649e88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch project\n",
    "project_id = studio.create_project(dataset_id, \n",
    "                                   project_name='multiannotator_tutorial_itter', \n",
    "                                   modality='tabular', \n",
    "                                   model_type='fast',\n",
    "                                   label_column='label',\n",
    "                                  )\n",
    "print(f'Project successfully created and training has begun! project_id: {project_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29820061-bbd9-4fe9-8e51-3e424f9d2207",
   "metadata": {},
   "source": [
    "Once the Project has been launched successfully and you see your `project_id` you can feel free to close this notebook.  It will take some time for Cleanlab's AI to train on your data and analyze it. Come back after training is complete (you'll receive an email). You can also poll for a Project's status to programmatically wait until the results are ready for review as done below. You can optionally provide a `timeout` parameter after which the function will stop waiting even if the project is not ready.\n",
    "\n",
    "**Warning!** This next cell may take a long time to execute for big datasets. If your Jupyter notebook has timed out during this process, then you can resume work by re-running the cell (which should return instantly if the Project has completed; **do not** create a new Project)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8cba3b-5b52-4482-ab26-787f7925a03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanset_id = studio.get_latest_cleanset_id(project_id)\n",
    "studio.wait_until_cleanset_ready(cleanset_id)\n",
    "\n",
    "# Fetch predicted class probabilities for each data point:\n",
    "pred_probs = studio.download_pred_probs(cleanset_id)\n",
    "pred_probs = pred_probs.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eab5188",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Get Better Consensus Labels and Other Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d392ce5",
   "metadata": {},
   "source": [
    "Using the annotators' labels and the (out-of-sample) predicted class probabilities from the model, Cleanlab can estimate **improved consensus labels** for our data that are more accurate than our initial consensus labels were.\n",
    "\n",
    "Having accurate labels provides insight on each annotator's label quality and is key for boosting model accuracy and achieving dependable real-world results.\n",
    "\n",
    "In the **case of notebook timeout or closing of notebook**, rerun the cells up to *Get Initial Consensus Labels via Majority Vote* section. You can manually get your `pred_probs` by using the `project_id` printed above and running the code below.\n",
    "\n",
    "```python3\n",
    "    studio = Studio(api_key)\n",
    "    cleanset_id = studio.get_latest_cleanset_id(project_id)\n",
    "    studio.wait_until_cleanset_ready(cleanset_id)\n",
    "    pred_probs = studio.download_pred_probs(cleanset_id)\n",
    "    pred_probs = pred_probs.to_numpy()\n",
    "```\n",
    "\n",
    "If you do not know the `project_id` (and it is not printed above) then you can get it from the [Cleanlab Studio Web Client](https://app.cleanlab.ai/) from the `Name` column in the **Projects** section. The ID is the alpha-numeric key that follows the name and can be copied directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ca92617",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_label_quality_multiannotator(multiannotator_labels, pred_probs, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98042e7f",
   "metadata": {},
   "source": [
    "This `multiannotator.get_label_quality_multiannotator()` function returns a dictionary containing three items:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d7c0e2",
   "metadata": {},
   "source": [
    "1. `label_quality` which gives us the improved consensus labels using information from each of the annotators and the model. The DataFrame also contains information about the number of annotations, annotator agreement and consensus quality score for each data point. Higher values indicate greater confidence the consensus label is correct (unlike for many of Cleanlab\u2019s issue scores).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf945113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>consensus_label</th>\n",
       "      <th>consensus_quality_score</th>\n",
       "      <th>annotator_agreement</th>\n",
       "      <th>num_annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.658649</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.677441</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.653613</td>\n",
       "      <td>0.6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.676104</td>\n",
       "      <td>0.6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.719407</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   consensus_label  consensus_quality_score  annotator_agreement  \\\n",
       "0                0                 0.658649                  0.5   \n",
       "1                0                 0.677441                  1.0   \n",
       "2                0                 0.653613                  0.6   \n",
       "3                0                 0.676104                  0.6   \n",
       "4                0                 0.719407                  0.8   \n",
       "\n",
       "   num_annotations  \n",
       "0                2  \n",
       "1                3  \n",
       "2                5  \n",
       "3                5  \n",
       "4                5  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"label_quality\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984d65c4",
   "metadata": {},
   "source": [
    "2. `detailed_label_quality` which returns the label quality score for each label given by every annotator.  Higher values indicate greater confidence the label is correct (unlike for many of Cleanlab\u2019s issue scores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14251ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quality_annotator_A0001</th>\n",
       "      <th>quality_annotator_A0002</th>\n",
       "      <th>quality_annotator_A0003</th>\n",
       "      <th>quality_annotator_A0004</th>\n",
       "      <th>quality_annotator_A0005</th>\n",
       "      <th>quality_annotator_A0006</th>\n",
       "      <th>quality_annotator_A0007</th>\n",
       "      <th>quality_annotator_A0008</th>\n",
       "      <th>quality_annotator_A0009</th>\n",
       "      <th>quality_annotator_A0010</th>\n",
       "      <th>...</th>\n",
       "      <th>quality_annotator_A0041</th>\n",
       "      <th>quality_annotator_A0042</th>\n",
       "      <th>quality_annotator_A0043</th>\n",
       "      <th>quality_annotator_A0044</th>\n",
       "      <th>quality_annotator_A0045</th>\n",
       "      <th>quality_annotator_A0046</th>\n",
       "      <th>quality_annotator_A0047</th>\n",
       "      <th>quality_annotator_A0048</th>\n",
       "      <th>quality_annotator_A0049</th>\n",
       "      <th>quality_annotator_A0050</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.677441</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.653613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.211487</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.209905</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.676104</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.204992</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.719407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows \u00d7 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   quality_annotator_A0001  quality_annotator_A0002  quality_annotator_A0003  \\\n",
       "0                      NaN                      NaN                      NaN   \n",
       "1                      NaN                      NaN                      NaN   \n",
       "2                      NaN                      NaN                      NaN   \n",
       "3                      NaN                      NaN                      NaN   \n",
       "4                      NaN                      NaN                      NaN   \n",
       "\n",
       "   quality_annotator_A0004  quality_annotator_A0005  quality_annotator_A0006  \\\n",
       "0                      NaN                      NaN                      NaN   \n",
       "1                      NaN                      NaN                      NaN   \n",
       "2                      NaN                      NaN                      NaN   \n",
       "3                      NaN                      NaN                      NaN   \n",
       "4                      NaN                      NaN                      NaN   \n",
       "\n",
       "   quality_annotator_A0007  quality_annotator_A0008  quality_annotator_A0009  \\\n",
       "0                      NaN                      NaN                      NaN   \n",
       "1                 0.677441                      NaN                      NaN   \n",
       "2                      NaN                      NaN                      NaN   \n",
       "3                 0.209905                      NaN                      NaN   \n",
       "4                      NaN                      NaN                      NaN   \n",
       "\n",
       "   quality_annotator_A0010  ...  quality_annotator_A0041  \\\n",
       "0                      NaN  ...                      NaN   \n",
       "1                      NaN  ...                      NaN   \n",
       "2                      NaN  ...                      NaN   \n",
       "3                      NaN  ...                 0.676104   \n",
       "4                      NaN  ...                      NaN   \n",
       "\n",
       "   quality_annotator_A0042  quality_annotator_A0043  quality_annotator_A0044  \\\n",
       "0                      NaN                      NaN                      NaN   \n",
       "1                      NaN                      NaN                      NaN   \n",
       "2                 0.653613                      NaN                      NaN   \n",
       "3                      NaN                      NaN                      NaN   \n",
       "4                      NaN                      NaN                 0.204992   \n",
       "\n",
       "   quality_annotator_A0045  quality_annotator_A0046  quality_annotator_A0047  \\\n",
       "0                      NaN                      NaN                      NaN   \n",
       "1                      NaN                      NaN                      NaN   \n",
       "2                      NaN                      NaN                      NaN   \n",
       "3                      NaN                      NaN                      NaN   \n",
       "4                      NaN                      NaN                 0.719407   \n",
       "\n",
       "   quality_annotator_A0048  quality_annotator_A0049  quality_annotator_A0050  \n",
       "0                      NaN                      NaN                      NaN  \n",
       "1                      NaN                      NaN                      NaN  \n",
       "2                 0.211487                      NaN                      NaN  \n",
       "3                      NaN                      NaN                      NaN  \n",
       "4                      NaN                      NaN                      NaN  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"detailed_label_quality\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db02e63d",
   "metadata": {},
   "source": [
    "3. `annotator_stats` which gives us the annotator quality score for each annotator, alongisde other information such as the number of examples that each annotator labeled, their agreement with the consensus labels, and the class they perform the worst at. Once again, higher values indicate an annotator that provides better-quality labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efe16638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_quality</th>\n",
       "      <th>agreement_with_consensus</th>\n",
       "      <th>worst_class</th>\n",
       "      <th>num_examples_labeled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A0050</th>\n",
       "      <td>0.255696</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0047</th>\n",
       "      <td>0.305218</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0046</th>\n",
       "      <td>0.329592</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0049</th>\n",
       "      <td>0.332533</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0048</th>\n",
       "      <td>0.427690</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0031</th>\n",
       "      <td>0.528278</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0034</th>\n",
       "      <td>0.535969</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0021</th>\n",
       "      <td>0.591416</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0011</th>\n",
       "      <td>0.608669</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0015</th>\n",
       "      <td>0.630962</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       annotator_quality  agreement_with_consensus  worst_class  \\\n",
       "A0050           0.255696                  0.208333            2   \n",
       "A0047           0.305218                  0.294118            2   \n",
       "A0046           0.329592                  0.346154            1   \n",
       "A0049           0.332533                  0.310345            1   \n",
       "A0048           0.427690                  0.480000            2   \n",
       "A0031           0.528278                  0.580645            2   \n",
       "A0034           0.535969                  0.607143            2   \n",
       "A0021           0.591416                  0.718750            1   \n",
       "A0011           0.608669                  0.692308            1   \n",
       "A0015           0.630962                  0.678571            2   \n",
       "\n",
       "       num_examples_labeled  \n",
       "A0050                    24  \n",
       "A0047                    34  \n",
       "A0046                    26  \n",
       "A0049                    29  \n",
       "A0048                    25  \n",
       "A0031                    31  \n",
       "A0034                    28  \n",
       "A0021                    32  \n",
       "A0011                    26  \n",
       "A0015                    28  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"annotator_stats\"].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d09bfa",
   "metadata": {},
   "source": [
    "The `annotator_stats` DataFrame is sorted by increasing `annotator_quality`, showing us the worst annotators first.\n",
    "\n",
    "Notice that in the above table annotators with ids A0046 to A0050 have the worst annotator quality score, which is expected because we made the last 5 annotators systematically worse than the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ca8dd2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Comparing improved consensus labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b49657d",
   "metadata": {},
   "source": [
    "We can get the improved consensus labels from the `label_quality` DataFrame shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abd0fb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_consensus_label = results[\"label_quality\"][\"consensus_label\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd7a5fd",
   "metadata": {},
   "source": [
    "Since our toy dataset is synthetically generated by adding noise to each annotator's labels, we know the ground truth labels for each data point. Hence we can compare the accuracy of the consensus labels obtained using majority vote, and the improved consensus labels obtained using Cleanlab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdf061df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of majority vote labels = 0.8581081081081081\n",
      "Accuracy of Cleanlab consensus labels = 0.9763513513513513\n"
     ]
    }
   ],
   "source": [
    "majority_vote_accuracy = np.mean(true_labels == majority_vote_label)\n",
    "cleanlab_label_accuracy = np.mean(true_labels == improved_consensus_label)\n",
    "\n",
    "print(f\"Accuracy of majority vote labels = {majority_vote_accuracy}\")\n",
    "print(f\"Accuracy of Cleanlab consensus labels = {cleanlab_label_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c20b2c9",
   "metadata": {},
   "source": [
    "We can see that the accuracy of the consensus labels improved as a result of using Cleanlab, which not only takes the annotators' labels into account, but also a model to compute better consensus labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82dd4d5",
   "metadata": {},
   "source": [
    "### Inspecting consensus quality scores to find potential consensus label errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddb5453",
   "metadata": {},
   "source": [
    "We can get the consensus quality score from the `label_quality` DataFrame shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08949890",
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_quality_score = results[\"label_quality\"][\"consensus_quality_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f150a08",
   "metadata": {},
   "source": [
    "Besides obtaining improved consensus labels, Cleanlab also computes consensus quality scores for each data point. *Lower* scores represent consensus labels that are *less likely* correct (consider collecting another annotation for low-scoring data points).\n",
    "\n",
    "Here, we will extract 15 data points that have the lowest consensus quality score, and we can compare their average accuracy when compared to the true labels. We will also compute the average accuracy for the rest of the examples (rows) in the dataset for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6948b073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 15 worst quality examples = 0.8666666666666667\n",
      "Accuracy of better quality examples = 0.9822064056939501\n"
     ]
    }
   ],
   "source": [
    "sorted_consensus_quality_score = consensus_quality_score.sort_values()\n",
    "worst_quality = sorted_consensus_quality_score.index[:15]\n",
    "better_quality = sorted_consensus_quality_score.index[15:]\n",
    "\n",
    "worst_quality_accuracy = np.mean(true_labels[worst_quality] == improved_consensus_label[worst_quality])\n",
    "better_quality_accuracy = np.mean(true_labels[better_quality] == improved_consensus_label[better_quality])\n",
    "\n",
    "print(f\"Accuracy of 15 worst quality examples = {worst_quality_accuracy}\")\n",
    "print(f\"Accuracy of better quality examples = {better_quality_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b94a78-9dc5-46b3-bbe8-cf2803eaa45a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We observe that the 15 worst-consensus-quality-score data points have a lower average accuracy compared to the rest of the examples in the dataset. Cleanlab automatically determines which consensus labels are least trustworthy (perhaps want to have another annotator look at that data). Here we see these trustworthiness estimates really do correspond to the true quality of the consensus labels (which we know in this toy dataset because we have the true labels, unlike in your applications)\n",
    "\n",
    "## Retrain model using improved consensus labels\n",
    "\n",
    "After obtaining accurately-estimated consensus labels, you could again use Studio to retrain a better machine learning model using these better consensus labels. To retrain a model, follow the steps in [Train Model and Compute Out-of-Sample Predicted Probabilities](#train-model-and-compute-out-of-sample-predicted-probabilities) now setting the consensus labels to their improved counterparts like so: `formatted_dataframe['label'] = improved_consensus_label`. \n",
    "\n",
    "To quickly deploy this improved model for making predictions on new data, check out the [Deploying Reliable Models in Production](../inference_api) tutorial.\n",
    "\n",
    "### Further model improvements \n",
    "You can also repeat this process of getting better consensus labels using the model's out-of-sample predicted probabilities and then retraining the model with the improved labels to get even better predicted class probabilities\u00a0in a virtuous cycle!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5661f81-ff43-41d0-a6a8-e7e3bcf825a8",
   "metadata": {},
   "source": [
    "## Bringing Your Own Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a647163a-8df7-4103-a6c6-2c68f190e94f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "If your data are tabular (with each row corresponding to a different data point), you can replace the `formatted_dataframe` object in the [Train Model and Compute Out-of-Sample Predicted Probabilities](#train-model-and-compute-out-of-sample-predicted-probabilities) step above with your own multi-annotator labels and features. Then continue directly with the rest of the tutorial.\n",
    "\n",
    "### Formatting Labels from Multiple Annotators\n",
    "`multiannotator_labels` should be a numpy array or pandas DataFrame with each column representing an annotator and each row representing a data point. Your labels should be represented as integer indices 0, 1, ..., num_classes - 1, where data points that are not annotated by a particular annotator are represented using `np.nan` or `pd.NA`. If you have string labels or other labels that do not fit the required format, like individual lists of labels per annotator then you can convert them to the proper format using `cleanlab.internal.multiannotator_utils.format_multiannotator_labels` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a419f79-b252-4089-b188-cb85a7d34c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>question</td>\n",
       "      <td>statement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>statement</td>\n",
       "      <td>statement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>question</td>\n",
       "      <td>question</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1\n",
       "0   question  statement\n",
       "1  statement  statement\n",
       "2   question   question"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from cleanlab.internal.multiannotator_utils import format_multiannotator_labels\n",
    "\n",
    "annotator0_labels = [\"question\", \"statement\", \"question\"]  # List of labels from annotator 0\n",
    "annotator1_labels = [\"statement\", \"statement\", \"question\"]  # List of labels from annotator 1\n",
    "\n",
    "string_multiannotator_labels = pd.DataFrame(zip(annotator0_labels, annotator1_labels))  # Each annotator's labels are represent as a column of the DataFrame object.\n",
    "display(string_multiannotator_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b55ae35-cd28-4d18-b2be-3b2ebae5b2d0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "You can then use `format_multiannotator_labels()` function to transform the string labels into numerical values as required by Cleanlab. The function will return `multiannotator_labels` that are properly formatted to be passed to any `cleanlab.multiannotator` functions as well a dictionary `label_map` showing the mapping of new to old labels, such that ``mapping[k]`` returns the string name of the k-th class. Afterwards, you can use Cleanlab functionality to either get the [Majority Vote Labels](#get-initial-consensus-labels-via-majority-vote) or the [Improved Consensus Labels](#get-better-consensus-labels-and-other-statistics) depending on if you have predicted probabilites available from a trained classifier yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85d7532f-5811-4fa0-a3da-a291f86dd961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0  0  1\n",
       "1  1  1\n",
       "2  0  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "multiannotator_labels, label_map = format_multiannotator_labels(string_multiannotator_labels)\n",
    "display(multiannotator_labels)\n",
    "\n",
    "majority_vote_labels = get_majority_vote_label(multiannotator_labels)  # Example of getting majority vote labels with the formatted data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d51ab3-9fc6-45aa-9e0a-ac3f68c39ced",
   "metadata": {},
   "source": [
    "### Working with Features of Different Modalities \n",
    "Your features can be represented however you like as long as they are in a [format accepted by Cleanlab Studio](/studio/quickstart/web#upload-a-dataset)! The feature columns can stay in these acceptable formats when creating the dataset to be passed into Studio. For example, if you are working with an Image Dataset the feature column can contain an external url linking to the image file or use individual pixel values as feature columns. If following the [Large Scale Image Dataset](../large_image_datasets) tutorial, you could replace the `class_name` csv column with `majority_vote_labels`. \n",
    "\n",
    "If you are unsure on the proper data format, use the **How to Format Your Dataset** wizard on Cleanlab Studio's **Upload** page (in the web browser) to see the best way to create feature columns for your specific type of data.\n",
    "\n",
    "### Creating and Uploading a Custom Dataset into Studio\n",
    "\n",
    "Note that (as demonstrated earlier in this tutorial) prior to data upload (`studio.upload_dataset`), you should first produce initial consensus labels (eg. via majority-vote) and *only* upload those initial consensus labels (not the raw multi-annotator labels)!\n",
    "After using Cleanlab to generate a single set of initial consensus labels (eg. the `majority_vote_labels` above), you can create a `formatted_dataframe` to input directly into `studio.upload_dataset()`.\n",
    "\n",
    "The DataFrame should contain a \"label\" column (containing the initial consensus label) and one or more feature columns. The \"label\" column must contain a single class for each data point (row), while the features can be represented in any acceptable format discussed above.\n",
    "\n",
    "Below is an example of creating a `formatted_dataframe` object from a basic text dataset (with a single feature column \"sentence\"). From here, you can continue with the rest of the tutorial above by uploading this dataset to Cleanlab Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "123938e4-4a0d-4681-95bf-c39a0304ac0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We are running</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I like cake.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Do you like dogs?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sentence  label\n",
       "0     We are running      0\n",
       "1       I like cake.      1\n",
       "2  Do you like dogs?      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = [\"We are running\", \"I like cake.\", \"Do you like dogs?\"]\n",
    "formatted_dataframe = pd.DataFrame(sentences, columns=[\"sentence\"])\n",
    "formatted_dataframe[\"label\"] = majority_vote_labels\n",
    "\n",
    "display(formatted_dataframe.head())\n",
    "# dataset_id = studio.upload_dataset(formatted_dataframe, dataset_name='multiannotator_tutorial_custom_dataframe_example')  # Optional upload custom dataset to Cleanlab Studio on your own"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5305605f-5927-4d95-9cad-c16077f6c187",
   "metadata": {},
   "source": [
    "## How does our algorithm work?\n",
    "All estimates above are produced via the CROWDLAB algorithm, described in this paper that contains extensive benchmarks which show CROWDLAB can produce better estimates than popular methods like Dawid-Skene and GLAD:\n",
    "\n",
    "[CROWDLAB: Supervised learning to infer consensus labels and quality scores for data with multiple annotators](https://arxiv.org/abs/2210.06812)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "50292dbb1f747f7151d445135d392af3138fb3c65386d17d9510cb605222b10b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}