{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0d2e007",
   "metadata": {},
   "source": [
    "# Detecting Issues in Named Entity Recognition Datasets\n",
    "\n",
    "<head>\n",
    "  <meta name=\"title\" content=\"Detecting Issues in Named Entity Recognition Datasets\"/>\n",
    "  <meta property=\"og:title\" content=\"Detecting Issues in Named Entity Recognition Datasets\"/>\n",
    "  <meta name=\"twitter:title\" content=\"Detecting Issues in Named Entity Recognition Datasets\" />\n",
    "  <meta name=\"image\" content=\"/img/entityrecognition.png\" />\n",
    "  <meta property=\"og:image\" content=\"/img/entityrecognition.png\" />\n",
    "  <meta name=\"description\" content=\"A quick tutorial using an automated Data-Centric AI platform.\"  />\n",
    "  <meta property=\"og:description\" content=\"A quick tutorial using an automated Data-Centric AI platform.\" />\n",
    "  <meta name=\"twitter:description\" content=\"A quick tutorial using an automated Data-Centric AI platform.\" />\n",
    "</head>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3aee1a",
   "metadata": {},
   "source": [
    "In this tutorial, we'll use Cleanlab Studio to automatically find label errors and other issues in Named Entity Recognition (NER) data. Label issues in NER datasets encompass problems like incorrect class labels, inconsistent choices across different data annotators, incorrect entity boundary labeling, ambiguity in entity types (multiple types might appear reasonable), entities overlooked by annotators, etc. Identifying and resolving these annotation problems and other data issues is essential for producing a high-quality dataset that can be used to train reliable NER models.\n",
    "\n",
    "![Thumbnail showing NER errors.](./assets/named-entity-recognition-tutorial/thumbnail.jpg)\n",
    "\n",
    "This tutorial considers the popular CONLL-2003 Named Entity Recognition dataset that contains labeled examples of entities in text classified into categories such as persons, organizations, and locations.  You can replace this dataset with your own NER data as long as you transform it into the format described in the [Download and Prepare Raw Dataset](#download-and-prepare-raw-dataset) section.\n",
    "\n",
    "(Note: this tutorial requires that you've created a [Cleanlab](https://app.cleanlab.ai/) account)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da020bc",
   "metadata": {},
   "source": [
    "## Install and import required dependencies\n",
    "\n",
    "You can use `pip` to install all packages required for this tutorial as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0592763-6e03-4639-b6e9-c5f662f50d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"cleanlab[datalab]\" cleanlab-studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1349304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab_studio import Studio\n",
    "from cleanlab.internal.token_classification_utils import get_sentence, filter_sentence, process_token, mapping, merge_probs\n",
    "\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a1b2d1b-61e4-4997-801f-5f3657a03221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can find your Cleanlab Studio API key by going to app.cleanlab.ai/upload, \n",
    "# clicking \"Upload via Python API\", and copying the API key there\n",
    "API_KEY = \"<insert your API key>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc86e827-7f15-4695-b800-c0cb761f9e33",
   "metadata": {},
   "source": [
    "## Download and Prepare Raw Dataset\n",
    "\n",
    "We download the CONLL-2003 dataset into a `data/` directory. This tutorial analyzes CONLL-2003 but you can use any dataset as long as it is either: stored in a similar format on your local machine as this CONLL-2003 dataset, or it is available via the Hugging Face datasets bank in a similar format to the [TNER_huggingface](https://huggingface.co/datasets/tner/bionlp2004) dataset.\n",
    "\n",
    "**This tutorial focuses on using CONLL-2003 data which we download below**. To run this notebook on your own entity recognition dataset, first convert it to the CONLL-2003 format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9018de0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://data.deepai.org/conll2003.zip && mkdir -p data \n",
    "!unzip conll2003.zip -d data/ && rm conll2003.zip "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a2ce40-2ef2-48bb-959a-66becc58c912",
   "metadata": {},
   "source": [
    "### Accepted Local Format\n",
    "This notebook will also run with a dataset that is stored locally. For local datasets, they need to be in a format similar to the CONLL-2003 data.\n",
    "\n",
    "CONLL-2003 data are in the following format: \n",
    "\n",
    "`-DOCSTART- -X- -X- O` \n",
    "\n",
    "`[word] [pos_tags] [chunk_tags] [ner_tags]` <- Start of first sentence \n",
    "\n",
    "`...`\n",
    "\n",
    "`[word] [pos_tags] [chunk_tags] [ner_tags]` \n",
    "\n",
    "`[empty line]` \n",
    "\n",
    "`[word] [pos_tags] [chunk_tags] [ner_tags]` <- Start of second sentence \n",
    "\n",
    "`...`\n",
    "\n",
    "`[word] [pos_tags] [chunk_tags] [ner_tags]` \n",
    "\n",
    "The `ner_tags` (named-entity recognition tags) are stored in the IOB2 format. CONLL-2003 includes the classes detailed below however custom `ner_tags` are also accepted.\n",
    "\n",
    "| `ner_tags` |             Description              |\n",
    "|:----------:|:------------------------------------:|\n",
    "|     `O`    |      Other (not a named entity)      |\n",
    "|   `B-MIS`  | Beginning of a miscellaneous entity  |\n",
    "|   `I-MIS`  |         Miscellaneous entity         |\n",
    "|   `B-PER`  |     Beginning of a person entity     |\n",
    "|   `I-PER`  |            Person entity             |\n",
    "|   `B-ORG`  | Beginning of an organization entity  |\n",
    "|   `I-ORG`  |         Organization entity          |\n",
    "|   `B-LOC`  |    Beginning of a location entity    |\n",
    "|   `I-LOC`  |           Location entity            | \n",
    "\n",
    "For more information, see [here](https://paperswithcode.com/dataset/conll-2003). For all local datasets, we cast all-caps words into lowercase except for the first character (eg. `JAPAN` -> `Japan`), to discourage the tokenizer from breaking such words into multiple subwords.\n",
    "\n",
    "If you are working with **data in similar format to CONLL-2003**, set the `dataset_path` variable to either a single filepath or a list of all filepaths you want to upload to Cleanlab Studio. \n",
    "\n",
    "### Accepted Hugging Face format\n",
    "Beyond running Cleanlab Studio on a locally-stored NER dataset in the CONLL-2003 format, you can also use a dataset stored in the Hugging Face datasets repository in TNER format. The [TNER](https://huggingface.co/tner) Organization hosts models and common datasets for T-NER, which is a python tool for language model fine-tuning on NER data. An example format of one dataset is below. \n",
    "\n",
    "Here each example is made up of a list of `tokens` alongside a list of their respective named entity `tags` where for the `i`-th example, `tokens[i][j]` is the `j`-th token in the sentence with tag `tags[i][j]`. \n",
    "\n",
    "![Hugging Face TNER Format](./assets/named-entity-recognition-tutorial/tner_huggingface.png)\n",
    "\n",
    "To **upload your own dataset in TNER format**, format each example into `token` and `tag` lists and then follow the instructions [here](https://huggingface.co/docs/datasets/upload_dataset) then set the `dataset_path` variable to your dataset_repository/dataset_name or choose one of the datasets already provided by the organization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7214a76b-711e-4094-8f61-fe9b41b96407",
   "metadata": {},
   "source": [
    "### Reformatting Named Entity Recognition Data\n",
    "\n",
    "Occasionally, you may find improved performance in modifying the original Named Entities within a dataset before using Cleanlab Studio. This could involve consolidating 'beginning' and 'continuation' tags into a single named entity tag or eliminating specific word/tag combinations. To address this, we've introduced the `reformat_ner_data()` function below. This function accepts a list of filepaths *in accepted local format* and a dictionary that defines the desired transformations, and it generates new *files in accepted format* with the transformed tags.\n",
    "\n",
    "To utilize this function, create a `transformation_map` dictionary. Each dictionary key represents a named entity in your dataset, and its corresponding value defines the desired transformation. When multiple keys share the same value, these entities will be merged in the resulting files. If a specific key has a value of 'nan', the corresponding tokens will be omitted from sentences in the new files.\n",
    "\n",
    "Below is an example that demonstrates how to define transformations using the `transformation_map` for the CONLL-2003 example dataset.\n",
    "\n",
    "```python3\n",
    "transformation_map = {\n",
    "    \"O\": \"nan\",\n",
    "    \"B-MISC\": \"nan\",\n",
    "    \"I-MISC\": \"nan\",\n",
    "    \"B-PER\": \"has_person\",\n",
    "    \"I-PER\": \"has_person\",\n",
    "    \"B-ORG\": \"has_organization\",\n",
    "    \"I-ORG\": \"has_organization\",\n",
    "    \"B-LOC\": \"has_location\",\n",
    "    \"I-LOC\": \"has_location\",\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "This `transfomation_map` lists all the Named Entities found in the data as keys `['O', 'B-MISC', 'I-MISC', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']`. The tokens marked as \"O\", \"B-MISC\" and \"I-MISC\" are omitted from each sequence since these keys have `nan` set as their value. Entities marked as \"B-PER\" and \"I-PER\" are now marked as the same tag \"has_person\". A similar union happens to \"B-ORG\" and \"I-ORG\", which are united under \"has_organization\" and \"B-LOC\" and \"I-LOC\", which become \"has_location\". Basically we are saying our analysis only cares about the following entities: \"person\", \"organization\", \"location\". \n",
    "\n",
    "By passing this dictionary into the `reformat_ner_data(filepath, transformation_map)` function defined below, along with the desired `filepath` of the files you wish to transform, a new files featuring the updated entities will be generated. In the example below, you can locate the reformatted file at `'./data/transform_train.txt'` or at a location defined by the optional parameter `new_filepath`.\n",
    "\n",
    "```python3\n",
    "filepath = './data/train.txt'\n",
    "dataset_path = reformat_ner_data(filepath, transformation_map)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edab736-3224-4a9a-86ec-5474cdeea2dc",
   "metadata": {},
   "source": [
    "To reformat a dataset stored in the Hugging Face datasets repository, iterate through the examples and replace tags with their respective alternative in the `transform_data_entity_map`. Omit tags mapped to `nan`. Afterwards, upload the new dataset to Hugging Face and continue with the rest of the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d709a22-914c-4698-b5af-6e59104146b4",
   "metadata": {},
   "source": [
    "**Optional: Define helper methods for formatting and analyzing NER data with Cleanlab**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "570e90f3-bcbd-430b-8d93-64e130a9bfd5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def reformat_ner_data(\n",
    "    filepath, transformed_filepath_dic, new_filepath=None, overwrite=False, sep=\" \"\n",
    "):\n",
    "    \"\"\"Reformats data stored locally in the CONLL-2003 format using `transformed_filepath_dic`. Creates\n",
    "    new data files with applied transformations and returns their names. If overwrite is set to True\n",
    "    and new_filepath points to an already existing file, then data files are overwritten instead of\n",
    "    created.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath: str\n",
    "        Path pointing to local file in CONLL-2003 format.\n",
    "    transformed_filepath_dic: dict\n",
    "        Transformation dictionary where each key represents a named entity in your dataset, and its corresponding\n",
    "        value is a str of the desired transformation. If value is \"nan\", tokens with key are not included in the new file.\n",
    "    new_filepath: str\n",
    "        New filepath which the transformed data will be saved into.\n",
    "    overwrite: bool\n",
    "        If False, will throw error when attempting to overwrite an existing file. If set to True, data will be overwritten with a warning.\n",
    "    sep: str\n",
    "        Seperator between each type of tag.\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    new_filepath: str\n",
    "        Location of the reformatted file.\n",
    "    \"\"\"\n",
    "\n",
    "    ignored_labels = [key for key, value in transformed_filepath_dic.items() if value == \"nan\"]\n",
    "    if new_filepath is None:\n",
    "        split = filepath.split(\"/\")\n",
    "        new_filepath = \"/\".join(split[:-1] + [\"transform_\" + split[-1]])\n",
    "    if os.path.exists(new_filepath):\n",
    "        if overwrite:\n",
    "            print(\n",
    "                f\"Warning: File {new_filepath} already exists, overwriting existing file with reformatted data. \"\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                f\"Error: File {new_filepath} already exists, cannot overwrite existing file. Either set overwrite=True or provide a path to a new file.\"\n",
    "            )\n",
    "    with open(filepath) as lines, open(new_filepath, \"w\") as new_file:\n",
    "        data, sentence, label = [], [], []\n",
    "        for line in lines:\n",
    "            if len(line) == 0 or line.startswith(\"-DOCSTART\") or line[0] == \"\\n\":\n",
    "                new_file.write(line)\n",
    "                continue\n",
    "            splits = line.split(sep)\n",
    "            label = splits[-1][:-1]\n",
    "            if (\n",
    "                label in ignored_labels\n",
    "            ):  # Ignore NEs that are marked as \"nan\" in transformed_filepath_dic\n",
    "                continue\n",
    "            splits[-1] = transformed_filepath_dic[label] + splits[-1][-1]\n",
    "            new_line = sep.join(splits)\n",
    "            new_file.write(new_line)\n",
    "    return new_filepath\n",
    "\n",
    "\n",
    "def read_from_huggingface(dataset_path):\n",
    "    \"\"\"Reads Hugging Face dataset from dataset bank at dataset_path. Data should be in same format as data in TNER dataset bank.\"\"\"\n",
    "\n",
    "    dataset = load_dataset(dataset_path)\n",
    "    data_instances = dataset.keys()\n",
    "    if isinstance(data_instances, str):\n",
    "        data_instances = [data_instances]\n",
    "\n",
    "    given_words = []\n",
    "    given_labels = []\n",
    "    for instance in data_instances:\n",
    "        instance_dataset = dataset[instance]\n",
    "        given_words.extend(dataset[instance][\"tokens\"])\n",
    "        given_labels.extend(dataset[instance][\"tags\"])\n",
    "    return given_words, given_labels\n",
    "\n",
    "\n",
    "def read_local_data(filepath, sep=\" \"):\n",
    "    \"\"\"Reads local data that is in a format like the CONLL-2003 dataset from a single string filepath or a list of local files.\"\"\"\n",
    "\n",
    "    # This code is adapted from: https://github.com/kamalkraj/BERT-NER/blob/dev/run_ner.py\n",
    "    lines = open(filepath)\n",
    "    data, sentence, label = [], [], []\n",
    "    for line in lines:\n",
    "        if len(line) == 0 or line.startswith(\"-DOCSTART\") or line[0] == \"\\n\":\n",
    "            if len(sentence) > 0:\n",
    "                data.append((sentence, label))\n",
    "                sentence, label = [], []\n",
    "            continue\n",
    "        splits = line.split(sep)\n",
    "        word = splits[0]\n",
    "        if len(word) > 0 and word[0].isalpha() and word.isupper():\n",
    "            word = word[0] + word[1:].lower()\n",
    "        sentence.append(word)\n",
    "        label.append(splits[-1][:-1])\n",
    "\n",
    "    if len(sentence) > 0:\n",
    "        data.append((sentence, label))\n",
    "\n",
    "    given_words = [d[0] for d in data]\n",
    "    given_labels = [d[1] for d in data]\n",
    "\n",
    "    return given_words, given_labels\n",
    "\n",
    "\n",
    "def read_ner_data(dataset_path, format):\n",
    "    \"\"\"Function that takes in NER data and returns an array of given words and given labels for each example.\n",
    "    Compatible with \"conll2003_like\" and \"from_huggingface\" formats.\n",
    "    \"\"\"\n",
    "\n",
    "    if format == \"conll2003_like\":\n",
    "        if isinstance(dataset_path, str):\n",
    "            dataset_path = [dataset_path]\n",
    "        given_words, given_labels = [], []\n",
    "        for data in dataset_path:\n",
    "            words, labels = read_local_data(data, sep=\" \")\n",
    "            given_words.extend(words)\n",
    "            given_labels.extend(labels)\n",
    "    elif format == \"from_huggingface\":\n",
    "        given_words, given_label = read_from_huggingface(dataset_path)\n",
    "    else:\n",
    "        print(\n",
    "            f\"Error. format=={format} is not a supported as a NER data format at this time. Supported formats: [conll2003_like, from_huggingface].\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    ner_df = create_ner_df(given_words, given_labels)\n",
    "\n",
    "    return ner_df\n",
    "\n",
    "\n",
    "def create_ner_df(given_words, given_labels):\n",
    "    \"\"\"Transforms NER given_words and given_labels into a format required by Studio.\"\"\"\n",
    "\n",
    "    sentences = list(map(get_sentence, given_words))\n",
    "    sentences, mask = filter_sentence(sentences)\n",
    "    given_words = [words for m, words in zip(mask, given_words) if m]\n",
    "    given_labels = [labels for m, labels in zip(mask, given_labels) if m]\n",
    "\n",
    "    unique_labels = [list(np.unique(label)) for label in given_labels]\n",
    "    labels = [\",\".join(str(k) for k in label) for label in unique_labels]\n",
    "    id = range(len(labels))\n",
    "\n",
    "    ner_df = pd.DataFrame([id, sentences, given_labels, labels]).transpose()\n",
    "    ner_df.columns = [\"id\", \"tokens\", \"tags\", \"tag_sets\"]\n",
    "    ner_df[\"tags\"] = ner_df[\"tags\"].astype(str)\n",
    "\n",
    "    return ner_df\n",
    "\n",
    "\n",
    "def get_suggested_labels(cleanlab_cols):\n",
    "    \"\"\"Returns suggested labels as a list of labels for each example provided by cleanlab_cols.\"\"\"\n",
    "\n",
    "    suggested_labels = cleanlab_cols[\"suggested_label\"]\n",
    "    suggested_labels = [label.split(\",\") for label in suggested_labels]\n",
    "    return suggested_labels\n",
    "\n",
    "\n",
    "def get_issue_df(ner_df, cleanlab_cols):\n",
    "    \"\"\"Returns NER results in a dataframe containing original labels, cleanlab_cols and \"issue_details\" column which summarizes NER issues\n",
    "    found in each example.\n",
    "    \"\"\"\n",
    "    cleanlab_issue_col_names = [\n",
    "        \"cleanlab_row_ID\",\n",
    "        \"is_label_issue\",\n",
    "        \"label_issue_score\",\n",
    "        \"is_well_labeled\",\n",
    "        \"is_near_duplicate\",\n",
    "        \"near_duplicate_score\",\n",
    "        \"near_duplicate_cluster_id\",\n",
    "        \"is_ambiguous\",\n",
    "        \"ambiguous_score\",\n",
    "    ]\n",
    "\n",
    "    df_suggested_given = ner_df[[\"id\", \"tag_sets\"]].merge(\n",
    "        cleanlab_cols[[\"suggested_label\"]], how=\"right\", left_index=True, right_index=True\n",
    "    )\n",
    "    suggested_labels = df_suggested_given[\"suggested_label\"]\n",
    "    suggested_labels = [label.split(\",\") for label in suggested_labels]\n",
    "    given_labels = [tag.split(\",\") for tag in df_suggested_given[\"tag_sets\"]]\n",
    "\n",
    "    suggested_but_not_given = [\n",
    "        np.setdiff1d(suggested, given).tolist()\n",
    "        for given, suggested in zip(given_labels, suggested_labels)\n",
    "    ]\n",
    "    given_but_not_suggested = [\n",
    "        np.setdiff1d(given, suggested).tolist()\n",
    "        for given, suggested in zip(given_labels, suggested_labels)\n",
    "    ]\n",
    "    ne_issue_column = []\n",
    "    for suggested, given, is_issue in zip(\n",
    "        suggested_but_not_given, given_but_not_suggested, cleanlab_cols[\"is_label_issue\"]\n",
    "    ):\n",
    "        issue_detail = \"\"\n",
    "        if not is_issue:\n",
    "            issue_detail = np.nan\n",
    "        elif len(suggested) == 0 and len(given) == 0:\n",
    "            issue_detail = \"This sentence contains a label issue with one or more named entities.\"\n",
    "        else:\n",
    "            issue_detail = \"\"\n",
    "            if len(suggested) > 0:\n",
    "                issue_detail += (\n",
    "                    \"Possible named entitiy tags that are missing: \" + \",\".join(suggested) + \". \"\n",
    "                )\n",
    "            if len(given) > 0:\n",
    "                issue_detail += (\n",
    "                    \"Possible named entities tags that are incorrectly present: \"\n",
    "                    + \",\".join(given)\n",
    "                    + \". \"\n",
    "                )\n",
    "        ne_issue_column.append(issue_detail)\n",
    "\n",
    "    issue_df = cleanlab_cols[cleanlab_issue_col_names]\n",
    "    issue_df.insert(1, \"issue_details\", ne_issue_column)\n",
    "\n",
    "    return issue_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1a240f-5afc-40cd-a8b1-e2a323a98c68",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Load Data into Cleanlab Studio\n",
    "\n",
    "Once we have our data in an acceptable format, we can use the helper function `upload_ner_data()` to upload our dataset into Cleanlab Studio. The function returns the `dataset_id` which we will need for the following step to start a project. It takes in the following arguments:\n",
    "- **api_key**: Your Cleanlab Studio API key which you can find by going to app.cleanlab.ai/upload, clicking \"Upload via Python API\", and copying the API key there\n",
    "- **dataset_path**: The location of the dataset you want to run Named Entity Recognition on. If this data is in \"from_huggingface\" data format then `dataset_path` should be the Hugging Face hub data path. If data is in \"conll2003_like\" format, then the path can be a single local file or list of local file paths.\n",
    "- **data_format**:  Expected format of NER data to be read in. Acceptable formats are described in the [Download and Prepare Raw Dataset](#download-and-prepare-raw-dataset) section above.\n",
    "- **dataset_name (optional)**: Name for your dataset in Cleanlab Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46c692be-c76d-4b78-b978-5f9160f55c6b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def upload_ner_data(\n",
    "    api_key,\n",
    "    dataset_path,\n",
    "    data_format,\n",
    "    dataset_name=\"NER_tutorial_example_dataset\",\n",
    "):\n",
    "    \"\"\"Uploads dataset from dataset_path to Cleanlab Studio and returns the dataset_id.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    api_key: str\n",
    "        Cleanlab Studio API key.\n",
    "    dataset_path: str, list\n",
    "        Location of dataset (either local path or name of the dataset in hugging face).\n",
    "    data_format: str\n",
    "        Expected format of NER data to be read in. Acceptable formats: {\"conll2003_like\", \"from_huggingface\"}.\n",
    "    dataset_name: str\n",
    "        Name for your dataset in Cleanlab Studio.\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    dataset_id: str\n",
    "        Cleanlab Studio ID of the uploaded dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # Start Cleanlab Studio\n",
    "    studio = Studio(api_key)\n",
    "\n",
    "    # Upload Data to Studio\n",
    "    ner_df = read_ner_data(dataset_path, format=data_format)\n",
    "    display(ner_df.head())\n",
    "    dataset_id = studio.upload_dataset(ner_df, dataset_name=dataset_name)\n",
    "\n",
    "    return dataset_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6eb118-1449-442e-a063-f82512ead26c",
   "metadata": {},
   "source": [
    "For our example, let's transform the beginning and continuation named entity tags into single tags `has_[entity]` because we are interested in detecting annotation errors at the entity-level not with respect to beginning vs. continuation subcategories. Let's also group all miscellaneous entities together under `has_other` using `reformat_ner_data()` as described in [Reformatting Named Entity Recognition Data](#reformatting-named-entity-recognition-data).\n",
    "\n",
    "In your data, similarly group the entities that you are not interested in distinguishing between to avoid seeing a bunch of Cleanlab outputs related to trivial distinctions between entities within the same group.  We then pass in our API key and transformed filepaths into `upload_ner_data()` to upload the dataset to Cleanlab Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9ca480-2b97-4f1f-af1f-768f8015ab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = [\n",
    "    \"./data/train.txt\",\n",
    "    \"./data/valid.txt\",\n",
    "    \"./data/test.txt\",\n",
    "]  # test with multiple filepaths\n",
    "transform_data_entity_map = {\n",
    "    \"O\": \"has_other\",\n",
    "    \"B-MISC\": \"has_other\",\n",
    "    \"I-MISC\": \"has_other\",\n",
    "    \"B-PER\": \"has_person\",\n",
    "    \"I-PER\": \"has_person\",\n",
    "    \"B-ORG\": \"has_organization\",\n",
    "    \"I-ORG\": \"has_organization\",\n",
    "    \"B-LOC\": \"has_location\",\n",
    "    \"I-LOC\": \"has_location\",\n",
    "}\n",
    "dataset_path = [\n",
    "    reformat_ner_data(filepath, transform_data_entity_map, overwrite=True) for filepath in filepaths\n",
    "]\n",
    "data_format = \"conll2003_like\"  # Change this to from_huggingface if dataset_path points to a huggingface dataset.\n",
    "print(f\"data_format: {data_format}\\ndataset_path: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec3fd12d-5657-4331-a2a1-2371dbef5410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "      <th>tag_sets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Eu rejects German call to boycott British lamb.</td>\n",
       "      <td>['has_organization', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other']</td>\n",
       "      <td>has_organization,has_other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Peter Blackburn</td>\n",
       "      <td>['has_person', 'has_person']</td>\n",
       "      <td>has_person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Brussels 1996-08-22</td>\n",
       "      <td>['has_location', 'has_other']</td>\n",
       "      <td>has_location,has_other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The European Commission said on Thursday it disagreed with German advice to consumers to shun British lamb until scientists determine whether mad cow disease can be transmitted to sheep.</td>\n",
       "      <td>['has_other', 'has_organization', 'has_organization', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other']</td>\n",
       "      <td>has_organization,has_other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Germany's representative to the European Union's veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer.</td>\n",
       "      <td>['has_location', 'has_other', 'has_other', 'has_other', 'has_other', 'has_organization', 'has_organization', 'has_other', 'has_other', 'has_other', 'has_person', 'has_person', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_location', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other']</td>\n",
       "      <td>has_location,has_organization,has_other,has_person</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id  \\\n",
       "0  0   \n",
       "1  1   \n",
       "2  2   \n",
       "3  3   \n",
       "4  4   \n",
       "\n",
       "                                                                                                                                                                                                               tokens  \\\n",
       "0                                                                                                                                                                     Eu rejects German call to boycott British lamb.   \n",
       "1                                                                                                                                                                                                     Peter Blackburn   \n",
       "2                                                                                                                                                                                                 Brussels 1996-08-22   \n",
       "3                          The European Commission said on Thursday it disagreed with German advice to consumers to shun British lamb until scientists determine whether mad cow disease can be transmitted to sheep.   \n",
       "4  Germany's representative to the European Union's veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                        tags  \\\n",
       "0                                                                                                                                                                                                                                                                                                               ['has_organization', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other']   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                               ['has_person', 'has_person']   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                              ['has_location', 'has_other']   \n",
       "3                       ['has_other', 'has_organization', 'has_organization', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other']   \n",
       "4  ['has_location', 'has_other', 'has_other', 'has_other', 'has_other', 'has_organization', 'has_organization', 'has_other', 'has_other', 'has_other', 'has_person', 'has_person', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_location', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other', 'has_other']   \n",
       "\n",
       "                                             tag_sets  \n",
       "0                          has_organization,has_other  \n",
       "1                                          has_person  \n",
       "2                              has_location,has_other  \n",
       "3                          has_organization,has_other  \n",
       "4  has_location,has_organization,has_other,has_person  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading dataset...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|\n",
      "Ingesting Dataset...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|\n"
     ]
    }
   ],
   "source": [
    "dataset_id = upload_ner_data(api_key=API_KEY, dataset_path=dataset_path, data_format=data_format)\n",
    "print(f\"Uploaded dataset_id: {dataset_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e32604",
   "metadata": {},
   "source": [
    "## Train Model to Analyze the CONLL-2003 Data\n",
    "\n",
    "Once we have our data in an acceptable format, we can launch a Project in Cleanlab Studio. A Project automatically trains ML models that analyze the data for various issues, which takes some time to complete. To launch a Project, use the `launch_ner_project()` function, which takes in the following arguments:\n",
    "- **api_key**: Your Cleanlab Studio API key which you can find by going to app.cleanlab.ai/upload, clicking \"Upload via Python API\", and copying the API key there.\n",
    "- **dataset_id**: Cleanlab Studio ID of a dataset uploaded using `upload_ner_data()`\n",
    "- **project_name (optional)**: Name for resulting project.\n",
    "- **model_type (optional)**:  Type of model to train (either 'fast' or 'regular'). Cleanlab Studio's analysis of your dataset is based on training a ML model. A fast model may train quicker but give inferior results. If argument is not specified, a regular model is trained to return the best results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4166cb22-0abd-47a4-a9f4-f9a7836638e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_ner_project(\n",
    "    api_key, dataset_id, project_name=\"NER_tutorial_example_project\", model_type=\"regular\"\n",
    "):\n",
    "    \"\"\"Creates project and begins training in Cleanlab Studio with dataset provided in dataset_id.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    api_key: str\n",
    "        Cleanlab Studio API key.\n",
    "    dataset_id: str\n",
    "        Cleanlab Studio ID of an uploaded dataset.\n",
    "    project_name: str\n",
    "        Name for project in Cleanlab Studio. If no name is provided, the project is titled \"NER_tutorial_example_project\".\n",
    "    model_type: str\n",
    "        Type of model to train (fast or regular).\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    project_id: str\n",
    "        Cleanlab Studio ID of created project.\n",
    "    \"\"\"\n",
    "    # Start Cleanlab Studio\n",
    "    studio = Studio(api_key)\n",
    "\n",
    "    project_id = studio.create_project(\n",
    "        dataset_id,\n",
    "        project_name=project_name,\n",
    "        modality=\"text\",\n",
    "        task_type=\"multi-label\",\n",
    "        model_type=model_type,\n",
    "        label_column=\"tag_sets\",\n",
    "        text_column=\"tokens\",\n",
    "    )\n",
    "    return project_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6aece9-5bf2-4ac8-9e03-e0445ac6fc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = launch_ner_project(API_KEY, dataset_id)\n",
    "print(f\"Project successfully created and training has begun! project_id: {project_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45da5cc2-0c56-413f-97d1-1fa99135fe12",
   "metadata": {},
   "source": [
    "Once the Project has been launched successfully and you see your `project_id` you can feel free to close this notebook.  It will take some time for Cleanlab's AI to train on your data and analyze it. Come back after training is complete (you'll get an email) and continue with the notebook to review your results.\n",
    "You should only execute the above cell once per dataset. After launching the Project, you can poll for its status to programmatically wait until the results are ready for review as done below. You can optionally provide a `timeout` parameter after which the function will stop waiting even if the project is not ready.\n",
    "\n",
    "**Warning:** This next cell may take a long time to execute for big datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dccf0e-7c1a-4858-a6f3-106c65873625",
   "metadata": {},
   "outputs": [],
   "source": [
    "studio = Studio(API_KEY)\n",
    "cleanset_id = studio.get_latest_cleanset_id(project_id)\n",
    "print(f\"cleanset_id: {cleanset_id}\")\n",
    "studio.wait_until_cleanset_ready(cleanset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6c5429-8955-4771-8b63-0c93ca841ee3",
   "metadata": {},
   "source": [
    "If your Jupyter notebook has timed out during this process, then you can resume work by re-running the cell (which should return instantly if the project has completed; **do not** create a new Project).\n",
    "After notebook timeout: you'd also need to re-run the starting cells up to the *Initialize Helper Methods* section. Finally, redefine `dataset_path` and `data_format` as they are printed above. If you applied any reformatting to your data using `reformat_ner_data()` you should pass in the updated `dataset_path` pointing to these reformatted files (or apply the same transformations again to the original data). **Do not** create a new Project though!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6c5429-8955-4771-8b63-0c93ca841ee2",
   "metadata": {},
   "source": [
    "## Results of the Data Analysis \n",
    "\n",
    "Once it is ready, you can optionally go view your Project via the [web interface](https://app.cleanlab.ai/) to explore the results interactively.\n",
    "\n",
    "Identifying and analyzing examples with label issues is essential for improving dataset quality and training reliable NER models. Here we proceed programmatically via the `issues_df` returned by Cleanlab Studio, which lists automatically detected issues in our entity recognition dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4affee30-7f16-44e8-b530-6d577c56815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ner_results(api_key, cleanset_id, dataset_path, data_format):\n",
    "    \"\"\"Takes in cleanset_id and original data and returns DataFrame of issues detected in the dataset.\n",
    "    The rows of this DataFrame correspond to same ordering of text examples in your original dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    api_key: str\n",
    "        Cleanlab Studio API key.\n",
    "    cleanset_id: str\n",
    "        Cleanlab Studio ID of completed project.\n",
    "    dataset_path: str\n",
    "        Location of dataset (either local path or name of the dataset in hugging face).\n",
    "    data_format: str\n",
    "        Expected format of NER data to be read in. Acceptable formats: {\"conll2003_like\", \"from_huggingface\"}.\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    issue_df: pd.DataFrame\n",
    "         Dataframe containing original labels, cleanlab_cols and \"issue_details\" column which summarizes NER issues found in each example.\n",
    "    \"\"\"\n",
    "\n",
    "    # Start Cleanlab Studio\n",
    "    studio = Studio(api_key)\n",
    "\n",
    "    # Get Cleanlab Studio results\n",
    "    cleanlab_cols = studio.download_cleanlab_columns(cleanset_id)\n",
    "\n",
    "    # Create an issue df reporting results for NER using Cleanlab\n",
    "    ner_df = read_ner_data(dataset_path, format=data_format)\n",
    "    issue_df = get_issue_df(ner_df, cleanlab_cols)\n",
    "\n",
    "    # Add original columns for readability\n",
    "    issue_df = ner_df[[\"id\", \"tokens\", \"tags\"]].merge(\n",
    "        issue_df, how=\"right\", left_index=True, right_index=True\n",
    "    )\n",
    "    issue_df[\"tags\"] = issue_df[\"tags\"].apply(literal_eval)\n",
    "\n",
    "    return issue_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa7216f-ce39-4a15-b039-355c1610b56c",
   "metadata": {},
   "source": [
    "We want to review the examples most likely annotated with an incorrect set of named entity tags. To do so, we sort the `issue_df` DataFrame by `label_issue_score` (descending). The top resulting examples (with the highest label issue scores) are the ones Cleanlab estimates are most likely mislabeled in the original dataset.\n",
    "\n",
    "For your own dataset, you should review these examples and consider correcting their labels. You can also see which data suffers from other types of issues by sorting `issue_df` based on other issue scores (such as the `ambiguous_score` to see the most confusing examples). Each issue score indicates the severity of the issue, where higher values indicate problems worthy of your attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7373434c-0445-4b84-9e29-7e8ee6a44772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "      <th>cleanlab_row_ID</th>\n",
       "      <th>issue_details</th>\n",
       "      <th>is_label_issue</th>\n",
       "      <th>label_issue_score</th>\n",
       "      <th>is_well_labeled</th>\n",
       "      <th>is_near_duplicate</th>\n",
       "      <th>near_duplicate_score</th>\n",
       "      <th>near_duplicate_cluster_id</th>\n",
       "      <th>is_ambiguous</th>\n",
       "      <th>ambiguous_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9072</th>\n",
       "      <td>9072</td>\n",
       "      <td>Sao Paulo 1996-08-27</td>\n",
       "      <td>[has_person, has_person, has_other]</td>\n",
       "      <td>9073</td>\n",
       "      <td>Possible named entitiy tags that are missing: has_location. Possible named entities tags that are incorrectly present: has_person.</td>\n",
       "      <td>True</td>\n",
       "      <td>0.959718</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>859</td>\n",
       "      <td>False</td>\n",
       "      <td>0.558464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13237</th>\n",
       "      <td>13237</td>\n",
       "      <td>Denver 1996-08-29</td>\n",
       "      <td>[has_person, has_other]</td>\n",
       "      <td>13238</td>\n",
       "      <td>Possible named entitiy tags that are missing: has_location. Possible named entities tags that are incorrectly present: has_person.</td>\n",
       "      <td>True</td>\n",
       "      <td>0.958938</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.876708</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>False</td>\n",
       "      <td>0.563274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15769</th>\n",
       "      <td>15769</td>\n",
       "      <td>pakistan</td>\n",
       "      <td>[has_other]</td>\n",
       "      <td>15770</td>\n",
       "      <td>Possible named entitiy tags that are missing: has_location. Possible named entities tags that are incorrectly present: has_other.</td>\n",
       "      <td>True</td>\n",
       "      <td>0.958618</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>830</td>\n",
       "      <td>False</td>\n",
       "      <td>0.726484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18727</th>\n",
       "      <td>18727</td>\n",
       "      <td>Santiago 1996-12-05</td>\n",
       "      <td>[has_person, has_other]</td>\n",
       "      <td>18728</td>\n",
       "      <td>Possible named entitiy tags that are missing: has_location. Possible named entities tags that are incorrectly present: has_person.</td>\n",
       "      <td>True</td>\n",
       "      <td>0.955143</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.442889</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>False</td>\n",
       "      <td>0.483547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9112</th>\n",
       "      <td>9112</td>\n",
       "      <td>Sao Paulo 1996-08-27</td>\n",
       "      <td>[has_person, has_person, has_other]</td>\n",
       "      <td>9113</td>\n",
       "      <td>Possible named entitiy tags that are missing: has_location. Possible named entities tags that are incorrectly present: has_person.</td>\n",
       "      <td>True</td>\n",
       "      <td>0.954981</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>859</td>\n",
       "      <td>False</td>\n",
       "      <td>0.557956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                tokens                                 tags  \\\n",
       "9072    9072  Sao Paulo 1996-08-27  [has_person, has_person, has_other]   \n",
       "13237  13237     Denver 1996-08-29              [has_person, has_other]   \n",
       "15769  15769              pakistan                          [has_other]   \n",
       "18727  18727   Santiago 1996-12-05              [has_person, has_other]   \n",
       "9112    9112  Sao Paulo 1996-08-27  [has_person, has_person, has_other]   \n",
       "\n",
       "       cleanlab_row_ID  \\\n",
       "9072              9073   \n",
       "13237            13238   \n",
       "15769            15770   \n",
       "18727            18728   \n",
       "9112              9113   \n",
       "\n",
       "                                                                                                                             issue_details  \\\n",
       "9072   Possible named entitiy tags that are missing: has_location. Possible named entities tags that are incorrectly present: has_person.    \n",
       "13237  Possible named entitiy tags that are missing: has_location. Possible named entities tags that are incorrectly present: has_person.    \n",
       "15769   Possible named entitiy tags that are missing: has_location. Possible named entities tags that are incorrectly present: has_other.    \n",
       "18727  Possible named entitiy tags that are missing: has_location. Possible named entities tags that are incorrectly present: has_person.    \n",
       "9112   Possible named entitiy tags that are missing: has_location. Possible named entities tags that are incorrectly present: has_person.    \n",
       "\n",
       "       is_label_issue  label_issue_score  is_well_labeled  is_near_duplicate  \\\n",
       "9072             True           0.959718            False               True   \n",
       "13237            True           0.958938            False              False   \n",
       "15769            True           0.958618            False               True   \n",
       "18727            True           0.955143            False              False   \n",
       "9112             True           0.954981            False               True   \n",
       "\n",
       "       near_duplicate_score  near_duplicate_cluster_id  is_ambiguous  \\\n",
       "9072               1.000000                        859         False   \n",
       "13237              0.876708                       <NA>         False   \n",
       "15769              0.999996                        830         False   \n",
       "18727              0.442889                       <NA>         False   \n",
       "9112               1.000000                        859         False   \n",
       "\n",
       "       ambiguous_score  \n",
       "9072          0.558464  \n",
       "13237         0.563274  \n",
       "15769         0.726484  \n",
       "18727         0.483547  \n",
       "9112          0.557956  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLEANSET_ID = \"<insert your cleanset_id>\"  # We used the ID of the project trained above\n",
    "\n",
    "issue_df = get_ner_results(\n",
    "    API_KEY, cleanset_id, dataset_path, \"conll2003_like\"\n",
    ")  # Alternatively, pass in CLEANSET_ID\n",
    "issue_df = issue_df.sort_values(by=[\"label_issue_score\"], ascending=False)\n",
    "issue_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6d96e3-0fa1-4190-ac37-282b5a46f0e9",
   "metadata": {},
   "source": [
    "Lets take a closer look at the most likely label issue, example `id=18378` below. We can see the original tokens are \"Prince Rupert 1 3\" while their original tags are `[has_location, has_location, has_other, has_other]`. We know this example is an issue since the `is_label_issue` boolean is true and the example's `label_issue_score` is close to 1. \n",
    "\n",
    "Column `issue_details` will tell us more details on which tags Cleanlab Studio believes contribute to the example being marked a label issue. Here Cleanlab Studio suggests that possible named entity tags that are incorrectly present in the tokens are `has_location` while tags that are  missing from the tokens are `has_organization` and `has_person`. Tokens \"Prince Rupert\" pretty clearly represent a person however they were originally marked as `has_location`. With the help of `issue_details` we can confirm that `has_location` is incorrectly present in the tags. Instead, it should be replaced with one of the missing tokens: `has_person`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfc7f4c8-03a0-4853-b436-ddf005338d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "      <th>cleanlab_row_ID</th>\n",
       "      <th>issue_details</th>\n",
       "      <th>is_label_issue</th>\n",
       "      <th>label_issue_score</th>\n",
       "      <th>is_well_labeled</th>\n",
       "      <th>is_near_duplicate</th>\n",
       "      <th>near_duplicate_score</th>\n",
       "      <th>near_duplicate_cluster_id</th>\n",
       "      <th>is_ambiguous</th>\n",
       "      <th>ambiguous_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9072</th>\n",
       "      <td>9072</td>\n",
       "      <td>Sao Paulo 1996-08-27</td>\n",
       "      <td>[has_person, has_person, has_other]</td>\n",
       "      <td>9073</td>\n",
       "      <td>Possible named entitiy tags that are missing: has_location. Possible named entities tags that are incorrectly present: has_person.</td>\n",
       "      <td>True</td>\n",
       "      <td>0.959718</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>859</td>\n",
       "      <td>False</td>\n",
       "      <td>0.558464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                tokens                                 tags  \\\n",
       "9072  9072  Sao Paulo 1996-08-27  [has_person, has_person, has_other]   \n",
       "\n",
       "      cleanlab_row_ID  \\\n",
       "9072             9073   \n",
       "\n",
       "                                                                                                                            issue_details  \\\n",
       "9072  Possible named entitiy tags that are missing: has_location. Possible named entities tags that are incorrectly present: has_person.    \n",
       "\n",
       "      is_label_issue  label_issue_score  is_well_labeled  is_near_duplicate  \\\n",
       "9072            True           0.959718            False               True   \n",
       "\n",
       "      near_duplicate_score  near_duplicate_cluster_id  is_ambiguous  \\\n",
       "9072                   1.0                        859         False   \n",
       "\n",
       "      ambiguous_score  \n",
       "9072         0.558464  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issue_df.iloc[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6374ad37-3bef-4f73-b186-2a07c712b4d4",
   "metadata": {},
   "source": [
    "### Learn more about the dataset\n",
    "\n",
    "In addition to using Cleanlab Studio to detect label errors, we can also identify examples that are: *well labeled* or *ambiguous*.\n",
    "\n",
    "This information provides additional insights about our data annotations.\n",
    "\n",
    "Examples marked as **well labeled** are accurately annotated (with great confidence according to Cleanlab's AI) and do not require extra review from human annotators. These instances are indicated by `True` in the `is_well_labeled` column. Given the prevalence of label issues in this dataset, auto-marking examples as \"well labeled\" is exercised with caution, resulting in no instances here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca59c2c1-bf3c-484e-a3fa-93967871540f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of well labeled examples in dataset:  0\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Number of well labeled examples in dataset: \",\n",
    "    issue_df[issue_df[\"is_well_labeled\"] == True].shape[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eaaee1df-03bf-4e15-9dcd-c956dff78d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "      <th>cleanlab_row_ID</th>\n",
       "      <th>issue_details</th>\n",
       "      <th>is_label_issue</th>\n",
       "      <th>label_issue_score</th>\n",
       "      <th>is_well_labeled</th>\n",
       "      <th>is_near_duplicate</th>\n",
       "      <th>near_duplicate_score</th>\n",
       "      <th>near_duplicate_cluster_id</th>\n",
       "      <th>is_ambiguous</th>\n",
       "      <th>ambiguous_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, tokens, tags, cleanlab_row_ID, issue_details, is_label_issue, label_issue_score, is_well_labeled, is_near_duplicate, near_duplicate_score, near_duplicate_cluster_id, is_ambiguous, ambiguous_score]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issue_df[issue_df[\"is_well_labeled\"] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab9f928-ad66-428b-bb8b-130af7473238",
   "metadata": {},
   "source": [
    "Cleanlab Studio can also auto-detect which examples are **ambiguous**. These are examples that seemingly *may or may not* contain the given tags, it will be hard for your data annotators to decide correctly without precise instructions on how to handle such cases (different annotators may disagree). In this dataset, there are 68 examples judged to be ambiguous. Looking at one of the examples with a high ambiguity score, we can guess that the token \"Karlsruhe\" is a city in Germany but could also represent a team name or an individual player. Without additional context, annotators may struggle between choosing `has_organization`, `has_location` and `has_person`.\n",
    "\n",
    "Despite being ambiguous, this example is not considered a label issue. This is a common as ambiguous examples may be correctly labeled but still confusing and worthy of note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b60ee17c-be12-4b35-87de-4268f2375ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ambiguous examples in dataset:  68\n"
     ]
    }
   ],
   "source": [
    "ambiguous_df = issue_df[issue_df[\"is_ambiguous\"] == True]\n",
    "print(\"Number of ambiguous examples in dataset: \", ambiguous_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd885bb9-89ca-4bbd-95b2-7a8706ef343d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "      <th>cleanlab_row_ID</th>\n",
       "      <th>issue_details</th>\n",
       "      <th>is_label_issue</th>\n",
       "      <th>label_issue_score</th>\n",
       "      <th>is_well_labeled</th>\n",
       "      <th>is_near_duplicate</th>\n",
       "      <th>near_duplicate_score</th>\n",
       "      <th>near_duplicate_cluster_id</th>\n",
       "      <th>is_ambiguous</th>\n",
       "      <th>ambiguous_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14586</th>\n",
       "      <td>14586</td>\n",
       "      <td>Karlsruhe won the August 20 match 3-1 thanks to two late goals.</td>\n",
       "      <td>[has_organization, has_other, has_other, has_other, has_other, has_other, has_other, has_other, has_other, has_other, has_other, has_other, has_other]</td>\n",
       "      <td>14587</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.446649</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1052</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>True</td>\n",
       "      <td>0.999984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                                           tokens  \\\n",
       "14586  14586  Karlsruhe won the August 20 match 3-1 thanks to two late goals.   \n",
       "\n",
       "                                                                                                                                                         tags  \\\n",
       "14586  [has_organization, has_other, has_other, has_other, has_other, has_other, has_other, has_other, has_other, has_other, has_other, has_other, has_other]   \n",
       "\n",
       "       cleanlab_row_ID issue_details  is_label_issue  label_issue_score  \\\n",
       "14586            14587           NaN           False           0.446649   \n",
       "\n",
       "       is_well_labeled  is_near_duplicate  near_duplicate_score  \\\n",
       "14586            False              False                0.1052   \n",
       "\n",
       "       near_duplicate_cluster_id  is_ambiguous  ambiguous_score  \n",
       "14586                       <NA>          True         0.999984  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort by ambiguous score (most ambiguous first)\n",
    "ambiguous_df = ambiguous_df.sort_values(by=\"ambiguous_score\", ascending=False)\n",
    "ambiguous_df.iloc[[10]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "00885e89789f58e60dbba52a405dc834aaf92411914fde0d391f9b48289a0610"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}