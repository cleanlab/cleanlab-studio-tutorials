{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Curation with Train vs Test Splits\n",
    "\n",
    "<head>\n",
    "  <meta name=\"title\" content=\"Data Curation with Train vs Test Splits\"/>\n",
    "  <meta property=\"og:title\" content=\"Data Curation with Train vs Test Splits\"/>\n",
    "  <meta name=\"twitter:title\" content=\"Data Curation with Train vs Test Splits\"/>\n",
    "  <meta name=\"image\" content=\"/img/mlimprovement.png\" />\n",
    "  <meta property=\"og:image\" content=\"/img/mlimprovement.png\" />\n",
    "  <meta name=\"description\" content=\"How to improve machine learning by improving data quality to ensure reliable model evaluation and training\"  />\n",
    "  <meta property=\"og:description\" content=\"How to improve machine learning by improving data quality to ensure reliable model evaluation and training\" />\n",
    "  <meta name=\"twitter:description\" content=\"How to improve machine learning by improving data quality to ensure reliable model evaluation and training\" />\n",
    "</head>\n",
    "\n",
    "In typical Machine Learning projects, we split our dataset into training data for fitting models and test data to evaluate model performance. For noisy real-world datasets, detecting/correcting errors in the training data is important to train robust models, but it's less recognized that the test set can also be noisy.\n",
    "For accurate model evaluation, it is vital to find and fix issues in the test data as well. Some evaluation metrics are particularly sensitive to outliers and noisy labels.\n",
    "This tutorial demonstrates a way to use [Cleanlab Studio](https://app.cleanlab.ai/) to curate cleaner versions of your training and test data, ensuring **robust** model training and **reliable** performance evaluation.\n",
    "\n",
    "![Model deployment using Cleanlab Studio](./assets/train-test-tutorial/mlimprovement.png)\n",
    "\n",
    "Here's how we recommend handling noisy training and test data with Cleanlab Studio:\n",
    "\n",
    "- First focus on detecting issues in the test data. For the best detection, we recommend that you merge your training and test data and then run a Cleanlab Studio Project (which will benefit from more data) -- but only focus on project results for the test data.\n",
    "- Manually review/correct Cleanlab-detected issues in your test data. To avoid bias, we caution against automated correction of test data. Instead, test data changes should be individually verified to ensure they will lead to more accurate model evaluation.\n",
    "- Run a separate Cleanlab Studio project on the training data alone to detect issues in the training data (without any test set information leakage).\n",
    "- Optionally, use automated Cleanlab suggestions to algorithmically refine the training data (or manually review/correct Cleanlab-detected issues in your training data).\n",
    "- Estimate the final model's performance on the cleaned test data. **Do not compare the performance of different ML models estimated across different versions of your test data.** These estimates are incomparable.\n",
    "\n",
    "Even if you effectively clean noisy training data, the resulting model may not appear to be better, unless you properly clean the noisy test data as well. Thus we recommend curating clean test data to first establish reliable model evaluation, before curating the training data for improved model training. Automated data correction of test data may bias model evaluation, so we recommend that you manually correct test data issues reported by Cleanlab. You can rely on automated correction of training data once you have trustworthy model evaluation in place.\n",
    "\n",
    "Consider this tutorial as a blueprint for using Cleanlab Studio in ML projects spanning various data modalities and tasks.\n",
    "Let\u2019s get started!\n",
    "\n",
    "## Load the data\n",
    "\n",
    "First install and import required dependencies for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates our strategy on a tabular dataset of student grades which has some data entry errors. The same idea can be applied to any dataset, whether structured or unstructured (images/text). Our student grades dataset records three exam scores (numerical features), a written note (a categorical feature with missing values), and a potentially erroneous letter grade (categorical label to predict) for each student. Let's load the [training data](https://cleanlab-public.s3.amazonaws.com/Datasets/student-grades/train.csv) for fitting our ML model and [test data](https://cleanlab-public.s3.amazonaws.com/Datasets/student-grades/test.csv) for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stud_ID</th>\n",
       "      <th>exam_1</th>\n",
       "      <th>exam_2</th>\n",
       "      <th>exam_3</th>\n",
       "      <th>notes</th>\n",
       "      <th>noisy_letter_grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37fd76</td>\n",
       "      <td>99</td>\n",
       "      <td>59</td>\n",
       "      <td>70</td>\n",
       "      <td>missed class frequently -10</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>018bff</td>\n",
       "      <td>94</td>\n",
       "      <td>41</td>\n",
       "      <td>91</td>\n",
       "      <td>great participation +10</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b3c9a0</td>\n",
       "      <td>91</td>\n",
       "      <td>74</td>\n",
       "      <td>88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>076d92</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>65</td>\n",
       "      <td>cheated on exam, gets 0pts</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68827d</td>\n",
       "      <td>91</td>\n",
       "      <td>98</td>\n",
       "      <td>75</td>\n",
       "      <td>missed class frequently -10</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  stud_ID  exam_1  exam_2  exam_3                        notes  \\\n",
       "0  37fd76      99      59      70  missed class frequently -10   \n",
       "1  018bff      94      41      91      great participation +10   \n",
       "2  b3c9a0      91      74      88                          NaN   \n",
       "3  076d92       0      79      65   cheated on exam, gets 0pts   \n",
       "4  68827d      91      98      75  missed class frequently -10   \n",
       "\n",
       "  noisy_letter_grade  \n",
       "0                  D  \n",
       "1                  B  \n",
       "2                  B  \n",
       "3                  F  \n",
       "4                  C  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\n",
    "    \"https://cleanlab-public.s3.amazonaws.com/Datasets/student-grades/train.csv\"\n",
    ")\n",
    "df_test = pd.read_csv(\n",
    "    \"https://cleanlab-public.s3.amazonaws.com/Datasets/student-grades/test.csv\"\n",
    ")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curate clean test data\n",
    "\n",
    "We first demonstrate the process of improving the quality of noisy test data to facilitate more accurate model evaluation.\n",
    "\n",
    "### Merge train and test data \n",
    "\n",
    "We merge our training and test data into a larger dataset that will better enable Cleanlab Studio to detect issues (remember Cleanlab's AI is trained on the provided data and performs better with more data). In the merged dataset, we add a column *is_train* to distinguish between samples from the training vs test set. We'll use this column to focus on reviewing issues detected in the test data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stud_ID</th>\n",
       "      <th>exam_1</th>\n",
       "      <th>exam_2</th>\n",
       "      <th>exam_3</th>\n",
       "      <th>notes</th>\n",
       "      <th>noisy_letter_grade</th>\n",
       "      <th>is_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37fd76</td>\n",
       "      <td>99</td>\n",
       "      <td>59</td>\n",
       "      <td>70</td>\n",
       "      <td>missed class frequently -10</td>\n",
       "      <td>D</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>018bff</td>\n",
       "      <td>94</td>\n",
       "      <td>41</td>\n",
       "      <td>91</td>\n",
       "      <td>great participation +10</td>\n",
       "      <td>B</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b3c9a0</td>\n",
       "      <td>91</td>\n",
       "      <td>74</td>\n",
       "      <td>88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>076d92</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>65</td>\n",
       "      <td>cheated on exam, gets 0pts</td>\n",
       "      <td>F</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68827d</td>\n",
       "      <td>91</td>\n",
       "      <td>98</td>\n",
       "      <td>75</td>\n",
       "      <td>missed class frequently -10</td>\n",
       "      <td>C</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  stud_ID  exam_1  exam_2  exam_3                        notes  \\\n",
       "0  37fd76      99      59      70  missed class frequently -10   \n",
       "1  018bff      94      41      91      great participation +10   \n",
       "2  b3c9a0      91      74      88                          NaN   \n",
       "3  076d92       0      79      65   cheated on exam, gets 0pts   \n",
       "4  68827d      91      98      75  missed class frequently -10   \n",
       "\n",
       "  noisy_letter_grade  is_train  \n",
       "0                  D      True  \n",
       "1                  B      True  \n",
       "2                  B      True  \n",
       "3                  F      True  \n",
       "4                  C      True  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_c = df_train.copy()\n",
    "df_test_c = df_test.copy()\n",
    "df_train_c[\"is_train\"] = True\n",
    "df_test_c[\"is_train\"] = False\n",
    "df_full = pd.concat([df_train_c, df_test_c]).reset_index(drop=True)\n",
    "df_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the merged dataset as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.to_csv(\"full.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Project\n",
    "\n",
    "Let's launch a project in Cleanlab Studio to auto-detect issues in this dataset. Below is a video demonstrating how to load your dataset and run a project to analyze it. When loading the data, you can optionally override the inferred column types for your dataset (for our tutorial dataset, we specify *stud_ID* as an identifier column and the exam columns as numeric). After loading the data, we create a Cleanlab Studio Project. During project setup, we specify the label column as *noisy_letter_grade*, and *exam_1*, *exam_2*, *exam_3*, and *notes* as predictive columns used by Cleanlab's AI to estimate label issues. Note we ignore the ID column and the `is_train` boolean that we added.\n",
    "\n",
    "\n",
    "<Video\n",
    "  width=\"1792\"\n",
    "  height=\"1010\"\n",
    "  src=\"./assets/train-test-tutorial/full_dataset_upload.mp4\"\n",
    "  autoPlay={false}\n",
    "  loop={false}\n",
    "  muted={true}\n",
    "/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually review and address issues in test data\n",
    "\n",
    "The Project will take a while for Cleanlab's AI to train on your data and analyze it. When finished running, it will be marked Ready for Review and you'll get an email. In the project, you can quickly review the issues detected in your data. We focus solely on the test data by setting the filter `is_train=True` and then manually review individual test data examples.\n",
    "\n",
    "It is important *not* to auto-correct training data at this point in order to prevent test set information leakage. Beware that automated curation of the test data can introduce unforeseen biases in model evaluation. Thus, we recommend carefully reviewing the test data detected to have issues and only making corrections in cases where you are confident the changed test data will provide more reliable model evaluation. For instance, do not re-label a test data point unless you are certain the new label is right and do not exclude test data unless you are certain it is unrepresentative of the setting in which the model will be deployed.\n",
    "\n",
    "The video below demonstrates how this manual data correction process might be conducted for our tutorial dataset. During this process, we re-label certain test data points that Cleanlab Studio correctly identified as mislabeled. To expedite this data correction process, utilize the Analytics tab to review similar types of mislabeled data points. For example, all of the students annotated as *F*'s which Cleanlab Studio estimates should have been annotated as *A*'s instead. After addressing Label Issues detected by Cleanlab, you might also review examples flagged as Ambiguous or Outliers. Once we have addressed the detected issues in our test data, we export the cleaned dataset with Cleanlab-generated metadata columns back into CSV format.\n",
    "\n",
    "<Video\n",
    "  width=\"1792\"\n",
    "  height=\"1010\"\n",
    "  src=\"./assets/train-test-tutorial/clean_test_export.mp4\"\n",
    "  autoPlay={false}\n",
    "  loop={false}\n",
    "  muted={true}\n",
    "/>\n",
    "\n",
    "### Construct a clean version of the test data\n",
    "To construct our cleaned test dataset, we filter out the train rows and only keep the relevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should generally create your own cleanlab_columns_full.csv file by curating your dataset in Cleanlab Studio and exporting it.\n",
    "# For this tutorial, you can fetch the export file that we used here: https://cleanlab-public.s3.amazonaws.com/Datasets/student-grades/cleanlab_columns_full.csv\n",
    "cleanlab_columns = pd.read_csv(\"cleanlab_columns_full.csv\")\n",
    "df_test_cleaned = cleanlab_columns[cleanlab_columns[\"is_train\"] == False].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cleanlab_corrected_label` column contains corrected labels for rows that were mislabeled and corrected in the Cleanlab Studio interface. For rows without any issues, we backfill this column with the original labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_cleaned[\"cleanlab_corrected_label\"] = df_test_cleaned[\n",
    "    \"cleanlab_corrected_label\"\n",
    "].fillna(df_test_cleaned[\"noisy_letter_grade\"])\n",
    "df_test_cleaned = df_test_cleaned[\n",
    "    [\n",
    "        \"stud_ID\",\n",
    "        \"exam_1\",\n",
    "        \"exam_2\",\n",
    "        \"exam_3\",\n",
    "        \"notes\",\n",
    "        \"cleanlab_corrected_label\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our cleaned test dataset is shown below. By correcting badly annotated data, we can more reliably evaluate any ML models on this test set (and determine important decisions like when to deploy an updated model). Clean test labels are particularly important for sensitive evaluation metrics like precision/recall with imbalanced class frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stud_ID</th>\n",
       "      <th>exam_1</th>\n",
       "      <th>exam_2</th>\n",
       "      <th>exam_3</th>\n",
       "      <th>notes</th>\n",
       "      <th>cleanlab_corrected_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0094d7</td>\n",
       "      <td>94</td>\n",
       "      <td>91</td>\n",
       "      <td>93</td>\n",
       "      <td>great participation +10</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>01dd6e</td>\n",
       "      <td>92</td>\n",
       "      <td>99</td>\n",
       "      <td>87</td>\n",
       "      <td>missed homework frequently -10</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>02c7df</td>\n",
       "      <td>90</td>\n",
       "      <td>73</td>\n",
       "      <td>91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>03d6b7</td>\n",
       "      <td>93</td>\n",
       "      <td>47</td>\n",
       "      <td>79</td>\n",
       "      <td>great participation +10</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>042686</td>\n",
       "      <td>87</td>\n",
       "      <td>74</td>\n",
       "      <td>95</td>\n",
       "      <td>missed class frequently -10</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stud_ID  exam_1  exam_2  exam_3                           notes  \\\n",
       "1   0094d7      94      91      93         great participation +10   \n",
       "9   01dd6e      92      99      87  missed homework frequently -10   \n",
       "12  02c7df      90      73      91                             NaN   \n",
       "14  03d6b7      93      47      79         great participation +10   \n",
       "15  042686      87      74      95     missed class frequently -10   \n",
       "\n",
       "   cleanlab_corrected_label  \n",
       "1                         A  \n",
       "9                         B  \n",
       "12                        B  \n",
       "14                        B  \n",
       "15                        C  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model performance on clean test data\n",
    "\n",
    "The data curation principles demonstrated here can be applied to enhance model evaluation and training, **irrespective of the specific type of ML model you're using**. Here we choose an XGBoost model, an popular implementation of gradient-boosting decision trees (GBDT) for tabular data. XGBoost (>v1.6) is able to handle mixed data types (numerical and categorical) by setting `enable_categorical=True`, thereby simplifying the modeling process.\n",
    "\n",
    "## Preprocess the dataset \n",
    "\n",
    "Before training an XGBoost model, we preprocess the *notes* and *noisy_letter_grade* columns into categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label encoders for the grade and notes columns\n",
    "grade_le = preprocessing.LabelEncoder()\n",
    "notes_le = preprocessing.LabelEncoder()\n",
    "\n",
    "# Prepare the feature columns\n",
    "train_features = df_train.drop([\"stud_ID\", \"noisy_letter_grade\"], axis=1).copy()\n",
    "train_features[\"notes\"] = notes_le.fit_transform(train_features[\"notes\"])\n",
    "train_features[\"notes\"] = train_features[\"notes\"].astype(\"category\")\n",
    "\n",
    "# Encode the label column into a cateogorical feature\n",
    "train_labels = grade_le.fit_transform(df_train[\"noisy_letter_grade\"].copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the training set features after preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exam_1</th>\n",
       "      <th>exam_2</th>\n",
       "      <th>exam_3</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99</td>\n",
       "      <td>59</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94</td>\n",
       "      <td>41</td>\n",
       "      <td>91</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91</td>\n",
       "      <td>74</td>\n",
       "      <td>88</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>91</td>\n",
       "      <td>98</td>\n",
       "      <td>75</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   exam_1  exam_2  exam_3 notes\n",
       "0      99      59      70     3\n",
       "1      94      41      91     2\n",
       "2      91      74      88     5\n",
       "3       0      79      65     0\n",
       "4      91      98      75     3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we repeat the same preprocessing steps for our clean test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = df_test_cleaned.drop(\n",
    "    [\"stud_ID\", \"cleanlab_corrected_label\"], axis=1\n",
    ").copy()\n",
    "test_features[\"notes\"] = notes_le.transform(test_features[\"notes\"])\n",
    "test_features[\"notes\"] = test_features[\"notes\"].astype(\"category\")\n",
    "\n",
    "test_labels = grade_le.transform(df_test_cleaned[\"cleanlab_corrected_label\"].copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model with original (noisy) training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(tree_method=\"hist\", enable_categorical=True)\n",
    "model.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using clean test data to evaluate the performance of model trained on noisy training data\n",
    "\n",
    "Although curating clean test data does not directly help train a better ML model, more reliable model evaluation can improve our overall ML project. For instance, clean test data can enable better informed decisions regarding when to deploy a model and better model/hyperparameter selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model fit to noisy training data, measured on clean test data: 79.2%\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(test_features)\n",
    "acc_original = accuracy_score(test_labels, preds)\n",
    "print(\n",
    "    f\"Accuracy of model fit to noisy training data, measured on clean test data: {round(acc_original*100,1)}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curate clean training data\n",
    "\n",
    "To actually improve our model, we'll want to improve the quality of our noisy training data. We follow the same steps as before, but this time, we **only use the train data** to detect issues. This prevents information from the test data from potentially influencing the training data (and subsequently trained ML models), which is critical for unbiased model evaluation.\n",
    "\n",
    "Here we simply load the `train.csv` file into Cleanlab Studio and create a project as demonstrated previously. For the training data, feel free to rely on Cleanlab's automated suggestions to more efficiently fix issues in the dataset. After curating clean training data, we export it in the same way as before.\n",
    "\n",
    "The video below demonstrates how this process might look for our tutorial training data. Here Cleanlab Studio has identified a large number of mislabeled examples, which can be time-consuming to address individually. To expedite the data correction process for our training set, we use the `Analytics` tab to pinpoint subsets of the data suitable for automated correction. In this specific video, we automatically correct all data points for which Cleanlab Studio estimated a grade F was mislabeled as a grade A. Subsequently, we individually examine examples flagged as Ambiguous. We exclude some particularly confusing examples\u00a0that might hamper our model's learning. Excluding and relabeling data can be done more freely/automatically in the training data, because as long as we have reliable model evaluation in place, we will see when we have poorly altered the training data and are producing worse models. Cleanlab's automated suggestions can be useful for correcting many data points at once in large training datasets.\n",
    "\n",
    "<Video\n",
    "  width=\"1792\"\n",
    "  height=\"1010\"\n",
    "  src=\"./assets/train-test-tutorial/clean_train_export.mp4\"\n",
    "  autoPlay={false}\n",
    "  loop={false}\n",
    "  muted={true}\n",
    "/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should generally create your own cleanlab_columns_full.csv file by curating your dataset in Cleanlab Studio and exporting it.\n",
    "# For this tutorial, you can fetch the export file that we used here: https://cleanlab-public.s3.amazonaws.com/Datasets/student-grades/cleanlab_columns.csv\n",
    "df_train_cleaned = pd.read_csv(\"cleanlab_columns.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct our clean training set, we again backfill the `cleanlab_corrected_label` column with the original labels for training rows where no issues were detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cleaned[\"cleanlab_corrected_label\"] = df_train_cleaned[\n",
    "    \"cleanlab_corrected_label\"\n",
    "].fillna(df_train_cleaned[\"noisy_letter_grade\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cleaned = df_train_cleaned[\n",
    "    [\n",
    "        \"stud_ID\",\n",
    "        \"exam_1\",\n",
    "        \"exam_2\",\n",
    "        \"exam_3\",\n",
    "        \"notes\",\n",
    "        \"cleanlab_corrected_label\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stud_ID</th>\n",
       "      <th>exam_1</th>\n",
       "      <th>exam_2</th>\n",
       "      <th>exam_3</th>\n",
       "      <th>notes</th>\n",
       "      <th>cleanlab_corrected_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5686d8</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>cheated on exam, gets 0pts</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6fd557</td>\n",
       "      <td>97</td>\n",
       "      <td>98</td>\n",
       "      <td>92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>96968e</td>\n",
       "      <td>90</td>\n",
       "      <td>84</td>\n",
       "      <td>97</td>\n",
       "      <td>great participation +10</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ccd8c8</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>cheated on exam, gets 0pts</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50bebd</td>\n",
       "      <td>54</td>\n",
       "      <td>80</td>\n",
       "      <td>81</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  stud_ID  exam_1  exam_2  exam_3                       notes  \\\n",
       "0  5686d8      65       0      57  cheated on exam, gets 0pts   \n",
       "1  6fd557      97      98      92                         NaN   \n",
       "2  96968e      90      84      97     great participation +10   \n",
       "3  ccd8c8      53       0      76  cheated on exam, gets 0pts   \n",
       "4  50bebd      54      80      81                         NaN   \n",
       "\n",
       "  cleanlab_corrected_label  \n",
       "0                        F  \n",
       "1                        A  \n",
       "2                        A  \n",
       "3                        F  \n",
       "4                        A  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the clean training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_train_features = df_train_cleaned.drop(\n",
    "    [\"stud_ID\", \"cleanlab_corrected_label\"], axis=1\n",
    ").copy()\n",
    "cleaned_train_features[\"notes\"] = notes_le.transform(cleaned_train_features[\"notes\"])\n",
    "cleaned_train_features[\"notes\"] = cleaned_train_features[\"notes\"].astype(\"category\")\n",
    "\n",
    "cleaned_train_labels = grade_le.transform(\n",
    "    df_train_cleaned[\"cleanlab_corrected_label\"].copy()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model with clean training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_model = XGBClassifier(tree_method=\"hist\", enable_categorical=True)\n",
    "cleaned_model.fit(cleaned_train_features, cleaned_train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using clean test data to evaluate the performance of model trained on the clean training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model fit to clean training data, measured on clean test data: 91.5%\n"
     ]
    }
   ],
   "source": [
    "preds = cleaned_model.predict(test_features)\n",
    "acc_original = accuracy_score(test_labels, preds)\n",
    "print(\n",
    "    f\"Accuracy of model fit to clean training data, measured on clean test data: {round(acc_original*100,1)}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for our tutorial dataset, finding and fixing issues in the training data significantly improved ML model performance. The automated corrections suggested by Cleanlab Studio have allowed us to easily improve our ML model. Furthermore, such training data corrections should directly improve various different types of ML models in case we decide to switch the model later on.\n",
    "\n",
    "**Note: We are reliably evaluating performance here based on clean test data.** If you only make training data improvements but have a noisy test set, you may fail to realize when you have actually improved your model. In fact, only correcting the training data may make model performance appear go down on the original test data in datasets with systematic errors (even if the training data corrections were actually beneficial, they may introduce a distribution shift in such cases).\n",
    "\n",
    "## Review performance on noisy test data\n",
    "\n",
    "For completeness, let's see what the above model performances would be estimated as if we had used original (noisy) test data instead of our cleaned test data.\n",
    "\n",
    "We first preprocess the features of the original test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_test_features = df_test.drop(\n",
    "    [\n",
    "        \"stud_ID\",\n",
    "        \"noisy_letter_grade\",\n",
    "    ],\n",
    "    axis=1,\n",
    ").copy()\n",
    "noisy_test_features[\"notes\"] = notes_le.transform(noisy_test_features[\"notes\"])\n",
    "noisy_test_features[\"notes\"] = noisy_test_features[\"notes\"].astype(\"category\")\n",
    "\n",
    "noisy_test_labels = grade_le.transform(df_test[\"noisy_letter_grade\"].copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using noisy test data to evaluate the performance of model trained on noisy training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model fit to noisy training data, measured on noisy test data: 69.1%\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(noisy_test_features)\n",
    "acc_original = accuracy_score(noisy_test_labels, preds)\n",
    "print(\n",
    "    f\"Accuracy of model fit to noisy training data, measured on noisy test data: {round(acc_original*100,1)}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using noisy test data to evaluate the performance of model trained on clean training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model fit to clean training data, measured on noisy test data: 75.0%\n"
     ]
    }
   ],
   "source": [
    "preds = cleaned_model.predict(noisy_test_features)\n",
    "acc_original = accuracy_score(noisy_test_labels, preds)\n",
    "print(\n",
    "    f\"Accuracy of model fit to clean training data, measured on noisy test data: {round(acc_original*100,1)}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's evident how curating clean test data can significantly affect model performance evaluations. Relying on noisy test data can be misleading and lead to bad decisions.\n",
    "\n",
    "We again emphasize that **even if you effectively clean noisy training data, the resulting model may not appear to be better, unless you properly clean the noisy test data as well.** This tutorial demonstrated a strategy to curate clean test data for reliable model evaluation as well as clean training data for improved model training. Remember that **automated data correction of test data may bias model evaluation**, so manually correct test data issues reported by Cleanlab. You can rely on automated correction of training data once you have trustworthy model evaluation in place, since then you can reliably measure the resulting effect on models fit to the altered training data.\n",
    "\n",
    "If you have any questions, feel free to ask in our [Slack community](https://cleanlab.ai/slack)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}