{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring the quality of synthetic data\n",
    "\n",
    "This tutorial demonstrates how to format a dataset that has both real and synthetic examples so that Cleanlab Studio can analyze it. You will learn how to use simple functions that automatically assess the quality of your synthetic examples along multiple characteristics. You can use these quality scores to compare different synthetic data generators (including prompts being used to produce the synthetic data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and import dependencies\n",
    "\n",
    "Make sure you have `wget` and `zip` installed to run this tutorial. You can use pip to install all other packages required for this tutorial as follows:\n",
    "\n",
    "```bash\n",
    "pip install cleanlab-studio datasets\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab_studio import Studio\n",
    "from datasets import load_dataset\n",
    "from typing import Dict\n",
    "\n",
    "import os\n",
    "import datasets\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to render image_filename column of DataFrame as images\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "def path_to_img_html(path: str) -> str:\n",
    "    return f'<img src=\"{path}\" width=\"100\"/>'\n",
    "\n",
    "def display(df: pd.DataFrame) -> None:\n",
    "    return HTML(df.to_html(escape=False, formatters=dict(image_filename=path_to_img_html)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properly format your data\n",
    "\n",
    "Usually, your real data has been stored separately from your synthetic data. Here we demonstrate how to merge these two data sources into a single dataset with a binary `real_or_synthetic` column. This is the proper format to assess your synthetic data quality with Cleanlab Studio. The proportion of synthetic vs real data can vary, but try to provide enough real data that Cleanlab's AI system can learn key characteristics of the real data.\n",
    "\n",
    "We'll use a combination of real and synthetic images of watermelons and pineapples as our tutorial dataset. We do not consider separate training/test data splits for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc 'https://cleanlab-public.s3.amazonaws.com/Datasets/synthetic-quality/real_images.zip'\n",
    "!wget -nc 'https://cleanlab-public.s3.amazonaws.com/Datasets/synthetic-quality/synthetic_images.zip'\n",
    "\n",
    "!unzip -qu real_images.zip -d images/ && unzip -qu synthetic_images.zip -d images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd = os.getcwd()\n",
    "\n",
    "def dataset_to_dataframe(\n",
    "    dataset: datasets.Dataset,\n",
    "    label_column: str,\n",
    "    image_column: str,\n",
    "    synthetic_flag: str,\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    df = dataset.to_pandas()\n",
    "\n",
    "    # Create id column\n",
    "    ids = [f\"{synthetic_flag}_{i}\" for i in range(len(df))]\n",
    "    df[\"id\"] = ids\n",
    "\n",
    "    # Format image column\n",
    "    image_to_image_filename = lambda row: row[image_column][\"path\"].split(pwd)[-1].strip(\"/\")\n",
    "    df[\"image_filename\"] = df.apply(image_to_image_filename, axis=1)\n",
    "\n",
    "    # Format label column\n",
    "    int2str = dataset.features[label_column].int2str\n",
    "    df[\"label\"] = df[label_column].apply(int2str)\n",
    "\n",
    "    # Add synthetic flag\n",
    "    df[\"real_or_synthetic\"] = synthetic_flag\n",
    "\n",
    "    return df[[\"id\", \"image_filename\", \"label\", \"real_or_synthetic\"]]\n",
    "\n",
    "\n",
    "# Load dataset from local directory\n",
    "real_dataset = load_dataset(\"imagefolder\", data_dir=\"images/real\", split=\"train\")\n",
    "synthetic_dataset = load_dataset(\"imagefolder\", data_dir=\"images/synthetic\", split=\"train\")\n",
    "\n",
    "\n",
    "# Convert to pandas dataframes\n",
    "real_df = dataset_to_dataframe(\n",
    "    real_dataset, label_column=\"label\", image_column=\"image\", synthetic_flag=\"real\"\n",
    ")\n",
    "synthetic_df = dataset_to_dataframe(\n",
    "    synthetic_dataset, label_column=\"label\", image_column=\"image\", synthetic_flag=\"synthetic\"\n",
    ")\n",
    "\n",
    "# Combine real and synthetic dataframes\n",
    "combined_dataset_df = pd.concat([real_df, synthetic_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(combined_dataset_df.groupby([\"label\", \"real_or_synthetic\"]).sample(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Random images in the dataset](./assets/synthetic_data/random_images.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note this DataFrame also contains `id` and `label` columns, which are relevant metadata that we'd like to see in our Cleanlab Studio results.  Such metadata is optional, it will not be used by Cleanlab Studio to evaluate the quality of the synthetic data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the combined real+synthetic dataset into Cleanlab Studio\n",
    "\n",
    "### Dataset Structure and Formatting\n",
    "\n",
    "Cleanlab Studio has particular directory structure requirements when uploading locally-hosted image datasets, and we are adhering to these requirements to ensure the smooth integration of our dataset. \n",
    "\n",
    "Here's the expected directory structure:\n",
    "\n",
    "```\n",
    "images\n",
    "├── metadata.csv\n",
    "├── real\n",
    "│   ├── pineapple\n",
    "│   │   ├── image_0.png\n",
    "|   |   ├── image_1.png\n",
    "│   │   ⋮\n",
    "│   └── watermelon\n",
    "|       ├── image_1.png\n",
    "│       ├── image_0.png\n",
    "│       ⋮\n",
    "└── synthetic\n",
    "    ├── pineapple\n",
    "    │   ├── image_0.png\n",
    "    │   ├── image_1.png\n",
    "    │   ⋮\n",
    "    └── watermelon\n",
    "        ├── image_0.png\n",
    "        ├── image_1.png\n",
    "        ⋮\n",
    "```\n",
    "\n",
    "- **Parent Directory**: For our demonstration, `images/` serves as the top-level directory. It holds all images and the essential `metadata.csv` file.\n",
    "\n",
    "- **Real & Synthetic Directories**: These are fundamental divisions of our dataset. The `real/` and `synthetic/` directories clearly differentiate between actual and generated images.\n",
    "\n",
    "- **Sub-category Directories (Optional)**: Divisions such as `pineapple/` and `watermelon/` are for organizational clarity. Cleanlab Studio will not consider these in the synthetic data assessment we run here.\n",
    "\n",
    "Ensure that your image dataset respects a similar structure.\n",
    "\n",
    "We write the contents of our combined DataFrame to such a `metadata.csv` file and zip the `images/` directory for loading our data into Cleanlab Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset_df.to_csv(\"images/metadata.csv\", index=False)\n",
    "\n",
    "!zip -rq formatted_synthetic_data.zip images/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata.csv Contents\n",
    "The `metadata.csv` plays a critical role in helping Cleanlab Studio understand and categorize each image. Below, we delve into the purpose and requirements of each column:\n",
    "\n",
    "```\n",
    "id,image_filename,label (optional metadata),real_or_synthetic\n",
    "real_0,images/real/pineapple/image_0.png,pineapple,real\n",
    "real_1,images/real/pineapple/image_1.png,pineapple,real\n",
    "real_2,images/real/pineapple/image_10.png,pineapple,real\n",
    "...\n",
    "synthetic_199,images/synthetic/watermelon/image_97.png,watermelon,synthetic\n",
    "synthetic_200,images/synthetic/watermelon/image_98.png,watermelon,synthetic\n",
    "synthetic_201,images/synthetic/watermelon/image_99.png,watermelon,synthetic\n",
    "```\n",
    "\n",
    "- `id`: A unique identifier for each image. It doesn't have to follow the `real_`/`synthetic_` prefix format as demonstrated above. The chosen format in this tutorial is for clarity and convenience. However, it's crucial that each id is unique across the dataset.\n",
    "- `image_filename`: This column represents the path to the image file. The path should be relative to the top directory where the `metadata.csv` is located.\n",
    "- `real_or_synthetic`: *This column is essential for this tutorial*. It categorizes the image as either real or synthetic. Make sure you correctly specify this for every image.\n",
    "- label (optional metadata): This column is purely optional. It provides a label for the image but doesn't impact the evaluation. It's a piece of additional metadata attached to each example, allowing further analysis of the results if desired.\n",
    "\n",
    "From this point, it's just a matter of uploading the zip-file to Cleanlab Studio.\n",
    "\n",
    "If you're working with a large image dataset, you may want to host your images externally on S3. We [discuss this approach in the 'Finding Issues in Large-Scale Image Datasets' tutorial](https://help.cleanlab.ai/tutorials/large_image_datasets#prep-and-upload-dataset).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can find your API key by going to app.cleanlab.ai/upload, \n",
    "# clicking \"Upload via Python API\", and copying the API key there\n",
    "API_KEY = \"<insert your API key>\"\n",
    "studio = Studio(API_KEY)\n",
    "\n",
    "dataset_id = studio.upload_dataset(\"formatted_synthetic_data.zip\", dataset_name=\"SyntheticData\", modality=\"image\", id_column=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Project\n",
    "\n",
    "Using your `dataset_id` from the previous step, you can create a Project in Cleanlab Studio with one line of code! This will train Cleanlab's AI to analyze your synthetic data for shortcomings relative to the real data, which takes some time ( ~1-2 minutes for our tutorial dataset). You will get an email when the process has completed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = studio.create_project(\n",
    "    dataset_id, \n",
    "    project_name=\"SyntheticDataProject\", \n",
    "    modality=\"image\", \n",
    "    model_type=\"fast\", \n",
    "    label_column=\"real_or_synthetic\", # Don't confuse this with the column named \"label\" in the dataset\n",
    ")\n",
    "print(f'Project successfully created and training has begun! project_id: {project_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning!** This next cell may take a long time to execute while the model is being trained. If your notebook has timed out during this process then you can resume work by re-running the cell (which should return the `cleanset_id` instantly if the project has completed training).\n",
    "\n",
    "- In the **case of notebook timeout or closing of notebook**, rerun the code cells up to (but not including) the \"*Load the combined real+synthetic dataset into Cleanlab Studio*\" section. You can manually get your `cleanlab_cols` by using the `project_id` printed above and running the code below in a new cell. **DO NOT RUN `studio.create_project` A SECOND TIME!**\n",
    "\n",
    "  ```python\n",
    "  studio = Studio(api_key)\n",
    "  cleanset_id = studio.get_latest_cleanset_id(project_id)\n",
    "  project_status = studio.poll_cleanset_status(cleanset_id)\n",
    "  ```\n",
    "\n",
    "You can check your Project's status with the Python API using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanset_id = studio.get_latest_cleanset_id(project_id)\n",
    "project_status = studio.poll_cleanset_status(cleanset_id)\n",
    "print(f'Project training complete! cleanset_id: {cleanset_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training is complete, you can proceed to fetching the cleanlab columns from your Project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanlab_cols = studio.download_cleanlab_columns(cleanset_id)\n",
    "\n",
    "# Combine the dataset with the cleanlab columns\n",
    "cleanset_df = combined_dataset_df.merge(cleanlab_cols, on='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess the synthetic data via quality scores\n",
    "\n",
    "Now that we've obtained the results (`cleanlab_cols`) of our Project, we can score the quality of the synthetic data. Thus far, we have not explicitly told Cleanlab Studio that our goal is synthetic data assessment (we just ran a typical Project with a special `real_vs_synthetic` column in our dataset). Now we will use functions intended specifically for synthetic data assessment.\n",
    "\n",
    "\n",
    "Below are some helper functions for processing the dataframes, as well as adding a few helpful columns for the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "REAL_OR_SYNTH_COLUMN = \"real_or_synthetic\"\n",
    "\n",
    "def _fidelity_score(cleanset_df: pd.DataFrame, target_type: str = \"synthetic\") -> float:\n",
    "    \"\"\"Scores the dataset based on how indistinguishable the synthetic data is from real data.\"\"\"\n",
    "    return _lqs_to_synthetic_score(cleanset_df, target_type)\n",
    "\n",
    "def _representativeness_score(cleanset_df: pd.DataFrame, target_type: str = \"real\") -> float:\n",
    "    \"\"\"Scores the dataset on how well the real data is represented in the synthetic data.\"\"\"\n",
    "    return _lqs_to_synthetic_score(cleanset_df, target_type)\n",
    "\n",
    "def _diversity_score(cleanset_df: pd.DataFrame, target_type: str = \"synthetic\") -> float:\n",
    "    \"\"\"Scores the diversity among synthetic examples; higher scores indicate less repetition of similar synthetic instances.\"\"\"\n",
    "    return _calculate_synthetic_duplicate_scores(cleanset_df, target_type, False)\n",
    "\n",
    "def _freshness_score(cleanset_df: pd.DataFrame, target_type: str = \"synthetic\") -> float:\n",
    "    \"\"\"Scores the synthetic data's novelty; higher scores suggest synthetic data isn't just echoing real examples.\"\"\"\n",
    "    return _calculate_synthetic_duplicate_scores(cleanset_df, target_type, True)\n",
    "\n",
    "def _lqs_to_synthetic_score(cleanset_df: pd.DataFrame, target_type: str) -> float:\n",
    "    # 1. Filter rows by synthetic type\n",
    "    indices = cleanset_df[REAL_OR_SYNTH_COLUMN] == target_type\n",
    "\n",
    "    # 2. Get mean label quality score\n",
    "    mean_lqs = cleanset_df.loc[indices, \"label_issue_score\"].mean()\n",
    "\n",
    "    # 3. Compute the score\n",
    "    score = 1 - mean_lqs\n",
    "    return score\n",
    "\n",
    "\n",
    "def _calculate_synthetic_duplicate_scores(cleanset_df: pd.DataFrame, target_type: str, contains_real: bool) -> float:\n",
    "    \n",
    "    # (0. Define the complementary class name)\n",
    "    synthetic_class_names = cleanset_df[REAL_OR_SYNTH_COLUMN].unique()\n",
    "    assert len(synthetic_class_names) == 2, \"The dataset should contain both real and synthetic examples\"\n",
    "    complementary_class: str = synthetic_class_names[~(synthetic_class_names == target_type)][0]\n",
    "    \n",
    "    # 1. Extract rows that are marked as near duplicates\n",
    "    df_near_duplicates = cleanset_df.query(\"is_near_duplicate\")\n",
    "    \n",
    "    # 2. Compute the groups of near dupliates that contain real data, but always contain synthetic data\n",
    "    group_contains_real = df_near_duplicates.groupby('near_duplicate_id').apply(lambda group: (group[REAL_OR_SYNTH_COLUMN] == complementary_class).any())\n",
    "    \n",
    "    # 3. Filter the dataframe based on the synthetic type (real or synthetic) and whether it's a near duplicate or not\n",
    "    filtered_df = df_near_duplicates[df_near_duplicates[REAL_OR_SYNTH_COLUMN] == target_type]\n",
    "    \n",
    "    # 4. Count the synthetic examples based on the condition (whether they duplicate real data or not)\n",
    "    if contains_real:\n",
    "        synthetic_duplicate_count = filtered_df[filtered_df['near_duplicate_id'].map(group_contains_real)].shape[0]\n",
    "    else:\n",
    "        synthetic_duplicate_count = filtered_df[~filtered_df['near_duplicate_id'].map(group_contains_real)].shape[0]\n",
    "    \n",
    "    # 5. Compute the total synthetic examples\n",
    "    total_synthetic_examples = cleanset_df[cleanset_df[REAL_OR_SYNTH_COLUMN] == target_type].shape[0]\n",
    "    \n",
    "    # 6. Calculate the final score\n",
    "    score = 1 - (synthetic_duplicate_count / total_synthetic_examples)\n",
    "    \n",
    "    return score\n",
    "\n",
    "\n",
    "def _display_example_counts(cleanset_df: pd.DataFrame, real_type: str):\n",
    "    \"\"\"Displays the number of real and synthetic examples in the dataset.\"\"\"\n",
    "    num_real_examples = len(cleanset_df.query(f\"{REAL_OR_SYNTH_COLUMN} == '{real_type}'\"))\n",
    "    num_synthetic_examples = len(cleanset_df) - num_real_examples\n",
    "    print(f\"Number of real examples: {num_real_examples}\")\n",
    "    print(f\"Number of synthetic examples: {num_synthetic_examples}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_synthetic_data_quality(cleanset_df: pd.DataFrame, synthetic_class_names: Optional[Tuple[str, str]] = None) -> Dict:\n",
    "    \"\"\"Computes the scores for a dataset consisting of real and synthetic data, to evaluate the quality of the synthetic data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cleanset_df: pd.DataFrame\n",
    "        The dataframe containing the dataset to score. It should contain a column named \"real_or_synthetic\" that indicates whether each example is real or synthetic.\n",
    "        It should also have the cleanset columns provided by Cleanlab Studio.\n",
    "\n",
    "    synthetic_class_names: Optional[Tuple[str, str]]\n",
    "        The class names of the \"real_or_synthetic\" column.\n",
    "        If not provided, the function will assume that the name of the synthetic class is \"synthetic\" and the name of the real class is \"real\".\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configure the synthetic class names\n",
    "    synthetic_type, real_type = synthetic_class_names or (\"synthetic\", \"real\")\n",
    "\n",
    "    # Display the number of real and synthetic examples\n",
    "    _display_example_counts(cleanset_df, real_type)\n",
    "\n",
    "    # Compute the scores\n",
    "    scores = {\n",
    "        \"fidelity\": _fidelity_score(cleanset_df, target_type=synthetic_type),\n",
    "        \"representativeness\": _representativeness_score(cleanset_df, target_type=real_type),\n",
    "        \"diversity\": _diversity_score(cleanset_df, target_type=synthetic_type),\n",
    "        \"freshness\": _freshness_score(cleanset_df, target_type=synthetic_type)\n",
    "    }\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of real examples: 200\n",
      "Number of synthetic examples: 202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'representativeness': 0.1389657307255,\n",
       " 'fidelity': 0.13592233734455428,\n",
       " 'diversity': 0.9108910891089109,\n",
       " 'freshness': 0.9752475247524752}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score the quality of the synthetic data in the dataset\n",
    "score_synthetic_data_quality(cleanset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores quantify how closely synthetic data mirrors real examples, its distinguishability from real data, its internal diversity, and its originality compared to real examples.\n",
    "\n",
    "- **Fidelity**: Evaluates how indistinguishable the synthetic data appears from real data. Low values indicate there are many unrealistic-looking synthetic samples which are obviously fake.\n",
    "- **Representativeness**: Measures how well represented the real data is amongst the synthetic data samples. Low values indicate there may exist tails of the real data distribution (or rare events) that the distribution of synthetic samples fails to capture. \n",
    "- **Diversity**: Evaluates how much variety there is among synthetic samples. Low values indicate an overly repetitive synthetic data generator that produced similar instances which look nearly duplicated.\n",
    "- **Freshness**: Evaluates the novelty of the synthetic data. Low values indicate many synthetic samples are near duplicates of an example from the real dataset, i.e. the synthetic data generator may be memorizing the real data too closely and failing to generalize.\n",
    "\n",
    "**Note**: For all quality scores, lower values indicate lower-quality synthetic data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate synthetic samples based on quality scores\n",
    "\n",
    "The fidelity score gives us an idea of how realistic the synthetic images look. A low score indicates that the synthetic data generator makes too many images that are easy to discern from real images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show unrealistic synthetic images\n",
    "\n",
    "columns = [\"id\", \"image_filename\", \"label\", \"label_issue_score\"]\n",
    "unrealistic_synthetic_images = (\n",
    "    cleanset_df\n",
    "    .query(\"real_or_synthetic == 'synthetic'\")\n",
    "    .sort_values(\"label_issue_score\",ascending=False)\n",
    "    .head()\n",
    "    [columns]\n",
    ")\n",
    "display(unrealistic_synthetic_images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Unrealistic synthetic examples](./assets/synthetic_data/unrealistic_synthetic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The representativeness score indicates whether real examples are well represented in the synthetic data. When the representativeness score is low, it means that the synthetic generator fails to pick up certain patterns in the real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show underrepresented real images\n",
    "\n",
    "columns = [\"id\", \"image_filename\", \"label\", \"label_issue_score\"]\n",
    "underrepresented_real_images = (\n",
    "    cleanset_df\n",
    "    .query(\"real_or_synthetic == 'real'\")\n",
    "    .sort_values(\"label_issue_score\",ascending=False)\n",
    "    .head(5)\n",
    "    [columns]\n",
    ")\n",
    "display(underrepresented_real_images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Underrepresented real examples](./assets/synthetic_data/underrepresented_real.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use two metrics, the diversity and freshness scores, to measure the occurrence of near duplicates among synthetic examples. The diversity score quantifies how many synthetic examples only resemble other synthetic examples. The freshness score highlights the proportion of synthetic examples that are near duplicates of real instances.\n",
    "To explore these near duplicates in detail, they can be grouped using the `near_duplicate_id` assigned to each identified instance.\n",
    "\n",
    "For visualizing the set of duplicate images, we have to look up the associated image filenames and collect those. Because near duplicates can technically be exact duplicates, it's important to display some identifier for each example. We use the `id` column for this purpose. With the formatting of the `id` column, we can also get a sense of how many synthetic and real examples are in each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_near_duplicate_group(df: pd.DataFrame) -> pd.Series:\n",
    "    # Create a dictionary with the near_duplicate_id as keys and list of indices as values\n",
    "    near_duplicate_groups = df.groupby('near_duplicate_id').apply(lambda x: x.index.tolist())\n",
    "    \n",
    "    # For each row, if it is a near duplicate, get the group indices excluding its own index\n",
    "    near_duplicate_group_column = df.apply(lambda row: [idx for idx in near_duplicate_groups.get(row['near_duplicate_id'], []) if idx != row.name] if row['is_near_duplicate'] else [], axis=1)\n",
    "    \n",
    "    return near_duplicate_group_column\n",
    "\n",
    "def get_near_duplicate_groupings(df: pd.DataFrame) -> dict:\n",
    "    groups = get_near_duplicate_group(df)\n",
    "    return {index: group for index, group in groups.items()}\n",
    "\n",
    "\n",
    "def get_associated_images_html(index, df, groupings):\n",
    "    associated_ids = groupings.get(index, [])\n",
    "    associated_paths = df.loc[associated_ids, 'image_filename'].tolist()\n",
    "\n",
    "    img_htmls = [path_to_img_html(path) for path in associated_paths]\n",
    "\n",
    "    # Aad a caption of the id for each image\n",
    "    captions = [f'<figcaption style=\"text-align:center\">{df.loc[id][\"id\"]}</figcaption>' for id in associated_ids]\n",
    "\n",
    "    # Wrap each image in a figure tag, and add a caption\n",
    "    img_htmls_with_captions = [f'<figure>{img_html}{caption}</figure>' for img_html, caption in zip(img_htmls, captions)]\n",
    "\n",
    "    # In line all the images to show them side by side, instead of one below the other\n",
    "    img_htmls_side_by_side = '<div style=\"display:flex\">' + ''.join(img_htmls_with_captions) + '</div>'\n",
    "    # Align the images by the caption vertically\n",
    "    return img_htmls_side_by_side\n",
    "\n",
    "def display_with_associated_images(df: pd.DataFrame, n:int=5) -> None:\n",
    "    groupings = get_near_duplicate_groupings(df)\n",
    "    filtered_df = df[df.is_near_duplicate].copy()\n",
    "    filtered_df['associated_images'] = filtered_df.index.map(lambda index: get_associated_images_html(index, df, groupings))\n",
    "    columns = [\"id\", \"image_filename\", \"label\", \"real_or_synthetic\", \"associated_images\", \"near_duplicate_id\"]\n",
    "    return HTML(filtered_df[columns].groupby(\"near_duplicate_id\").first().to_html(escape=False, formatters=dict(image_filename=path_to_img_html)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can look at sets of near duplicates that are exclusively synthetic. \n",
    "\n",
    "This contributes to the \"diversity\" score of the synthetic data, where a low scores means that most of the synthetic data is likely being regurgitated over and over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_low_diversity_synthetic_examples(df: pd.DataFrame):\n",
    "    groupings = get_near_duplicate_groupings(df)\n",
    "\n",
    "    filtered_df = df[df.is_near_duplicate].copy()\n",
    "    group_contains_real = filtered_df.groupby('near_duplicate_id').apply(lambda group: (group['real_or_synthetic'] == 'real').any())\n",
    "    stale_near_duplicate_ids = group_contains_real[group_contains_real == False].index\n",
    "    filtered_df['associated_images'] = filtered_df.index.map(lambda index: get_associated_images_html(index, df, groupings))\n",
    "    filtered_df['is_stale'] = filtered_df['near_duplicate_id'].isin(stale_near_duplicate_ids)\n",
    "    stale_near_duplicates = filtered_df.query(\"is_stale\")\n",
    "\n",
    "    \n",
    "    columns = [\"id\", \"image_filename\", \"label\", \"real_or_synthetic\", \"associated_images\", \"near_duplicate_id\"]\n",
    "    return HTML(stale_near_duplicates[columns].groupby(\"near_duplicate_id\").first().to_html(escape=False, formatters=dict(image_filename=path_to_img_html)))\n",
    "\n",
    "\n",
    "display_low_diversity_synthetic_examples(cleanset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Low diversity synthetic near duplicate examples](./assets/synthetic_data/diversity_issues.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other group of near duplicates worth looking at in this tutorial are the set of synthetic examples that are similar to some real examples. These contribute to the \"freshness\" score that we calculated.\n",
    "\n",
    "A low freshness score means that the synthetic data generator has likely memorized it’s training data and struggles to generate novel samples that are actually different from the real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_stale_near_duplicates(df: pd.DataFrame):\n",
    "    groupings = get_near_duplicate_groupings(df)\n",
    "\n",
    "    filtered_df = df[df.is_near_duplicate].copy()\n",
    "    group_contains_real = filtered_df.groupby(\"near_duplicate_id\").apply(lambda group: (group[\"real_or_synthetic\"] == \"real\").any())\n",
    "    memorized_near_duplicate_ids = group_contains_real[group_contains_real == True].index\n",
    "    filtered_df[\"associated_images\"] = filtered_df.index.map(lambda index: get_associated_images_html(index, df, groupings))\n",
    "    filtered_df[\"is_memorized\"] = filtered_df[\"near_duplicate_id\"].isin(memorized_near_duplicate_ids)\n",
    "\n",
    "    stale_near_duplicates = filtered_df.query(\"is_memorized\")\n",
    "\n",
    "    \n",
    "    columns = [\"id\", \"image_filename\", \"label\", \"real_or_synthetic\", \"associated_images\", \"near_duplicate_id\"]\n",
    "    return HTML(stale_near_duplicates[columns].groupby(\"near_duplicate_id\").first().to_html(escape=False, formatters=dict(image_filename=path_to_img_html)))\n",
    "\n",
    "display_stale_near_duplicates(cleanset_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Low freshness near duplicate examples](./assets/synthetic_data/freshness_issues.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "The quality of your synthetic dataset is vital. We've shown you how to assess it with one particular image dataset, but the code from this tutorial can be generally applied to any synthetic and real data you might have (including text or tabular data).\n",
    "\n",
    "Try repeatedly running this notebook and the `score_synthetic_data_quality` function for every set of prompts (or other data generation settings you can tweak) to see if they improve the quality of your synthetic dataset. Note you can run many Cleanlab Studio projects simultaneously to speed up your synthetic data optimization and prompt engineering efforts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
