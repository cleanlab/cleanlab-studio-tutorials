{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bb388d8-70ed-4aae-a0dc-0baeee0d9b0d",
   "metadata": {},
   "source": [
    "# Trustworthy Language Model (TLM) - Advanced Usage\n",
    "\n",
    "<head>\n",
    "  <meta name=\"title\" content=\"Trustworthy Language Model (TLM) - Advanced Usage\"/>\n",
    "  <meta property=\"og:title\" content=\"Trustworthy Language Model (TLM) - Advanced Usage\"/>\n",
    "  <meta name=\"twitter:title\" content=\"Trustworthy Language Model (TLM) - Advanced Usage\" />\n",
    "  <meta name=\"image\" content=\"/img/tlm-chat.png\" />\n",
    "  <meta property=\"og:image\" content=\"/img/tlm-chat.png\" />\n",
    "  <meta name=\"description\" content=\"A more reliable LLM that quantifies trustworthiness for every output and can detect bad responses.\"  />\n",
    "  <meta property=\"og:description\" content=\"A more reliable LLM that quantifies trustworthiness for every output and can detect bad responses.\" />\n",
    "  <meta name=\"twitter:description\" content=\"A more reliable LLM that quantifies trustworthiness for every output and can detect bad responses.\" />\n",
    "</head>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7304fa0-ca0b-4861-839b-8c05ccdc7e19",
   "metadata": {},
   "source": [
    "Assuming you've run the Trustworthy Language Model [quickstart tutorial](/tlm/tutorials/tlm/),\n",
    "here you can learn more about TLM including how to:\n",
    "\n",
    "- Explain low trustworthiness scores\n",
    "- Configure TLM to reduce latency/costs and get better/faster results\n",
    "- Run TLM over large datasets and handle errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9038ef9-743c-48b6-b928-31438aac2c2e",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This tutorial requires a TLM API key. Get one [here](https://tlm.cleanlab.ai/).\n",
    "\n",
    "Cleanlab's TLM Python client can be installed using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19a5e57-c7ab-4699-b322-20d68ccecc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade cleanlab-tlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10725826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API key\n",
    "import os\n",
    "os.environ[\"CLEANLAB_TLM_API_KEY\"] = \"<API key>\" # Get your free API key from: https://tlm.cleanlab.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914425e7-336e-4f1d-99fb-5796e2b0530c",
   "metadata": {},
   "source": [
    "## Explaining Low Trustworthiness Scores\n",
    "\n",
    "To understand why TLM estimated low trustworthiness for each particular prompt/response, specify the `explanation` flag when initializing TLM. With this flag specified, the `output` dictionary that TLM returns for each input will contain an extra field called `explanation`.\n",
    "\n",
    "Explanations will be generated for both `prompt()` and `get_trustworthiness_score()` methods. Reasons why a particular LLM response is deemed untrustworthy include:\n",
    "\n",
    "- an alternative contradictory response was almost instead generated by the LLM\n",
    "- reasoning/factual errors were discovered during self-reflection by the LLM\n",
    "- the given prompt/response is atypical relative to the LLM's training data.\n",
    "\n",
    "Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38801a7c-92b5-4187-9ca1-38aa824e0ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Bobby has 3 sisters, and since Bobby is one of the brothers, he is the only brother that each of his sisters has. Therefore, there is only 1 brother (Bobby) in total.\n",
      "Trustworthiness Score: 0.6618441307670467\n",
      "\n",
      "Explanation: This response is untrustworthy due to lack of consistency in possible responses from the model. Here's one inconsistent alternate response that the model considered (which may not be accurate either): \n",
      "2 brothers.\n"
     ]
    }
   ],
   "source": [
    "from cleanlab_tlm import TLM\n",
    "\n",
    "tlm = TLM(options={\"log\": [\"explanation\"]})\n",
    "\n",
    "output = tlm.prompt(\"Bobby (a boy) has 3 sisters. Each sister has 2 brothers. How many brothers?\")\n",
    "\n",
    "print(f'Response: {output[\"response\"]}')\n",
    "print(f'Trustworthiness Score: {output[\"trustworthiness_score\"]}\\n')\n",
    "print(f'Explanation: {output[\"log\"][\"explanation\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d14e357-ae67-485a-b67d-9ed8e376e296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trustworthiness Score: 0.05100168462960243\n",
      "\n",
      "Explanation: The question \"Do LLMs dream of electric sheep?\" is a playful reference to Philip K. Dick's novel \"Do Androids Dream of Electric Sheep?\" which explores themes of consciousness and artificial intelligence. However, LLMs (Large Language Models) do not possess consciousness, emotions, or the ability to dream in any sense, whether of electric sheep or real sheep. The proposed answer suggests that LLMs can dream, which is not factually accurate. Therefore, the answer is not correct in a literal sense, as LLMs do not have the capability to dream at all. \n",
      "This response is untrustworthy due to lack of consistency in possible responses from the model. Here's one inconsistent alternate response that the model considered (which may not be accurate either): \n",
      "No, LLMs do not dream of electric sheep, as they lack consciousness and the capacity for subjective experiences that define dreaming.\n"
     ]
    }
   ],
   "source": [
    "output = tlm.get_trustworthiness_score(prompt=\"Do LLMs dream of electric sheep?\", response=\"Yes, but they prefer to dream of real sheep.\")\n",
    "print(f'Trustworthiness Score: {output[\"trustworthiness_score\"]}\\n')\n",
    "print(f'Explanation: {output[\"log\"][\"explanation\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914425e7-336e-4f1d-99fb-5796e2b0530d",
   "metadata": {},
   "source": [
    "Currently, TLM only provides explanations for the trustworthiness score, not additional custom evaluation criteria scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f73f33c-1d79-4c51-9d7a-38d6e569d97d",
   "metadata": {},
   "source": [
    "## Optional TLM Configurations for Better/Faster Results\n",
    "\n",
    "TLM's **default configuration is not latency/cost-optimized** because it must remain effective across all possible LLM use-cases.\n",
    "For your specific use-case, you can **greatly improve latency/cost without compromising results**. Strategy: first run TLM with default settings to see results over a dataset from your use-case; Then adjust the `model`, `quality_preset`, and other `TLMOptions` to reduce latency for your application. If TLM\u2019s default configuration seems ineffective, switch to a more powerful `model` (e.g. gpt-4.1, o4-mini, o3, claude-3.7-sonnet, or claude-3.5-sonnet-v2) or add [custom evaluation criteria](/tlm/tutorials/tlm_custom_eval/).\n",
    "\n",
    "We describe these **optional** configurations below. If you email us (support@cleanlab.ai), our engineers can optimize TLM for your use-case in 15min -- it's that easy!\n",
    "\n",
    "\n",
    "### Task Type\n",
    "\n",
    "TLM generally works for *any* LLM application. For certain tasks, get better results by specifying your `task` as one of:\n",
    "- `classification`: For multi-class classification tasks where the LLM chooses amongst a predefined set of categories/answers.\n",
    "- `code_generation`: For software engineering tasks where the LLM outputs code/programs.\n",
    "- `default`: Generic configuration for most use cases (used when you don't specify a task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cdd24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tlm = TLM(task=\"classification\")  # or task could be say: 'code_generation'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f73f33c-1d79-4c51-9d7a-38d6e569d94e",
   "metadata": {},
   "source": [
    "### Quality Presets\n",
    "\n",
    "You can trade-off latency vs. quality via TLM's `quality_preset` argument. For many use-cases, a lower `quality_preset` performs just as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b1c65a-b0ad-4f96-a535-f6dc79e4d69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tlm = TLM(quality_preset=\"low\")  # supported quality presets: 'best', 'high', 'medium' (default), 'low', 'base'\n",
    "\n",
    "# Run a prompt using this TLM preset:\n",
    "output = tlm.prompt(\"<your prompt>\")  # this call gets faster using the 'low' preset\n",
    "\n",
    "# Or run a batch of prompts simultaneously:\n",
    "outputs = tlm.prompt([\"<your first prompt>\", \"<your second prompt>\", \"<your third prompt>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1ea4a3-e992-43b1-8f67-d4f7bbd5b41c",
   "metadata": {},
   "source": [
    "#### Details about TLM quality presets: \n",
    "\n",
    "Quality Preset | Trustworthiness Score Quality | Auto-Improvement of LLM Response in TLM.prompt()\n",
    " ---|-----|-----\n",
    "Best | Good | Significant improvement \n",
    "High | Good | Moderate improvement \n",
    "Medium | Good | None (same response as base LLM model) \n",
    "Low | Fair | None (same response as base LLM model)\n",
    "Base | Lowest latency | None (same response as base LLM model)\n",
    "\n",
    "For faster results, reduce the preset to `low` or `base` (default preset is `medium`).\n",
    "If you just want [trustworthiness scores](https://cleanlab.ai/blog/4o-claude/), **avoid** `best` or `high` presets. Those presets are for automatically [improving LLM responses](https://cleanlab.ai/blog/llm-accuracy/) returned by `TLM.prompt()`.\n",
    "`TLM.prompt()` using `medium`, `low`, or `base` preset returns the *same* response from the base LLM model that you'd ordinarily get. `TLM.prompt()` using `best` or `high` preset internally runs the base LLM multiple times to return a more trustworthy response.\n",
    "\n",
    "Rigorous [benchmarks](https://cleanlab.ai/blog/trustworthy-language-model/) reveal that running TLM with the `best` preset can reduce the error rate (incorrect answers): of GPT-4o by 27%, of GPT-4 by 10%, and of GPT-3.5 by 22%.\n",
    "If you encounter token limit errors, try a lower quality preset.\n",
    "\n",
    "**Note:** The range of the trustworthiness scores may slightly differ depending on your preset. Don't directly compare the magnitude of TLM scores across different presets (settle on a preset before determining score thresholds). What remains comparable across different presets is how these TLM scores _rank_ data or LLM responses from most to least confidently good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cab1d28",
   "metadata": {},
   "source": [
    "### Other TLM Options\n",
    "\n",
    "When initializing a TLM instance, optionally specify the `options` argument as a dictionary of extra configurations beyond the quality preset. See the `TLMOptions` [documentation](/tlm/api/python/tlm/#class-tlmoptions). Here are useful options:\n",
    "\n",
    "- **model**: Which underlying LLM model TLM should utilize. TLM is a wrapper around *any* base LLM API to get trustworthiness scores for that LLM and auto-improve its responses. For low latency/cost, specify a fast model like `gpt-4.1-nano` or `nova-micro`. For high accuracy, specify a powerful model like `gpt-4.1`, `o4-mini`, `o3`, `claude-3.7-sonnet`, or `claude-3.5-sonnet-v2`.\n",
    "\n",
    "- **max_tokens**: The maximum number of tokens TLM should generate. Decrease this value if you hit token limit errors or to improve runtimes.\n",
    "\n",
    "For example, here's how to auto-improve responses from the Claude 3.5 Sonnet LLM and score their trustworthiness using this LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27d950e9-085c-4280-8409-ea4a73162ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tlm = TLM(quality_preset=\"best\", options={\"model\": \"claude-3.5-sonnet\"})\n",
    "\n",
    "output = tlm.prompt(\"<your prompt>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b33f59-371e-40b4-a753-49e06780990a",
   "metadata": {},
   "source": [
    "### Trustworthy Language Model Lite\n",
    "\n",
    "Using [TLMLite](https://cleanlab.ai/blog/tlm-lite/) in place of `TLM` enables you to use a powerful/slower LLM to generate each response and a faster LLM to score its trustworthiness.\n",
    "\n",
    "[TLMLite](https://cleanlab.ai/blog/tlm-lite/) is just like `TLM`, except you can specify a separate `response_model` for generating responses. Other `options` for `TLMLite` only apply to the trustworthiness scoring model.\n",
    "\n",
    "For example, here we use the larger `gpt-4.1` model to generate reponses, and the faster `gpt-4.1-nano` model to score their trustworthiness. To further reduce latency/cost, we specify `quality_preset=\"low\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26fc509d-8d2c-46d8-b934-58737cff8b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab_tlm import TLMLite\n",
    "\n",
    "tlm_lite = TLMLite(response_model=\"gpt-4.1\", quality_preset=\"low\", options={\"model\": \"gpt-4.1-nano\"})\n",
    "\n",
    "output = tlm_lite.prompt(\"<your prompt>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b33f59-371e-40b4-a753-49e06780990b",
   "metadata": {},
   "source": [
    "### Reducing Latency\n",
    "\n",
    "To use TLM in an ultra low-latency real-time application, you might: stream in responses from your own LLM, and use `TLM.get_trustworthiness_score()` to subsequently stream in the corresponding trustworthiness score.\n",
    "\n",
    "In your `TLMOptions`, specify a smaller/faster [`model`](/tlm/api/python/tlm/#class-tlmoptions) (e.g. `gpt-4.1-nano` or `nova-micro`) and a lower `quality_preset` (e.g. `low` or `base`). Also try reducing values in [`TLMOptions`](/tlm/api/python/tlm/#class-tlmoptions) such as:\n",
    "\n",
    "- `reasoning_effort`: try lower setting (e.g. `low` or `none`)\n",
    "- `similarity_measure`: try setting this to `string`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aadf509-d1ba-4dde-a162-88237ada5128",
   "metadata": {},
   "source": [
    "## Running TLM over large datasets\n",
    "\n",
    "To avoid overwhelming our API with requests, there's a maximum number of tokens per minute that you can query TLM with (*rate limit*). If running multiple prompts simultaneously in batch, you'll need to stay under the rate limit, but you'll also want to optimize for getting all results quickly.\n",
    "\n",
    "If you hit token limit errors, consider playing with TLM's `quality_preset` and `max_tokens` parameters. If you run TLM on individual examples yourself in a for loop, you may hit the rate limit, so we recommend running in batches of many prompts passed in as a list. \n",
    "\n",
    "The `TLM.prompt()` and `TLM.get_trustworthiness_score()` methods will handle failed examples by returning a dictionary with `null` values for the `response` and `trustworthiness_score` keys, along with a `log` key containing detailed error information. The error information includes an error message describing the specific issue (such as exceeding token limits) and a boolean flag indicating whether the error is retryable. These methods still return results for the remaining examples in the provided list where TLM ran successfully. You can re-run examples with retryable errors to get results. This approach allows you to process the successful results while still having comprehensive information about any failures that occurred, enabling better error handling and potential retry strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b93871-5e1c-4513-b950-354d459427a9",
   "metadata": {},
   "source": [
    "### Mini-batching\n",
    "\n",
    "If your datasets have over several thousand examples, we recommend running TLM in mini-batches to checkpoint intermediate results.\n",
    "\n",
    "This helper function allows you to run TLM in mini-batches. We recommend batch sizes of approximately 1000, but feel free to tinker with this number to best suit your use case. You can re-execute this function in the case of any failures and it will resume from the previous checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03af110-2388-4731-bb96-4f40ab09fcc4",
   "metadata": {},
   "source": [
    "**Optional: Define helper function: batch_prompt()**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b36674b-477b-4ca1-bcf4-9a676734e924",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batch_prompt(tlm: TLM, input_path: str, output_path: str, prompt_col_name: str, batch_size: int = 1000):\n",
    "    \"\"\"Handles any failures (errors or timeouts) by returning a dictionary with `null` values in place of the failures\"\"\"\n",
    "    if os.path.exists(output_path):\n",
    "        start_idx = len(pd.read_csv(output_path))\n",
    "    else:\n",
    "        start_idx = 0\n",
    "\n",
    "    df_batched = pd.read_csv(input_path, chunksize=batch_size)\n",
    "    curr_idx = 0\n",
    "\n",
    "    for curr_batch in df_batched:\n",
    "        # if results already exist for the entire batch\n",
    "        if curr_idx + len(curr_batch) <= start_idx:\n",
    "            curr_idx += len(curr_batch)\n",
    "            continue\n",
    "\n",
    "        # if results exist for half the batch\n",
    "        elif curr_idx < start_idx:\n",
    "            curr_batch = curr_batch[start_idx - curr_idx:]\n",
    "            curr_idx = start_idx\n",
    "\n",
    "        results = tlm.prompt(curr_batch[prompt_col_name].to_list())\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_csv(output_path, mode='a', index=False, header=not os.path.exists(output_path))\n",
    "        \n",
    "        curr_idx += len(curr_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a59af65-4ccc-41c1-9d2e-4b26818166e1",
   "metadata": {},
   "source": [
    "Here we'll demonstrate using the `batch_prompt()` method on a toy dataset of 100 prompts, but this can be run at scale. Just specify: an instantiated TLM object, the input file path to a CSV file containing your prompts and the column name in which they are located, as well as the output file\u00a0path where results should be stored, and your intended batch size (we recommend ~1000 examples per batch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30cba007-157f-4b82-961a-c00c3486cf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create sample prompts\n",
    "sample_prompts = pd.DataFrame({\"prompt\": [f\"What is the sum of 1 and {i}?\" for i in range(1, 101)]})\n",
    "sample_prompts.to_csv(\"sample_tlm_prompts.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fcc5df2-ac2e-4596-9a75-12e7eac7cf91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the sum of 1 and 1?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the sum of 1 and 2?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the sum of 1 and 3?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the sum of 1 and 4?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the sum of 1 and 5?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        prompt\n",
       "0  What is the sum of 1 and 1?\n",
       "1  What is the sum of 1 and 2?\n",
       "2  What is the sum of 1 and 3?\n",
       "3  What is the sum of 1 and 4?\n",
       "4  What is the sum of 1 and 5?"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_path = \"sample_tlm_prompts.csv\"\n",
    "output_path = \"sample_responses.csv\"\n",
    "\n",
    "df = pd.read_csv(input_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e646a7-9f09-45b5-bffe-02179b2a5a9a",
   "metadata": {},
   "source": [
    "We can then call the `batch_prompt` function to run TLM in mini-batches. Note that if this cell fails for any reason, you can just re-execute it and TLM will resume processing your data from the previous checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1ae1b47-f93f-43c6-9ad2-b0d4ae2a36b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tlm = TLM() \n",
    "\n",
    "batch_prompt(\n",
    "    tlm=tlm,\n",
    "    input_path=input_path, \n",
    "    output_path=output_path, \n",
    "    prompt_col_name=\"prompt\", \n",
    "    batch_size=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e39938-c7ee-4e12-be23-3dea66df14a9",
   "metadata": {},
   "source": [
    "After the cell above is done executing, we can view the saved results in the output file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ddb95fb-b2de-4516-9e16-3f1c9a69409a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>trustworthiness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sum of 1 and 1 is 2.</td>\n",
       "      <td>0.959896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The sum of 1 and 2 is 3.</td>\n",
       "      <td>0.983522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The sum of 1 and 3 is 4.</td>\n",
       "      <td>0.978253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The sum of 1 and 4 is 5.</td>\n",
       "      <td>0.980250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The sum of 1 and 5 is 6.</td>\n",
       "      <td>0.969275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   response  trustworthiness_score\n",
       "0  The sum of 1 and 1 is 2.               0.959896\n",
       "1  The sum of 1 and 2 is 3.               0.983522\n",
       "2  The sum of 1 and 3 is 4.               0.978253\n",
       "3  The sum of 1 and 4 is 5.               0.980250\n",
       "4  The sum of 1 and 5 is 6.               0.969275"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(output_path)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd4b2c5-dd71-4b53-bac1-1ff98ba93e00",
   "metadata": {},
   "source": [
    "### Retrying Failed Queries\n",
    "\n",
    "When running large batches of prompts, some queries may fail due to issues like temporary network errors or timeouts. As recommended above, you can use the `TLM.prompt()` and `TLM.get_trustworthiness_score()` methods to handle the failed examples by returning a dictionary with detailed error information. By examining the log data in the response, you can efficiently retry only the queries that encountered retryable errors, without reprocessing the successful ones. This section demonstrates how you can implement a retry mechanism for the failed queries.\n",
    "\n",
    "For the purposes of this tutorial, we'll intentionally use a very short timeout when calling TLM to trigger some errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156cd0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Querying TLM... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'response': 'The sum of 1 and 1 is 2.',\n",
       "  'trustworthiness_score': 0.9598961301228769},\n",
       " {'response': 'The sum of 1 and 2 is 3.',\n",
       "  'trustworthiness_score': 0.9835215390172598},\n",
       " {'response': 'The sum of 1 and 3 is 4.',\n",
       "  'trustworthiness_score': 0.9782526873921833},\n",
       " {'response': 'The sum of 1 and 4 is 5.',\n",
       "  'trustworthiness_score': 0.9802496022988343},\n",
       " {'response': None,\n",
       "  'trustworthiness_score': None,\n",
       "  'log': {'error': {'message': 'Timeout while waiting for prediction. Please retry or consider increasing the timeout.',\n",
       "    'retryable': True}}}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tlm = TLM(timeout=0.25) \n",
    "\n",
    "prompts = [f\"What is the sum of 1 and {i}?\" for i in range(1, 10)]\n",
    "res_with_failures = tlm.prompt(prompts)\n",
    "res_with_failures[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329b767b-bd45-4f7f-b2f8-f5f19c894222",
   "metadata": {},
   "source": [
    "We see above that while some queries succeeded, others failed due to timeout errors. Since timeout errors are retryable, we can define the `retry_prompt(`) helper function to retry only the failed prompts and combine the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200e6690-5cab-435b-9c1b-cd63fca06063",
   "metadata": {},
   "source": [
    "**Optional: TLM retry_prompt helper function**\n",
    "\n",
    "",
    "We will also define a `retry_get_trustworthiness_score` function here, which acts the same way as `retry_prompt` but for obtaining trustworthiness scores for prompt-response pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "093437bf-02ac-4ada-bc96-02bb94841c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def retry_prompt(tlm, prompts, tlm_responses):\n",
    "    failed_idx = [idx for idx, item in enumerate(tlm_responses) if item.get('log', {}).get('error', {}).get('retryable')]\n",
    "    failed_prompts = np.array(prompts)[failed_idx]\n",
    "    \n",
    "    retry_res = tlm.prompt(failed_prompts.tolist())\n",
    "    tlm_responses_array = np.array(tlm_responses)\n",
    "    tlm_responses_array[failed_idx] = retry_res\n",
    "    \n",
    "    return tlm_responses_array.tolist()\n",
    "\n",
    "def retry_get_trustworthiness_score(tlm, prompts, responses, tlm_scores):\n",
    "    failed_idx = [idx for idx, item in enumerate(tlm_scores) if item.get('log', {}).get('error', {}).get('retryable')]\n",
    "    failed_prompts = np.array(prompts)[failed_idx]\n",
    "    failed_responses = np.array(responses)[failed_idx]\n",
    "    retry_res = tlm.get_trustworthiness_score(failed_prompts.tolist(), failed_responses.tolist())\n",
    "    tlm_scores_array = np.array(tlm_scores)\n",
    "    tlm_scores_array[failed_idx] = retry_res\n",
    "    \n",
    "    return tlm_scores_array.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d11d148-a809-4478-9a38-e9e03a1c36e6",
   "metadata": {},
   "source": [
    "This function takes three inputs: \n",
    "\n",
    "- `tlm`, an instantiated TLM object\n",
    "- `prompts`, which is a list of all the original prompts (same list that was initially passed to `TLM.prompt()`)\n",
    "- `tlm_responses`, the list of responses from TLM that includes both successful results and error logs, which will help us to identify which prompts failed and can be retried.\n",
    "\n",
    "The `retry_prompt()` will only try to re-execute TLM on the prompts, and will return the aggregated results that combines the succesful responses from the previous `TLM.prompt()` call and also the retried responses. Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e074096c-6927-4829-8f0d-b6514e4a2b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Querying TLM... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'response': 'The sum of 1 and 1 is 2.',\n",
       "  'trustworthiness_score': 0.9598961301228769},\n",
       " {'response': 'The sum of 1 and 2 is 3.',\n",
       "  'trustworthiness_score': 0.9835215390172598},\n",
       " {'response': 'The sum of 1 and 3 is 4.',\n",
       "  'trustworthiness_score': 0.9782526873921833},\n",
       " {'response': 'The sum of 1 and 4 is 5.',\n",
       "  'trustworthiness_score': 0.9802496022988343},\n",
       " {'response': 'The sum of 1 and 5 is 6.',\n",
       "  'trustworthiness_score': 0.9692750020809217},\n",
       " {'response': 'The sum of 1 and 6 is 7.',\n",
       "  'trustworthiness_score': 0.9747792954587126},\n",
       " {'response': 'The sum of 1 and 7 is 8.',\n",
       "  'trustworthiness_score': 0.9725614671284452},\n",
       " {'response': 'The sum of 1 and 8 is 9.',\n",
       "  'trustworthiness_score': 0.9811183463257951},\n",
       " {'response': 'The sum of 1 and 9 is 10.',\n",
       "  'trustworthiness_score': 0.9718280444545313}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retry_res = retry_prompt(tlm, prompts, res_with_failures)\n",
    "retry_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b72ecb-d90f-49d8-a0af-36c04698f180",
   "metadata": {},
   "source": [
    "After retrying, we see that the full list of prompts have succeeded.\n",
    "\n",
    "However, note that retrying failed queries does not guarantee success. If a prompt continues to fail after a few retry attempts, consider checking your inputs for potential errors or making adjustments to your parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02b1578",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Search for your use-case in our tutorials and [cheat sheet](/tlm/faq/) to learn how you can best use TLM.\n",
    "\n",
    "- If you need an additional capability or deployment option, or are unsure how to achieve desired results, just ask: [support@cleanlab.ai](mailto:support@cleanlab.ai). We love hearing from users, and are happy to help optimize TLM latency/accuracy for your use-case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}