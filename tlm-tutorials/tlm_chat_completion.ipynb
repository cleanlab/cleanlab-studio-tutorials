{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "626216e6",
   "metadata": {},
   "source": [
    "# Using TLM with OpenAI's Chat Completions API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82efd2ae",
   "metadata": {},
   "source": [
    "This tutorial demonstrates the easiest ways to score the trustworthiness of responses from the OpenAI [Chat Completions API](https://platform.openai.com/docs/api-reference/chat). With *minimal* changes to your existing Chat Completions API code, you can score the trustworthiness of every LLM response in real-time (works for all OpenAI models and most non-OpenAI LLMs, which also support the Chat Completions API, such as: Gemini, DeepSeek, Llama, etc). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1efe315",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "The Python packages required for this tutorial can be installed using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "828db034",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade cleanlab-tlm openai azure-ai-inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8c5b8e",
   "metadata": {},
   "source": [
    "This tutorial requires a TLM API key. Get one [here](https://tlm.cleanlab.ai/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dbd8218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CLEANLAB_TLM_API_KEY\"] = \"<Cleanlab TLM API key>\"  # Get your free API key from: https://tlm.cleanlab.ai/\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<OpenAI API key>\"  # for using OpenAI client library, not strictly necessary for all workflows shown here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ace027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from cleanlab_tlm.utils.chat_completions import TLMChatCompletion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8f07fc",
   "metadata": {},
   "source": [
    "## Overview of this tutorial\n",
    "\n",
    "We'll showcase three different workflows to incorporate trust scoring into your existing LLM code, with minimal code changes:\n",
    "\n",
    "- Workflow 1 & 2: Use your own existing LLM infrastructure to generate responses, then use Cleanlab to score them\n",
    "- Workflow 3: Use Cleanlab for both generating and scoring responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e12122",
   "metadata": {},
   "source": [
    "## Workflow 1: Score Responses from Existing LLM Calls\n",
    "\n",
    "One way to use TLM if you're already using OpenAI's ChatCompletions API is to score any existing LLM call you've made. This works for LLMs beyond OpenAI models (many LLM providers like Gemini or DeepSeek also support OpenAI's Chat Completions API)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836451af",
   "metadata": {},
   "source": [
    "You can first obtain generate LLM responses as usual using the [OpenAI API](https://github.com/openai/openai-python) (or any other existing LLM infrastructure you use):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88348955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-BllXeJIA0HPwmc8YbtZXEhkIwZKir', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The capital of France is Paris.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1750723866, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_6f2eabb9a5', usage=CompletionUsage(completion_tokens=7, prompt_tokens=24, total_tokens=31, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_kwargs = dict(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "client = OpenAI()\n",
    "response = client.chat.completions.create(**openai_kwargs)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc414d2-8e73-4891-8b7a-665a91767dfc",
   "metadata": {},
   "source": [
    "We can then use TLM to score the generated response.\n",
    "\n",
    "Here, we first instantiate a `TLMChatCompletion` object. For more configurations, view all the valid arguments in our [API documentation](/tlm/api/python/utils.chat_completions/#class-tlmchatcompletion).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f9b5017",
   "metadata": {},
   "outputs": [],
   "source": [
    "tlm = TLMChatCompletion(quality_preset=\"medium\", options={\"model\": \"gpt-4.1-mini\", \"log\": [\"explanation\"]}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbb8c3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The capital of France is Paris.\n",
      "TLM Score: 0.9873\n",
      "TLM Explanation: Did not find a reason to doubt trustworthiness.\n"
     ]
    }
   ],
   "source": [
    "score_result = tlm.score(\n",
    "    response=response,\n",
    "    **openai_kwargs\n",
    ")\n",
    "\n",
    "print(f\"Response: {response.choices[0].message.content}\")\n",
    "print(f\"TLM Score: {score_result['trustworthiness_score']:.4f}\")\n",
    "print(f\"TLM Explanation: {score_result['log']['explanation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a25e26f",
   "metadata": {},
   "source": [
    "<details><summary> What's different if I'm using Azure OpenAI? <b>(click to expand)</b></summary>\n",
    "\n",
    "The only difference would be that your existing code to generate the response would look like this:\n",
    "\n",
    "```python\n",
    "client = AzureOpenAI(\n",
    "    api_version=\"<your-api-version>\",\n",
    "    azure_endpoint=\"<your-azure-endpoint>\",\n",
    "    api_key=\"<your-azure-api-key>\",\n",
    ")\n",
    "response = client.chat.completions.create(**openai_kwargs)\n",
    "```\n",
    "\n",
    "instead of:\n",
    "\n",
    "```python\n",
    "client = OpenAI()\n",
    "response = client.chat.completions.create(**openai_kwargs)\n",
    "```\n",
    "\n",
    "The code to score this response using TLM remains identical as shown above.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29f54ce",
   "metadata": {},
   "source": [
    "## Workflow 2: Adding a Decorator to your LLM Call\n",
    "\n",
    "Alternatively, you decorate your call to `openai.chat.completions.create()` with a decorator that then appends the trust score as a key in the returned response. This workflow only requires minimal initial setup; after that zero changes are needed in the rest of your existing code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "710cfbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def add_trust_scoring(tlm_instance):\n",
    "    \"\"\"Decorator factory that creates a trust scoring decorator.\"\"\"\n",
    "    def trust_score_decorator(fn):\n",
    "        @functools.wraps(fn)\n",
    "        def wrapper(**kwargs):\n",
    "            response = fn(**kwargs)\n",
    "            score_result = tlm_instance.score(response=response, **kwargs)\n",
    "            response.tlm_metadata = score_result\n",
    "            return response\n",
    "        return wrapper\n",
    "    return trust_score_decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40b4a57e-9fce-4011-9046-81baf172e9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tlm = TLMChatCompletion(quality_preset=\"medium\", options={\"model\": \"gpt-4.1-mini\", \"log\": [\"explanation\"]}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524021e9-4e45-4648-98eb-72400c66d541",
   "metadata": {},
   "source": [
    "Then decorate your OpenAI Chat Completions function like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40b4a57e-9fce-4011-9046-81baf172e9va",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "client.chat.completions.create = add_trust_scoring(tlm)(client.chat.completions.create)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524021e9-4e45-4648-98eb-72400c66d542",
   "metadata": {},
   "source": [
    "After you decorate OpenAI's Chat Completions function like this, all of your existing Chat Completions API code will automatically compute trust scores as well (zero change needed in other code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc8aa301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-BllXeduYKX6ltuxvHw0pFhGBu7s8R', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The capital of France is Paris.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1750723866, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_6f2eabb9a5', usage=CompletionUsage(completion_tokens=7, prompt_tokens=24, total_tokens=31, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)), tlm_metadata={'trustworthiness_score': 0.9872832331806422, 'log': {'explanation': 'Did not find a reason to doubt trustworthiness.'}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(**openai_kwargs)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "445c199c-ef55-4860-9337-38f8c573a22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The capital of France is Paris.\n",
      "TLM Score: 0.9873\n",
      "TLM Explanation: Did not find a reason to doubt trustworthiness.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Response: {response.choices[0].message.content}\")\n",
    "print(f\"TLM Score: {response.tlm_metadata['trustworthiness_score']:.4f}\")\n",
    "print(f\"TLM Explanation: {response.tlm_metadata['log']['explanation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbd8fcb",
   "metadata": {},
   "source": [
    "<details><summary> What's different if I'm using Azure OpenAI? <b>(click to expand)</b></summary>\n",
    "\n",
    "The only difference would be that your existing code to generate the response would look like this:\n",
    "\n",
    "```python\n",
    "client = AzureOpenAI(\n",
    "    api_version=\"<your-api-version>\",\n",
    "    azure_endpoint=\"<your-azure-endpoint>\",\n",
    "    api_key=\"<your-azure-api-key>\",\n",
    ")\n",
    "client.chat.completions.create = add_trust_scoring(tlm)(client.chat.completions.create)\n",
    "```\n",
    "\n",
    "instead of:\n",
    "\n",
    "```python\n",
    "client = OpenAI()\n",
    "client.chat.completions.create = add_trust_scoring(tlm)(client.chat.completions.create)\n",
    "```\n",
    "\n",
    "The code to score this response using TLM remains identical as shown above.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42b6420",
   "metadata": {},
   "source": [
    "## Workflow 3: Use Cleanlab to Generate and Score Responses\n",
    "\n",
    "For convenience, you can alternatively generate responses using Cleanlab's infrastructure while simultaneously providing trustworthiness scores. Response-generation can be done using any of the OpenAI LLM models supported within TLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10cd9f4",
   "metadata": {},
   "source": [
    "### Using the OpenAI Client\n",
    "\n",
    "To do this, simply point the OpenAI client at Cleanlab's backend instead of OpenAI's.\n",
    "Instantiate an [OpenAI client](https://github.com/openai/openai-python), point its `base_url` to the Cleanlab backend (see URL below), and specify your Cleanlab API key. After that, you can use the `chat.completions.create()` method as you normally would (zero change to any existing code), obtaining responses and trust scores without needing an OpenAI key/account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55f0983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=\"<Cleanlab TLM API key>\",  # get your API key from: https://tlm.cleanlab.ai/\n",
    "    base_url=\"https://api.cleanlab.ai/api/v1/openai_trustworthy_llm/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c44e262",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-BllXgv6mrp09H0UskaXkBkElqmE7a', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The capital of France is Paris.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1750723868, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier=None, system_fingerprint='fp_6f2eabb9a5', usage=CompletionUsage(completion_tokens=7, prompt_tokens=24, total_tokens=31, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)), _request_id='req_a35d1e47a2e058a61405402895f18707', tlm_metadata={'trustworthiness_score': 0.9898358185685113})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "    ]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30f94554-8d95-4c44-aa46-402780a4e5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The capital of France is Paris.\n",
      "TLM Score: 0.9898\n"
     ]
    }
   ],
   "source": [
    "print(f\"Response: {response.choices[0].message.content}\")\n",
    "print(f\"TLM Score: {response.tlm_metadata['trustworthiness_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a25e26g",
   "metadata": {},
   "source": [
    "<details><summary> What's different if I'm using Azure OpenAI? <b>(click to expand)</b></summary>\n",
    "\n",
    "If you were using the Azure OpenAI client, simply make the following replacements in your code:\n",
    "\n",
    "- `from openai import AzureOpenAI` -> `from openai import OpenAI`\n",
    "- `client = AzureOpenAI()` ->  `client = OpenAI(...)` with the arguments specified above\n",
    "\n",
    "The rest of this section should work with your existing code, as the API interface and input/output types are the same between `OpenAI` and `AzureOpenAI`.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba1819b",
   "metadata": {},
   "source": [
    "### Using the Azure AI Inference Client\n",
    "\n",
    "Alternatively, you can also use TLM via the `azure-ai-inference` client by pointing at Cleanlab's backend.\n",
    "Here we instantiate the `ChatCompletionsClient` from Azure and point its `endpoint` to the Cleanlab backend (see URL below), and specify your Cleanlab API key. After that, you can use the `complete()` method as you normally would (zero change to any existing code) to obtain responses and trust scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3e2aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "azure_client = ChatCompletionsClient(\n",
    "    credential=AzureKeyCredential(\"<Cleanlab TLM API key>\"),  # get your API key from: https://tlm.cleanlab.ai/\n",
    "    endpoint=\"https://api.cleanlab.ai/api/v1/openai_trustworthy_llm/\",  # replace with your TLM service URL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4930bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_request_id': 'req_aa8c5380d40a742bcaf2e2f4ed24e163', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'annotations': [], 'audio': None, 'content': 'The capital of France is Paris.', 'function_call': None, 'refusal': None, 'role': 'assistant', 'tool_calls': None}}], 'created': 1752104263, 'id': 'chatcmpl-BrYe7Ab5cTikH6309vd0fFXGToYSb', 'model': 'gpt-4.1-mini-2025-04-14', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': 'fp_6f2eabb9a5', 'tlm_metadata': {'trustworthiness_score': 0.998297591743598}, 'usage': {'completion_tokens': 7, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens': 24, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'total_tokens': 31}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = azure_client.complete(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42e22e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The capital of France is Paris.\n",
      "TLM Score: 0.9982\n"
     ]
    }
   ],
   "source": [
    "print(f\"Response: {response.choices[0].message.content}\")\n",
    "print(f\"TLM Score: {response['tlm_metadata']['trustworthiness_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0471715",
   "metadata": {},
   "source": [
    "<details><summary> Getting Faster/Better Results <b>(click to expand)</b></summary>\n",
    "\n",
    "The default TLM settings are not latency-optimized because they have to remain effective across all possible LLM use-cases. For your specific use-case, you can greatly improve latency without compromising results.\n",
    "\n",
    "**Strategy**: first run TLM with default settings to see what results look like over a dataset from your use-case; once results look promising, adjust the TLM preset/options/model to reduce latency for your application.\n",
    "\n",
    "View more tips to improve latency and accuracy in our [FAQ](/tlm/faq/#reduce-latency) and [Advanced Tutorial](/tlm/tutorials/tlm_advanced/#optional-tlm-configurations-for-betterfaster-results).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b38ed2",
   "metadata": {},
   "source": [
    "<details><summary> Running over Batches/Datasets <b>(click to expand)</b></summary>\n",
    "\n",
    "When processing large datasets, here are some tips to handle rate limits and implement proper batching strategies.\n",
    "\n",
    "**Prevent hitting rate limits**:\n",
    "- Process data in small batches (e.g. 10-50 requests at a time)\n",
    "- Add sleep intervals between batches (e.g. `time.sleep(1)`) to stay under rate limits\n",
    "\n",
    "**Handling errors**:\n",
    "- Save partial results frequently to avoid losing progress\n",
    "- Consider using a try/except block to catch errors, and implement retry logic when rate limits are hit\n",
    "\n",
    "You may find the basic TLM API showcased in our [Quickstart tutorial](/tlm/tutorials/tlm/) simpler for running TLM over datasets, as it manages all of the above for you.\n",
    "\n",
    "Otherwise, here are helper functions to help with batching LLM calls when relying on the Chat Completions API:\n",
    "\n",
    "```python\n",
    "from openai import OpenAI\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"<Cleanlab TLM API key>\",  # get your API key from: https://tlm.cleanlab.ai/\n",
    "    base_url=\"https://api.cleanlab.ai/api/v1/openai_trustworthy_llm/\"\n",
    ")\n",
    "\n",
    "def invoke_llm_with_retries(openai_kwargs, retries=3, backoff=2):\n",
    "    attempt = 0\n",
    "    while attempt <= retries:\n",
    "        try:\n",
    "            # the code to invoke the LLM goes here, feel free to modify\n",
    "            response = client.chat.completions.create(**openai_kwargs)\n",
    "            return {\n",
    "                \"response\": response.choices[0].message.content,\n",
    "                \"trustworthiness_score\": response.tlm_metadata[\"trustworthiness_score\"],\n",
    "                \"raw_completion\": response\n",
    "            }\n",
    "        except Exception as e:\n",
    "            if attempt == retries:\n",
    "                return {\"error\": str(e), \"input\": openai_kwargs}\n",
    "            sleep_time = backoff ** attempt\n",
    "            time.sleep(sleep_time)\n",
    "            attempt += 1\n",
    "\n",
    "def run_batch(batch_data, batch_size=20, max_threads=8, sleep_time=5):\n",
    "    results = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(batch_data), batch_size)):\n",
    "        data = batch_data[i:i + batch_size]\n",
    "        batch_results = [None] * len(data)\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_threads) as executor:\n",
    "            future_to_idx = {executor.submit(invoke_llm_with_retries, d): idx for idx, d in enumerate(data)}\n",
    "            for future in as_completed(future_to_idx):\n",
    "                idx = future_to_idx[future]\n",
    "                batch_results[idx] = future.result()\n",
    "                \n",
    "        results.extend(batch_results)\n",
    "\n",
    "        # sleep to prevent hitting rate limits\n",
    "        if i + batch_size < len(batch_data):\n",
    "            time.sleep(sleep_time)\n",
    "            \n",
    "    return results\n",
    "\n",
    "\n",
    "sample_input = dict(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "    ]\n",
    ")\n",
    "sample_batch = [sample_input] * 10\n",
    "run_batch(sample_batch)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e68e267",
   "metadata": {},
   "source": [
    "## Resources to learn more about Chat Completions API\n",
    "\n",
    "- [OpenAI guide on rate limits](https://cookbook.openai.com/examples/how_to_handle_rate_limits)\n",
    "- [Chat Completions API reference](https://platform.openai.com/docs/api-reference/chat)\n",
    "- [OpenAI client library](https://github.com/openai/openai-python)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}