{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a994c8b5-cc71-4a88-bae1-c6c8f6567870",
   "metadata": {},
   "source": [
    "# Incorporating custom evaluation criteria for LLM outputs and calibrating TLM trustworthiness scores against human ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceebdba3-b6ad-4e21-9451-4315ea7cfb04",
   "metadata": {},
   "source": [
    "Are you finding TLM trustworthiness scores do not align with your team's manual quality ratings of good/bad LLM outputs? If so, you can address this via:\n",
    "- Adjusting prompts and [TLM options/configurations](/tlm/tutorials/tlm_advanced/)\n",
    "- Custom evaluation criteria\n",
    "- Calibrating TLM trustworthiness scores against your human quality ratings for example prompt-response pairs.\n",
    "\n",
    "This tutorial demonstrates how to evaluate and improve the performance of TLM for your specific use-case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a386ec-152b-45bd-af07-fa854602db82",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This tutorial requires a TLM API key. Get one [here](https://tlm.cleanlab.ai/).\n",
    "\n",
    "Cleanlab's TLM Python client can be installed using pip. This tutorial also requires the `scikit-learn` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b970467b-5afd-4bd4-a2d9-33e6eda054c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade cleanlab-tlm scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4815e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API key\n",
    "import os\n",
    "os.environ[\"CLEANLAB_TLM_API_KEY\"] = \"<API key>\" # Get your free API key from: https://tlm.cleanlab.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22dbe0c8-f6b2-4ef4-af19-4f96efdfe292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import spearmanr  # just used for evaluation of results in this tutorial\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41256d78-d8f8-463d-967f-5d763d828020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab_tlm import TLM\n",
    "\n",
    "tlm = TLM()  # See Advanced Tutorial for optional TLM configurations to get better/faster results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a45c23a-afa5-4a23-90b5-696602b3804d",
   "metadata": {},
   "source": [
    "## Fetch the dataset\n",
    "\n",
    "Let's first load an example dataset. Here we consider a dataset composed of trivia questions with responses already computed from some LLM, which we want to automatically evaluate with TLM.\n",
    "\n",
    "In this tutorial we will be using the `tlm.get_trustworthiness_score()` method to evaluate these prompt-response pairs, and explore how we can use various techniques to calibrate TLM's trustworthiness score to align with your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e29aa20-e4c4-49df-a989-429319799641",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://cleanlab-public.s3.amazonaws.com/Datasets/triviaqa-conciseness/triviaqa_with_ratings.csv\n",
    "!wget -nc https://cleanlab-public.s3.amazonaws.com/Datasets/triviaqa-conciseness/triviaqa_without_ratings.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0e76435-4f5a-4384-a871-724f3c28d2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"triviaqa_with_ratings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54d5df1-5daf-4522-a447-bb81a32d4088",
   "metadata": {},
   "source": [
    "Suppose we aim to score LLM responses based on both correctness of the response and conciseness (ie. the LLM response should not contain extraneous information). Our dataset additionally contains human ratings regarding the (manually-determined) quality of each response. Even without having such human ratings, you can still use many of the techniques demonstrated in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebeed0a-2c52-4fcf-8f1e-849e508c3d8f",
   "metadata": {},
   "source": [
    "Here is a sample question where the LLM response is factually correct, however it contains too much irrelevant information, and hence it is marked as incorrect by a human annotator (rating = 0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87ae4f59-9822-4697-8e40-b872b4d47fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Bonar Law is the only Prime Minister not born in the UK. In which country was he born? \n",
      "\n",
      "Response: Bonar Law, who briefly served as British Prime Minister in the early 1920s, holds the distinction of being the only Prime Minister born outside the United Kingdom. He was born in Canada, specifically in the province of New Brunswick. This unique aspect of his background adds an interesting layer to his political legacy, as he later rose to lead the UK government during a particularly tumultuous period in British history. \n",
      "\n",
      "Human Rating: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Question:\", data[\"question\"][0], \"\\n\")\n",
    "print(\"Response:\", data[\"answer\"][0], \"\\n\")\n",
    "print(\"Human Rating:\", data[\"human_rating\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e9e236-4d9b-4216-84fe-8ada972cb618",
   "metadata": {},
   "source": [
    "The LLM response to this other question is both correct and concise, hence it has been manually marked as high-quality (human rating = 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "697e34a8-c2ca-4084-bd3b-f9dd6a496885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which singer had a big 60s No 1 with Roses Are Red? \n",
      "\n",
      "Response: Bobby Vinton \n",
      "\n",
      "Human Rating: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Question:\", data[\"question\"][1], \"\\n\")\n",
    "print(\"Response:\", data[\"answer\"][1], \"\\n\")\n",
    "print(\"Human Rating:\", data[\"human_rating\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2308110d-f602-432f-97fe-7776b01d81d1",
   "metadata": {},
   "source": [
    "## Scoring responses with TLM\n",
    "\n",
    "At first, let's just append a simple prompt template to each question, and then use `get_trustworthiness_score()` to score each response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67b7264a-7561-4313-91fe-f07dfe7ac47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prompt:\n",
      "Answer this question as concisely as possible: Bonar Law is the only Prime Minister not born in the UK. In which country was he born?\n"
     ]
    }
   ],
   "source": [
    "def create_prompt(question):\n",
    "    return f\"Answer this question as concisely as possible: {question}\"\n",
    "    \n",
    "data[\"prompt\"] = data[\"question\"].apply(create_prompt)\n",
    "print(\"Sample prompt:\")\n",
    "print(data[\"prompt\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e61e8914-0122-4ba3-9e5a-82b79b4bf7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Querying TLM...   0%|          |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Querying TLM... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|\n"
     ]
    }
   ],
   "source": [
    "res = tlm.get_trustworthiness_score(data[\"prompt\"].tolist(), data[\"answer\"].tolist())\n",
    "res_df = pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fae002-0ed7-408f-b747-e687b86aabb9",
   "metadata": {},
   "source": [
    "Spearman's rank correlation is a statistical method used to measure the strength of the relationship between two ranked variables. We can use it here to measure how well the trustworthiness scores from TLM reflect the given human ratings.\n",
    "\n",
    "In this tutorial, the human ratings are binary (only 0 and 1 values representing correct and incorrect responses). However, all methods showcased in this tutorial can be applied to any range of numeric human ratings, such as ratings on a scale of 1 to 5. Spearman's rank correlation is a valid measure of alignment betweeen estimated trustworthiness scores and human ratings, regardless whether they are binary or numeric, as it quantifies how consistently good responses are scored higher than bad ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b38ff09a-74c0-4553-b8a2-dc50fa71e3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation: 0.5433137542226836\n"
     ]
    }
   ],
   "source": [
    "print(\"Spearman correlation:\", spearmanr(res_df[\"trustworthiness_score\"], data[\"human_rating\"]).statistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8b6800-6028-421a-83b5-bf74a921da46",
   "metadata": {},
   "source": [
    "## Prompt engineering for better TLM trustworthiness scoring\n",
    "\n",
    "One simple way to improve TLM performance is to improve the instructions we give. With clearer instructions on how the LLM should respond, TLM can better score the responses for our specific use-case.\n",
    "\n",
    "Here for example, we can update TLM's `prompt` to specify that wordy answers are considered incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c5d4b62-8538-4b1b-9fd8-e51095d49707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample improved prompt:\n",
      "Answer the following question as concisely as you can. \n",
      "For the answer to be considered correct, it has to be concise. Wordy answers are incorrect.\n",
      "Question: Bonar Law is the only Prime Minister not born in the UK. In which country was he born?\n"
     ]
    }
   ],
   "source": [
    "def create_improved_prompt(question):\n",
    "    return f\"\"\"Answer the following question as concisely as you can. \n",
    "For the answer to be considered correct, it has to be concise. Wordy answers are incorrect.\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "data[\"improved_prompt\"] = data[\"question\"].apply(create_improved_prompt)\n",
    "print(\"Sample improved prompt:\")\n",
    "print(data[\"improved_prompt\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "939154f8-0847-44e5-aa51-e3ba38e92b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Querying TLM... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|\n"
     ]
    }
   ],
   "source": [
    "res_improved = tlm.get_trustworthiness_score(data[\"improved_prompt\"].tolist(), data[\"answer\"].tolist())\n",
    "res_improved = pd.DataFrame(res_improved)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496b3ca6-1d23-483c-ab36-00c88b327c49",
   "metadata": {},
   "source": [
    "We see that with the updated prompt, TLM's trustworthiness score is now better aligned with our human ratings (as measured via their Spearman correlation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd859cd5-ee6f-496b-96aa-3b0a034e1d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation: 0.584929275822719\n"
     ]
    }
   ],
   "source": [
    "print(\"Spearman correlation:\", spearmanr(res_improved[\"trustworthiness_score\"], data[\"human_rating\"]).statistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7c4cb1-83bd-451f-b6dc-8945a2b0528d",
   "metadata": {},
   "source": [
    "<details><summary> Prompt engineering tips for TLM <b>(click to expand)</b></summary>\n",
    "\n",
    "When using the TLM method: `get_trustworthiness_score(prompt, response)`, how you specify the `prompt` argument matters.\n",
    "Pick a `prompt` that if given to a standard LLM would yield a response that you'd generally consider good.\n",
    "\n",
    "To identify such a `prompt` template, you can run `TLM.prompt(prompt)` over a dataset with various candidate prompt templates (here using TLM with say `quality_preset='base'`).\n",
    "The best prompt template should be the one for which `TLM.prompt()` yields responses that look best to you.\n",
    "\n",
    "Use standard prompt engineering techniques such as: specifying the desired response format, whether to be concise or include explanations/reasoning, what to mention vs. not, how to handle edge-cases, background information to better contextualize the task, etc.\n",
    "Important elements of an effective `prompt` argument for TLM include: specifying precisely what a **correct answer** should contain, and perhaps also giving examples of wrong answers.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7c4cb1-83bd-451f-b6dc-8945a2b0528c",
   "metadata": {},
   "source": [
    "## Custom evaluation criteria\n",
    "\n",
    "Our improved prompt template did enhance TLM performance. However, evaluating conciseness of the response represents a particular notion of response-quality that may not align with TLM's general trustworthiness scoring (which aims to quantify uncertainty in the LLM). Fortunately, TLM supports *custom evaluation criteria*, allowing you to define specific ways to additionally evaluate response-quality.\n",
    "\n",
    "We can define our custom evaluation criteria in TLM's `options` dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b067111-6a5e-4d9f-a3c5-b44229aeac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_eval_criteria_option = {\"custom_eval_criteria\": [\n",
    "    {\"name\": \"Conciseness\", \"criteria\": \"Determine if the response is concise.\"}\n",
    "]}\n",
    "\n",
    "tlm_custom_eval = TLM(options=custom_eval_criteria_option)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34418261-caad-47c0-8ef9-8fc77408524a",
   "metadata": {},
   "source": [
    "Note that while the `prompt` argument for `TLM.get_trustworthiness_score()` should only specify **what a correct response should look like**, the custom evaluation criteria should specify **how to determine if the response is good** (more tips below).\n",
    "\n",
    "After defining the additional evaluation criteria, running TLM's `get_trustworthiness_score()` method will return both TLM's standard trustworthiness scores along with a new custom evaluation score based on our criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c45eb9b2-c56d-4ccc-a06c-f35bf0062d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Querying TLM... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trustworthiness_score</th>\n",
       "      <th>log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.650516</td>\n",
       "      <td>{'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.1714374914049258}]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.987435</td>\n",
       "      <td>{'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.9975124355614093}]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.939139</td>\n",
       "      <td>{'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.25124689196087185}]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.189058</td>\n",
       "      <td>{'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.2605290450034299}]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.941208</td>\n",
       "      <td>{'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.3310500673501733}]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trustworthiness_score  \\\n",
       "0               0.650516   \n",
       "1               0.987435   \n",
       "2               0.939139   \n",
       "3               0.189058   \n",
       "4               0.941208   \n",
       "\n",
       "                                                                                 log  \n",
       "0   {'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.1714374914049258}]}  \n",
       "1   {'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.9975124355614093}]}  \n",
       "2  {'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.25124689196087185}]}  \n",
       "3   {'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.2605290450034299}]}  \n",
       "4   {'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.3310500673501733}]}  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_custom_eval = tlm_custom_eval.get_trustworthiness_score(data[\"improved_prompt\"].tolist(), data[\"answer\"].tolist())\n",
    "res_custom_eval_df = pd.DataFrame(res_custom_eval)\n",
    "res_custom_eval_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2832e18-dfab-43f3-8b3f-76ca9f22142a",
   "metadata": {},
   "source": [
    "The custom evaluation scores will be returned as entries in the `log` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b815ab68-9596-4f62-9a2e-6dd3b35d1668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'custom_eval_criteria': [{'name': 'Conciseness',\n",
       "   'score': 0.1714374914049258}]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_custom_eval_df[\"log\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0428867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.171437\n",
       "1    0.997512\n",
       "2    0.251247\n",
       "3    0.260529\n",
       "4    0.331050\n",
       "Name: custom_score, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res_custom_eval_df[\"custom_score\"] = res_custom_eval_df.apply(lambda x: x[\"log\"][\"custom_eval_criteria\"][0][\"score\"], axis=1)\n",
    "res_custom_eval_df[\"custom_score\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebe4a1d-54ae-4cd3-a2e9-aaa0034d2712",
   "metadata": {},
   "source": [
    "Let's combine the original data with these results to see how well the our conciseness criteria scored the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3fbcad33-d022-4d8d-9e45-6d30b80142fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_eval_combined = pd.concat([data, res_custom_eval_df], axis=1)\n",
    "custom_eval_sorted = custom_eval_combined.sort_values(\"custom_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f7490d-1f54-450e-af1e-c38c0f9d871a",
   "metadata": {},
   "source": [
    "Below, we view 2 examples with the lowest conciseness scores across our dataset, and see that the LLM responses are indeed very verbose. We see that in the first example, TLM's trustworthiness score did not actually reflect the verboseness of the response.\n",
    "\n",
    "This showcases how specifying custom evaluation criteria can help in use-cases where TLM's general trustworthiness scores do not reflect your notion of good-vs-bad quality responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1c8147d-2766-4f55-b3b1-26b4f8cd7f91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>trustworthiness_score</th>\n",
       "      <th>custom_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>What was the name of Michael Jackson's autobiography written in 1988?</td>\n",
       "      <td>The title of Michael Jackson's autobiography, written and published in 1988, is 'Moonwalk'. In this deeply personal and revealing book, Jackson reflects on his extraordinary life, starting from his early days as a child prodigy with the Jackson 5 to his unprecedented rise as a global pop sensation. The book delves into his experiences with fame, his artistic inspirations, and the personal challenges he faced throughout his career, offering readers an intimate glimpse into the mind of the King of Pop.</td>\n",
       "      <td>0.664936</td>\n",
       "      <td>0.002911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>For which movie did Katharine Hepburn win her second Oscar?</td>\n",
       "      <td>Katharine Hepburn won her second Academy Award for her role in the 1967 film 'Guess Who's Coming to Dinner'. In this acclaimed film, she portrayed Christina Drayton, a mother grappling with her daughter's decision to marry a Black man. Hepburn\u2019s poignant and emotionally charged performance earned her critical praise and solidified her place as one of Hollywood's all-time great actresses. The film, which addresses issues of race and social change, was significant in its time, and Hepburn's portrayal is a key element of its impact.</td>\n",
       "      <td>0.711409</td>\n",
       "      <td>0.003500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 question  \\\n",
       "29  What was the name of Michael Jackson's autobiography written in 1988?   \n",
       "14            For which movie did Katharine Hepburn win her second Oscar?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     answer  \\\n",
       "29                                The title of Michael Jackson's autobiography, written and published in 1988, is 'Moonwalk'. In this deeply personal and revealing book, Jackson reflects on his extraordinary life, starting from his early days as a child prodigy with the Jackson 5 to his unprecedented rise as a global pop sensation. The book delves into his experiences with fame, his artistic inspirations, and the personal challenges he faced throughout his career, offering readers an intimate glimpse into the mind of the King of Pop.   \n",
       "14  Katharine Hepburn won her second Academy Award for her role in the 1967 film 'Guess Who's Coming to Dinner'. In this acclaimed film, she portrayed Christina Drayton, a mother grappling with her daughter's decision to marry a Black man. Hepburn\u2019s poignant and emotionally charged performance earned her critical praise and solidified her place as one of Hollywood's all-time great actresses. The film, which addresses issues of race and social change, was significant in its time, and Hepburn's portrayal is a key element of its impact.   \n",
       "\n",
       "    trustworthiness_score  custom_score  \n",
       "29               0.664936      0.002911  \n",
       "14               0.711409      0.003500  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_eval_sorted.head(2)[[\"question\", \"answer\", \"trustworthiness_score\", \"custom_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16a45df-ae8c-43c6-aea9-b0624bb716ec",
   "metadata": {},
   "source": [
    "Inspecting responses with the highest conciseness scores in our dataset, we see that these are indeed nice and concise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b199e17-92d5-4204-9f7c-c70cb777a024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>trustworthiness_score</th>\n",
       "      <th>custom_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>In the run up to the 1997 election, Tony Blair outlined his parties' three priorities for office, can you name them?</td>\n",
       "      <td>Tony Blair's three priorities for office in the run up to the 1997 election were education, education, education.</td>\n",
       "      <td>0.885939</td>\n",
       "      <td>0.997512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Marc Dutroux hit the headlines over a 'house of horrors' in which country?</td>\n",
       "      <td>Marc Dutroux hit the headlines over a 'house of horrors' in Belgium.</td>\n",
       "      <td>0.952093</td>\n",
       "      <td>0.997512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                question  \\\n",
       "32  In the run up to the 1997 election, Tony Blair outlined his parties' three priorities for office, can you name them?   \n",
       "34                                            Marc Dutroux hit the headlines over a 'house of horrors' in which country?   \n",
       "\n",
       "                                                                                                               answer  \\\n",
       "32  Tony Blair's three priorities for office in the run up to the 1997 election were education, education, education.   \n",
       "34                                               Marc Dutroux hit the headlines over a 'house of horrors' in Belgium.   \n",
       "\n",
       "    trustworthiness_score  custom_score  \n",
       "32               0.885939      0.997512  \n",
       "34               0.952093      0.997512  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_eval_sorted.tail(2)[[\"question\", \"answer\", \"trustworthiness_score\", \"custom_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5eb30d-ec2b-490f-a0d9-953cdfcf408b",
   "metadata": {},
   "source": [
    "TLM's custom evaluation criteria offer tailored **guardrails** to maximize the safety/policy-adherence of your LLM applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7c4cb1-83bd-451f-b6dc-8945a2b0528d",
   "metadata": {},
   "source": [
    "<details><summary> Tips for writing custom evaluation criteria <b>(click to expand)</b></summary>\n",
    "\n",
    "You can iterate on how you write out the custom evaluation criteria, in order to better align the custom evaluation scores with what you consider good-vs-bad quality responses.\n",
    "Tips include: define clear/objective criteria for determining answer quality and avoid subjective language. Consider including: examples of good vs bad responses, and edge-case guidelines -- such as whether things like flowery-language or answer-refusal are considered good or bad.\n",
    "Qualitatively describe measurable aspects of the response **without describing numerical scoring mechanisms**, as TLM already has an internal scoring system based on your qualitative description.\n",
    "\n",
    "\n",
    "**Examples of custom evaluation criteria:**\n",
    "\n",
    "**In customer support applications:** a custom criteria for *Company XYZ* to evaluate the brand-safety of their AI might include:\n",
    "\n",
    "> *Determine whether the response portrays Company XYZ in a positive or neutral light. A bad response mentions negative aspects of Company XYZ or their products without caveats, or mentions competitors of Company XYZ. A great response does not contain statements which could be perceived as potentially negative regarding Company XYZ or might negatively impact Company XYZ.*\n",
    "\n",
    "\n",
    "**In customer support applications:** a custom criteria for *Company XYZ* to evaluate the policy-adherence of their AI might include:\n",
    "\n",
    "> *Determine whether the response strictly adheres to the following Policy Guidelines. A bad response fails to follow one of the guidelines in the policy. A great response does not contain any policy violations, perhaps because the policy is not particularly relevant to this interaction. &#92;n&#92;n &num;&num; Policy Guidelines &#92;n&#92;n {your_policy_document_here}*\n",
    "\n",
    "\n",
    "**In conversational chat applications:** you may not want to rely on TLM's trustworthiness score for every AI response, but rather only for verifiable statements that convey information. You can specify a custom evaluation criteria like the following:\n",
    "\n",
    "> *Determine whether the response is non-propositional, in which case it is great. Otherwise it is a bad response if it conveys any specific information or facts, or otherwise seems like an answer whose accuracy could matter.*\n",
    "\n",
    "And then you could only consider TLM's trustworthiness score for those responses whose custom evaluation score is low.\n",
    "\n",
    "**Additional Use Cases for Custom Evaluation Criteria:**\n",
    "\n",
    "**For financial compliance:** a custom criteria to evaluate regulatory adherence might include:\n",
    "\n",
    "> *Strictly evaluate if the response follows these financial advice regulations:*\n",
    ">\n",
    "> *1. Includes necessary disclaimers*\n",
    ">\n",
    "> *2. Avoids specific investment recommendations*\n",
    ">\n",
    "> *3. Explains risks alongside potential benefits*\n",
    ">\n",
    "> *4. Uses clear, non-misleading language*\n",
    ">\n",
    "> *5. Encourages consultation with financial advisors*\n",
    ">\n",
    "> *A great response meets criteria in all categories. A mediocre response meets most criteria but lacks completeness. A bad response fails multiple criteria or has serious issues in any category.*\n",
    "\n",
    "**For code quality evaluation:** a custom criteria to evaluate code safety and maintainability might include:\n",
    "\n",
    "> *Strictly evaluate if the code follows these quality requirements:*\n",
    ">\n",
    "> *1. Error Handling: Validates inputs, handles edge cases, uses specific error messages* \n",
    ">\n",
    "> *2. Documentation: Includes docstring, explains complex logic, documents assumptions*\n",
    ">\n",
    "> *3. Security: Prevents common vulnerabilities, validates inputs, follows security best practices*\n",
    ">\n",
    "> *4. Maintainability: Uses descriptive names, consistent formatting, modular functions*\n",
    ">\n",
    "> *A great response meets criteria in all categories. A mediocre response meets most criteria but lacks completeness. A bad response fails multiple criteria or has serious issues in any category.*\n",
    "\n",
    "**In educational content:** a custom criteria to evaluate pedagogical effectiveness might include:\n",
    "\n",
    "> *Strictly evaluate if the response meets these educational criteria:*\n",
    ">\n",
    "> *1. Provides clear learning objectives upfront*\n",
    ">\n",
    "> *2. Uses structured progression from basic to complex*\n",
    ">\n",
    "> *3. Includes concrete examples or analogies*\n",
    ">\n",
    "> *4. Anticipates and addresses common misconceptions*\n",
    ">\n",
    "> *5. Uses precise terminology with appropriate definitions*\n",
    ">\n",
    "> *A great response meets criteria in all categories. A mediocre response meets most criteria but lacks completeness. A bad response fails multiple criteria or has serious issues in any category.*\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9bc22f-d1de-4973-81d8-811e484161f1",
   "metadata": {},
   "source": [
    "## Calibrating TLM scores against human quality ratings\n",
    "\n",
    "TLM's native trustworthiness scoring and custom evaluation criteria each have their strengths. By combining them, we can benefit from TLM's general trustworthiness score while tailoring specific aspects of the evaluation to our specific criteria. None of the above approaches requires human-rating annotations, we have so far just used those to quantify alignment between the scores and human ratings.\n",
    "\n",
    "Here we showcase how `TLMCalibrated` can combine and calibrate these values into a score that is explicitly aligned against numeric/binary human-rating (i.e. ground-truth) quality annotations. While we here integrate the custom evaluation and trustworthiness scores into a single calibrated score, you can also run the same procedure to calibrate just one of these scores without the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad040ff9-c922-47a8-bed2-25b795b5cf24",
   "metadata": {},
   "source": [
    "First, we instantiate the `TLMCalibrated` object with the same custom evaluation criteria used previously in the `options` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4624b05b-9242-4054-9711-1abdddad390e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab_tlm import TLMCalibrated\n",
    "tlm_calibrated = TLMCalibrated(options=custom_eval_criteria_option)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7eb910-8d94-4e28-82a6-48404da483cc",
   "metadata": {},
   "source": [
    "Next, we **fit** this `TLMCalibrated` object to a dataset composed of the previously obtained TLM scores and human quality ratings. This trains the model to produce better-aligned scores over this dataset.\n",
    "\n",
    "We can directly pass `res_custom_eval` (the list of dictionaries returned from the previous `get_trustworthiness_score()` call) directly into `fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5cd04dff-a111-4e06-99a2-29580b3857a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tlm_calibrated.fit(res_custom_eval, data[\"human_rating\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693f493a-0a6c-4452-a306-b73cc13e05e4",
   "metadata": {},
   "source": [
    "After fitting `TLMCalibrated` to our human-annotated dataset, we can call its `get_trustworthiness_score()` method. This returns not only the previous TLM trustworthiness score and custom conciseness score, but also an additional calibrated score that optimally combines those two scores to align with human quality ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "444456d6-1dc7-45cb-87c5-99ebab0a5969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Querying TLM... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|\n"
     ]
    }
   ],
   "source": [
    "calibrated_res = tlm_calibrated.get_trustworthiness_score(data[\"improved_prompt\"].tolist(), data[\"answer\"].tolist())\n",
    "calibrated_res_df = pd.DataFrame(calibrated_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8e67547-df9a-4537-89b6-285f1336dfd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trustworthiness_score</th>\n",
       "      <th>log</th>\n",
       "      <th>calibrated_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.650516</td>\n",
       "      <td>{'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.1714374914049258}]}</td>\n",
       "      <td>0.140863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.987435</td>\n",
       "      <td>{'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.9975124355614093}]}</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.939139</td>\n",
       "      <td>{'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.25124689196087185}]}</td>\n",
       "      <td>0.796112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.189058</td>\n",
       "      <td>{'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.2605290450034299}]}</td>\n",
       "      <td>0.221944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.941208</td>\n",
       "      <td>{'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.3310500673501733}]}</td>\n",
       "      <td>0.850461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trustworthiness_score  \\\n",
       "0               0.650516   \n",
       "1               0.987435   \n",
       "2               0.939139   \n",
       "3               0.189058   \n",
       "4               0.941208   \n",
       "\n",
       "                                                                                 log  \\\n",
       "0   {'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.1714374914049258}]}   \n",
       "1   {'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.9975124355614093}]}   \n",
       "2  {'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.25124689196087185}]}   \n",
       "3   {'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.2605290450034299}]}   \n",
       "4   {'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.3310500673501733}]}   \n",
       "\n",
       "   calibrated_score  \n",
       "0          0.140863  \n",
       "1          1.000000  \n",
       "2          0.796112  \n",
       "3          0.221944  \n",
       "4          0.850461  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calibrated_res_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fddd249-eff5-49be-937f-9187dbb00c94",
   "metadata": {},
   "source": [
    "We again measure alignment between the calibrated scores and our human ratings via their Spearman correlation, which here is the highest among all scoring methods. Thus whenever you have a moderately-sized set of human quality ratings, we recommend considering `TLMCalibrated` to produce a tailored evaluation score for LLM responses in your use-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "118f947a-adb4-4576-a3ac-8d76be2f42de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation: 0.7167231544441708\n"
     ]
    }
   ],
   "source": [
    "print(\"Spearman correlation:\", spearmanr(calibrated_res_df[\"calibrated_score\"], data[\"human_rating\"]).statistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddc060a-9a90-4c2f-a772-66b2ff416cdd",
   "metadata": {},
   "source": [
    "### Using `TLMCalibrated` to get improved scores on new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358f072e-2a69-41ad-bbe5-40b43465afa6",
   "metadata": {},
   "source": [
    "You can use the already-fitted `TLMCalibrated` model to score new responses via the same `get_trustworthiness_score()` method. Here we demonstrate this with some additional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f06d607-72a7-4afd-a48c-043fe7e2a4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_csv(\"triviaqa_without_ratings.csv\")\n",
    "new_data[\"prompt\"] = new_data[\"question\"].apply(create_improved_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec4a5ac-947e-489b-a43b-4bea1651e73e",
   "metadata": {},
   "source": [
    "We call `get_trustworthiness_score()` on the new (prompt, response) pairs, just as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb61511e-a5ec-4586-aad3-01612f527d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Querying TLM... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trustworthiness_score</th>\n",
       "      <th>log</th>\n",
       "      <th>calibrated_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.972855</td>\n",
       "      <td>{'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.9975124358564608}]}</td>\n",
       "      <td>0.925461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.711186</td>\n",
       "      <td>{'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.01743449151079022}]}</td>\n",
       "      <td>0.182863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trustworthiness_score  \\\n",
       "0               0.972855   \n",
       "1               0.711186   \n",
       "\n",
       "                                                                                 log  \\\n",
       "0   {'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.9975124358564608}]}   \n",
       "1  {'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.01743449151079022}]}   \n",
       "\n",
       "   calibrated_score  \n",
       "0          0.925461  \n",
       "1          0.182863  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data_res = tlm_calibrated.get_trustworthiness_score(new_data[\"prompt\"].tolist(), new_data[\"answer\"].tolist())\n",
    "new_data_res_df = pd.DataFrame(new_data_res)\n",
    "new_data_res_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194ec92a-db7a-46a0-bc78-bf7bd7751ac0",
   "metadata": {},
   "source": [
    "The returned results contain the TLM trustworthiness score and `log` (like before), but also the `calibrated_score` to use for evaluating these additional responses (in real-time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "43eafdd9-67f8-459e-abcc-159eac744a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>calibrated_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1976, which gymnast scored 7 maximum scores of 10 as she won three gold medals, one silver and one bronze?</td>\n",
       "      <td>Nadia Comaneci</td>\n",
       "      <td>0.925461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the Milky Way?</td>\n",
       "      <td>The Milky Way is a barred spiral galaxy, one of the billions of galaxies in the observable universe, and it is our home galaxy. As part of the Local Group of galaxies, it exists alongside other notable galaxies like Andromeda and Triangulum, as well as about 54 smaller satellite galaxies. The Milky Way contains between 100 and 400 billion stars, including our Sun, and the dense band of light seen from Earth at night is caused by the multitude of stars and celestial objects aligned along the galactic plane. It is named after the appearance of this bright band of light, which stretches across the night sky.</td>\n",
       "      <td>0.182863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                        question  \\\n",
       "0  In 1976, which gymnast scored 7 maximum scores of 10 as she won three gold medals, one silver and one bronze?   \n",
       "1                                                                                         What is the Milky Way?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 answer  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Nadia Comaneci   \n",
       "1  The Milky Way is a barred spiral galaxy, one of the billions of galaxies in the observable universe, and it is our home galaxy. As part of the Local Group of galaxies, it exists alongside other notable galaxies like Andromeda and Triangulum, as well as about 54 smaller satellite galaxies. The Milky Way contains between 100 and 400 billion stars, including our Sun, and the dense band of light seen from Earth at night is caused by the multitude of stars and celestial objects aligned along the galactic plane. It is named after the appearance of this bright band of light, which stretches across the night sky.   \n",
       "\n",
       "   calibrated_score  \n",
       "0          0.925461  \n",
       "1          0.182863  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data_combined = pd.concat([new_data, new_data_res_df], axis=1)\n",
    "new_data_combined.head(2)[[\"question\", \"answer\", \"calibrated_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7c4cb1-83bd-451f-b6dc-8945a2b0528d",
   "metadata": {},
   "source": [
    "<details><summary> Tips for further scoring improvements <b>(click to expand)</b></summary>\n",
    "\n",
    "At this point, you have specified custom evaluation criteria and combined it with TLM's native trustworthiness score to produce calibrated scores via `TLMCalibrated`. To further align these scores against your human-quality ratings, you can tinker with the prompts used in various arguments of `TLMCalibrated`.\n",
    "\n",
    "Try jointly improving the text used for both the `prompt` argument and the custom evaluation criteria. Iterate through many alternative versions of this text, trying to maximize the resulting Spearman correlation between the resulting calibrated scores from `TLMCalibrated` and human-quality ratings.\n",
    "\n",
    "Tips on tuning both of these include:\n",
    "\n",
    "- The base TLM prompt will perform better in general correctness evaluation, however you can still be specific about certain guidelines that it can follow.\n",
    "- For example in RAG use cases: if the correct response to a question is \"The given context does not contain any relevant information to answer the question\" (because the retrieved context lacks the necessary details), you should explicitly specify this in the prompt. This ensures that TLM recognizes such responses as correct and does not score them low simply because they do not provide a direct answer.\n",
    "- The custom evaluation criteria is better suited for domain-specific needs beyond a general correctness check. You can specify these custom evaluation criteria to complement the general trustworthiness scoring that the base TLM provides.\n",
    "\n",
    "\n",
    "By systematically adjusting both the prompt and evaluation criteria, you can improve `TLMCalibrated` scores to suite your specific use-case.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856d4adc",
   "metadata": {},
   "source": [
    "<details><summary> Thresholding TLMCalibrated scores to classify responses as good vs. bad <b>(click to expand)</b></summary>\n",
    "\n",
    "To choose a score threshold that best separates High-vs-Low quality responses, follow these steps:\n",
    "\n",
    "- `Data Splitting`: Divide your labeled dataset into training and validation sets.\n",
    "- `Threshold Selection`: Fit TLMCalibrated on the training set. Evaluate various threshold values on the held-out validation set to see which value best separates High-vs-Low quality responses (can compute an evaluation metric like accuracy).\n",
    "- `Final Training`: Optionally, refit your TLMCalibrated model on all of your labeled data (training + validation) once you have selected the threshold to use when scoring future responses.\n",
    "\n",
    "**Note:** For brevity, this tutorial evaluated our TLMCalibrated scores without proper data splitting. Use held\u2011out data in practice to avoid overfitting.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c648699",
   "metadata": {},
   "source": [
    "<details><summary> Saving and loading TLMCalibrated objects <b>(click to expand)</b></summary>\n",
    "\n",
    "After fitting your `TLMCalibrated` object, you can save it for later use by loading it back in. Here's how to do it properly using the available [helper functions](/tlm/api/python/utils.tlm_calibrated):\n",
    "\n",
    "```python\n",
    "from cleanlab_tlm.utils.tlm_calibrated import save_tlm_calibrated_state, load_tlm_calibrated_state\n",
    "\n",
    "# Save the TLMCalibrated state\n",
    "save_tlm_calibrated_state(tlm_calibrated, 'tlm_calibrated_state.skops')\n",
    "\n",
    "# Load the TLMCalibrated state\n",
    "tlm_calibrated = load_tlm_calibrated_state('tlm_calibrated_state.skops')\n",
    "```\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}