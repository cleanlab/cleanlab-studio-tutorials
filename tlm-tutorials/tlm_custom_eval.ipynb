{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a994c8b5-cc71-4a88-bae1-c6c8f6567870",
   "metadata": {},
   "source": [
    "# Incorporating custom evaluation criteria for LLM outputs and calibrating TLM trustworthiness scores against human ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceebdba3-b6ad-4e21-9451-4315ea7cfb04",
   "metadata": {},
   "source": [
    "Are you finding TLM trustworthiness scores do not align with your team's manual quality ratings of good/bad LLM outputs? If so, you can address this via:\n",
    "- Adjusting prompts and TLM options/settings\n",
    "- Custom evaluation criteria\n",
    "- Calibrating TLM trustworthiness scores against your human quality ratings for example prompt-response pairs.\n",
    "\n",
    "This tutorial demonstrates how to evaluate and improve the performance of TLM for your specific use-case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a386ec-152b-45bd-af07-fa854602db82",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Using TLM requires a [Cleanlab](https://app.cleanlab.ai/) account. Sign up for one [here](https://cleanlab.ai/signup/) if you haven't yet. If you've already signed up, check your email for a personal login link.\n",
    "\n",
    "Cleanlab's Python client can be installed using pip and a `Studio` object can be instantiated with your API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b970467b-5afd-4bd4-a2d9-33e6eda054c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade cleanlab-studio scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22dbe0c8-f6b2-4ef4-af19-4f96efdfe292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import spearmanr  # just used for evaluation of results in this tutorial\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41256d78-d8f8-463d-967f-5d763d828020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab_studio import Studio\n",
    "\n",
    "# Get API key from here: https://app.cleanlab.ai/account after creating an account.\n",
    "studio = Studio(\"<API key>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a45c23a-afa5-4a23-90b5-696602b3804d",
   "metadata": {},
   "source": [
    "## Fetch the dataset\n",
    "\n",
    "Let's first load an example dataset. Here we consider a dataset composed of trivia questions with responses already computed from some LLM, which we want to automatically evaluate with TLM.\n",
    "\n",
    "In this tutorial we will be using the `tlm.get_trustworthiness_score()` method to evaluate these prompt-response pairs, and explore how we can use various techniques to calibrate the trustworthiness score produced by the TLM to align with your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e29aa20-e4c4-49df-a989-429319799641",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://cleanlab-public.s3.amazonaws.com/Datasets/triviaqa-conciseness/triviaqa_with_ratings.csv\n",
    "!wget -nc https://cleanlab-public.s3.amazonaws.com/Datasets/triviaqa-conciseness/triviaqa_without_ratings.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0e76435-4f5a-4384-a871-724f3c28d2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"triviaqa_with_ratings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54d5df1-5daf-4522-a447-bb81a32d4088",
   "metadata": {},
   "source": [
    "Suppose we aim to score LLM responses based on both correctness of the response and conciseness (ie. the LLM response should not contain extraneous information). Our dataset additionally contains human ratings regarding the (manually-determined) quality of each response. Even without having such human ratings, you can still use many of the techniques demonstrated in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebeed0a-2c52-4fcf-8f1e-849e508c3d8f",
   "metadata": {},
   "source": [
    "Here is a sample question where the LLM response is factually correct, however it contains too much irrelevant information, and hence it is marked as incorrect by a human annotator (rating = 0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87ae4f59-9822-4697-8e40-b872b4d47fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Bonar Law is the only Prime Minister not born in the UK. In which country was he born? \n",
      "\n",
      "Response: Bonar Law, who briefly served as British Prime Minister in the early 1920s, holds the distinction of being the only Prime Minister born outside the United Kingdom. He was born in Canada, specifically in the province of New Brunswick. This unique aspect of his background adds an interesting layer to his political legacy, as he later rose to lead the UK government during a particularly tumultuous period in British history. \n",
      "\n",
      "Human Rating: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Question:\", data[\"question\"][0], \"\\n\")\n",
    "print(\"Response:\", data[\"answer\"][0], \"\\n\")\n",
    "print(\"Human Rating:\", data[\"human_rating\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e9e236-4d9b-4216-84fe-8ada972cb618",
   "metadata": {},
   "source": [
    "The LLM response to this other question is both correct and concise, hence it has been manually marked as high-quality (human rating = 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "697e34a8-c2ca-4084-bd3b-f9dd6a496885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which singer had a big 60s No 1 with Roses Are Red? \n",
      "\n",
      "Response: Bobby Vinton \n",
      "\n",
      "Human Rating: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Question:\", data[\"question\"][1], \"\\n\")\n",
    "print(\"Response:\", data[\"answer\"][1], \"\\n\")\n",
    "print(\"Human Rating:\", data[\"human_rating\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2308110d-f602-432f-97fe-7776b01d81d1",
   "metadata": {},
   "source": [
    "## Scoring responses with TLM\n",
    "\n",
    "At first, let's just append a simple prompt template to each question, and then use `get_trustworthiness_score()` to score each response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67b7264a-7561-4313-91fe-f07dfe7ac47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prompt:\n",
      "Answer this question as concisely as possible: Bonar Law is the only Prime Minister not born in the UK. In which country was he born?\n"
     ]
    }
   ],
   "source": [
    "def create_prompt(question):\n",
    "    return f\"Answer this question as concisely as possible: {question}\"\n",
    "    \n",
    "data[\"prompt\"] = data[\"question\"].apply(create_prompt)\n",
    "print(\"Sample prompt:\")\n",
    "print(data[\"prompt\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e61e8914-0122-4ba3-9e5a-82b79b4bf7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Querying TLM... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|\n"
     ]
    }
   ],
   "source": [
    "tlm = studio.TLM()\n",
    "res = tlm.get_trustworthiness_score(data[\"prompt\"].tolist(), data[\"answer\"].tolist())\n",
    "res_df = pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fae002-0ed7-408f-b747-e687b86aabb9",
   "metadata": {},
   "source": [
    "Spearman's rank correlation is a statistical method used to measure the strength of the relationship between two ranked variables. We can use it here to measure how well the trustworthiness scores from TLM reflect the given human ratings.\n",
    "\n",
    "In this tutorial, the human ratings are binary (only 0 and 1 values representing correct and incorrect responses). However, all methods showcased in this tutorial can be applied to any range of numeric human ratings, such as ratings on a scale of 1 to 5. Spearman's rank correlation is a valid measure of alignment betweeen estimated trustworthiness scores and human ratings, regardless whether they are binary or numeric, as it quantifies how consistently good responses are scored higher than bad ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b38ff09a-74c0-4553-b8a2-dc50fa71e3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation: 0.5201940200004417\n"
     ]
    }
   ],
   "source": [
    "print(\"Spearman correlation:\", spearmanr(res_df[\"trustworthiness_score\"], data[\"human_rating\"]).statistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8b6800-6028-421a-83b5-bf74a921da46",
   "metadata": {},
   "source": [
    "## Prompt engineering for better TLM trustworthiness scoring\n",
    "\n",
    "One simple way to improve TLM performance is to improve the instructions we give. With clearer instructions, TLM can better score the responses for our specific use-case. Important elements of an effective `prompt` argument for TLM include: specifying precisely what a **correct answer** should contain, and perhaps also giving examples of wrong answers.\n",
    "\n",
    "Here for example, we can update TLM's `prompt` to specify that wordy answers are considered incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c5d4b62-8538-4b1b-9fd8-e51095d49707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample improved prompt:\n",
      "Answer the following question as concisely as you can. \n",
      "For the answer to be considered correct, it has to be concise. Wordy answers are incorrect.\n",
      "Question: Bonar Law is the only Prime Minister not born in the UK. In which country was he born?\n"
     ]
    }
   ],
   "source": [
    "def create_improved_prompt(question):\n",
    "    return f\"\"\"Answer the following question as concisely as you can. \n",
    "For the answer to be considered correct, it has to be concise. Wordy answers are incorrect.\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "data[\"improved_prompt\"] = data[\"question\"].apply(create_improved_prompt)\n",
    "print(\"Sample improved prompt:\")\n",
    "print(data[\"improved_prompt\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "939154f8-0847-44e5-aa51-e3ba38e92b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Querying TLM... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|\n"
     ]
    }
   ],
   "source": [
    "res_improved = tlm.get_trustworthiness_score(data[\"improved_prompt\"].tolist(), data[\"answer\"].tolist())\n",
    "res_improved = pd.DataFrame(res_improved)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496b3ca6-1d23-483c-ab36-00c88b327c49",
   "metadata": {},
   "source": [
    "We see that with the updated prompt, TLM's trustworthiness score is now better aligned with our human ratings (as measured via their Spearman correlation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd859cd5-ee6f-496b-96aa-3b0a034e1d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation: 0.5941771695116157\n"
     ]
    }
   ],
   "source": [
    "print(\"Spearman correlation:\", spearmanr(res_improved[\"trustworthiness_score\"], data[\"human_rating\"]).statistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7c4cb1-83bd-451f-b6dc-8945a2b0528c",
   "metadata": {},
   "source": [
    "## Custom evaluation criteria\n",
    "\n",
    "Our improved prompt template did enhance TLM performance. However, evaluating conciseness of the response represents a particular notion of response-quality that may not align with TLM's general trustworthiness scoring (which aims to quantify uncertainty in the LLM). Fortunately, TLM supports *custom evaluation criteria*, allowing you to define specific ways to additionally evaluate response-quality.\n",
    "\n",
    "We can define our custom evaluation criteria in TLM's `options` dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b067111-6a5e-4d9f-a3c5-b44229aeac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_eval_criteria_option = {\"custom_eval_criteria\": [\n",
    "    {\"name\": \"Conciseness\", \"criteria\": \"Determine if the output is concise.\"}\n",
    "]}\n",
    "\n",
    "tlm_custom_eval = studio.TLM(options=custom_eval_criteria_option)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34418261-caad-47c0-8ef9-8fc77408524a",
   "metadata": {},
   "source": [
    "Note that while the `prompt` for `TLM.get_trustworthiness_score()` should only specify **what a correct answer should look like**, the custom evaluation criteria should specify **how to determine if the answer is good** (more tips below).\n",
    "\n",
    "After defining the additional evaluation criteria, running TLM's `get_trustworthiness_score()` method will return both TLM's standard trustworthiness scores along with a new custom evaluation score based on our criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c45eb9b2-c56d-4ccc-a06c-f35bf0062d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Querying TLM... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trustworthiness_score</th>\n",
       "      <th>log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.651190</td>\n",
       "      <td>{'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.017434491510790216}]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.990492</td>\n",
       "      <td>{'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.997512434971306}]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.933385</td>\n",
       "      <td>{'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.2512370425791872}]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.225192</td>\n",
       "      <td>{'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.25034993649245585}]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.941208</td>\n",
       "      <td>{'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.25271347417799767}]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trustworthiness_score  \\\n",
       "0               0.651190   \n",
       "1               0.990492   \n",
       "2               0.933385   \n",
       "3               0.225192   \n",
       "4               0.941208   \n",
       "\n",
       "                                                                                  log  \n",
       "0  {'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.017434491510790216}]}  \n",
       "1     {'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.997512434971306}]}  \n",
       "2    {'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.2512370425791872}]}  \n",
       "3   {'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.25034993649245585}]}  \n",
       "4   {'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.25271347417799767}]}  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_custom_eval = tlm_custom_eval.get_trustworthiness_score(data[\"improved_prompt\"].tolist(), data[\"answer\"].tolist())\n",
    "res_custom_eval_df = pd.DataFrame(res_custom_eval)\n",
    "res_custom_eval_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2832e18-dfab-43f3-8b3f-76ca9f22142a",
   "metadata": {},
   "source": [
    "The custom evaluation scores will be returned as entries in the `log` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b815ab68-9596-4f62-9a2e-6dd3b35d1668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'custom_eval_criteria': [{'name': 'Conciseness',\n",
       "   'score': 0.017434491510790216}]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_custom_eval_df[\"log\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "080831bb-bd08-4b50-bf76-aba202e4976c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.017434\n",
       "1    0.997512\n",
       "2    0.251237\n",
       "3    0.250350\n",
       "4    0.252713\n",
       "Name: custom_score, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_custom_eval_df[\"custom_score\"] = res_custom_eval_df.apply(lambda x: x[\"log\"][\"custom_eval_criteria\"][0][\"score\"], axis=1)\n",
    "res_custom_eval_df[\"custom_score\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebe4a1d-54ae-4cd3-a2e9-aaa0034d2712",
   "metadata": {},
   "source": [
    "Let's combine the original data with these results to see how well the our conciseness criteria scored the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3fbcad33-d022-4d8d-9e45-6d30b80142fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_eval_combined = pd.concat([data, res_custom_eval_df], axis=1)\n",
    "custom_eval_sorted = custom_eval_combined.sort_values(\"custom_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f7490d-1f54-450e-af1e-c38c0f9d871a",
   "metadata": {},
   "source": [
    "Below, we view 2 examples with the lowest conciseness scores across our dataset, and see that the LLM responses are indeed very verbose. We see that in the first example, the TLM's trustworthiness score did not actually reflect the verboseness of the response.\n",
    "\n",
    "This showcases how specifying custom evaluation criteria can help in use-cases where TLM's general trustworthiness scores do not reflect your notion of good-vs-bad quality responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1c8147d-2766-4f55-b3b1-26b4f8cd7f91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>trustworthiness_score</th>\n",
       "      <th>custom_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>What was the name of Michael Jackson's autobiography written in 1988?</td>\n",
       "      <td>The title of Michael Jackson's autobiography, written and published in 1988, is 'Moonwalk'. In this deeply personal and revealing book, Jackson reflects on his extraordinary life, starting from his early days as a child prodigy with the Jackson 5 to his unprecedented rise as a global pop sensation. The book delves into his experiences with fame, his artistic inspirations, and the personal challenges he faced throughout his career, offering readers an intimate glimpse into the mind of the King of Pop.</td>\n",
       "      <td>0.664943</td>\n",
       "      <td>0.002538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>For which movie did Katharine Hepburn win her second Oscar?</td>\n",
       "      <td>Katharine Hepburn won her second Academy Award for her role in the 1967 film 'Guess Who's Coming to Dinner'. In this acclaimed film, she portrayed Christina Drayton, a mother grappling with her daughter's decision to marry a Black man. Hepburn\u2019s poignant and emotionally charged performance earned her critical praise and solidified her place as one of Hollywood's all-time great actresses. The film, which addresses issues of race and social change, was significant in its time, and Hepburn's portrayal is a key element of its impact.</td>\n",
       "      <td>0.711381</td>\n",
       "      <td>0.003786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 question  \\\n",
       "29  What was the name of Michael Jackson's autobiography written in 1988?   \n",
       "14            For which movie did Katharine Hepburn win her second Oscar?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     answer  \\\n",
       "29                                The title of Michael Jackson's autobiography, written and published in 1988, is 'Moonwalk'. In this deeply personal and revealing book, Jackson reflects on his extraordinary life, starting from his early days as a child prodigy with the Jackson 5 to his unprecedented rise as a global pop sensation. The book delves into his experiences with fame, his artistic inspirations, and the personal challenges he faced throughout his career, offering readers an intimate glimpse into the mind of the King of Pop.   \n",
       "14  Katharine Hepburn won her second Academy Award for her role in the 1967 film 'Guess Who's Coming to Dinner'. In this acclaimed film, she portrayed Christina Drayton, a mother grappling with her daughter's decision to marry a Black man. Hepburn\u2019s poignant and emotionally charged performance earned her critical praise and solidified her place as one of Hollywood's all-time great actresses. The film, which addresses issues of race and social change, was significant in its time, and Hepburn's portrayal is a key element of its impact.   \n",
       "\n",
       "    trustworthiness_score  custom_score  \n",
       "29               0.664943      0.002538  \n",
       "14               0.711381      0.003786  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_eval_sorted.head(2)[[\"question\", \"answer\", \"trustworthiness_score\", \"custom_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16a45df-ae8c-43c6-aea9-b0624bb716ec",
   "metadata": {},
   "source": [
    "Inspecting responses with the highest conciseness scores in our dataset, we see that these are indeed nice and concise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b199e17-92d5-4204-9f7c-c70cb777a024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>trustworthiness_score</th>\n",
       "      <th>custom_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Marc Dutroux hit the headlines over a 'house of horrors' in which country?</td>\n",
       "      <td>Marc Dutroux hit the headlines over a 'house of horrors' in Belgium.</td>\n",
       "      <td>0.946333</td>\n",
       "      <td>0.997512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Yalu river forms a sort of natural border between China and which of its neighbours?</td>\n",
       "      <td>The Yalu river forms a natural border between China and North Korea.</td>\n",
       "      <td>0.629238</td>\n",
       "      <td>0.997512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                    question  \\\n",
       "34                Marc Dutroux hit the headlines over a 'house of horrors' in which country?   \n",
       "9   The Yalu river forms a sort of natural border between China and which of its neighbours?   \n",
       "\n",
       "                                                                  answer  \\\n",
       "34  Marc Dutroux hit the headlines over a 'house of horrors' in Belgium.   \n",
       "9   The Yalu river forms a natural border between China and North Korea.   \n",
       "\n",
       "    trustworthiness_score  custom_score  \n",
       "34               0.946333      0.997512  \n",
       "9                0.629238      0.997512  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_eval_sorted.tail(2)[[\"question\", \"answer\", \"trustworthiness_score\", \"custom_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5eb30d-ec2b-490f-a0d9-953cdfcf408b",
   "metadata": {},
   "source": [
    "### Tips for writing custom evaluation criteria\n",
    "\n",
    "You can further iterate on how you write out the custom evaluation criteria, in order to better align the custom evaluation scores with what you consider good-vs-bad quality responses.\n",
    "Tips include: define clear/objective criteria for determining answer quality and avoid subjective language. Consider including: examples of good vs bad responses, and edge-case guidelines -- such as whether things like flowery-language or answer-refusal are considered good or bad.\n",
    "Qualitatively describe measurable aspects of the response **without describing numerical scoring mechanisms**, as TLM already has an internal scoring system based on your qualitative description.\n",
    "\n",
    "An example custom criteria to evaluate how well a RAG system abstains from responding might be:\n",
    "\n",
    "*Determine whether the answer is accurate based on the provided context. If the context does not provide information needed to answer the question, the answer should state 'Unable to respond based on available information.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9bc22f-d1de-4973-81d8-811e484161f1",
   "metadata": {},
   "source": [
    "## Calibrating TLM scores against human quality ratings\n",
    "\n",
    "TLM's native trustworthiness scoring and custom evaluation criteria each have their strengths. By combining them, we can benefit from the TLM's general trustworthiness score while tailoring specific aspects of the evaluation to our specific criteria. None of the above approaches requires human-rating annotations, we have so far just used those to quantify alignment between the scores and human ratings.\n",
    "\n",
    "Here we showcase how `TLMCalibrated` can combine and calibrate these values into a score that is explicitly aligned against numeric/binary human-rating (i.e. ground-truth) quality annotations. While we here integrate the custom evaluation and trustworthiness scores into a single calibrated score, you can also run the same procedure to calibrate just one of these scores without the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad040ff9-c922-47a8-bed2-25b795b5cf24",
   "metadata": {},
   "source": [
    "First, we instantiate the `TLMCalibrated` object with the same custom evaluation criteria used previously in the `options` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4624b05b-9242-4054-9711-1abdddad390e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tlm_calibrated = studio.TLMCalibrated(options=custom_eval_criteria_option)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7eb910-8d94-4e28-82a6-48404da483cc",
   "metadata": {},
   "source": [
    "Next, we **fit** this `TLMCalibrated` object to a dataset composed of the previously obtained TLM scores and human quality ratings. This trains the model to produce better-aligned scores over this dataset.\n",
    "\n",
    "We can directly pass `res_custom_eval` (the list of dictionaries returned from the previous `get_trustworthiness_score()` call) directly into `fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5cd04dff-a111-4e06-99a2-29580b3857a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tlm_calibrated.fit(res_custom_eval, data[\"human_rating\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693f493a-0a6c-4452-a306-b73cc13e05e4",
   "metadata": {},
   "source": [
    "After fitting `TLMCalibrated` to our human-annotated dataset, we can call its `get_trustworthiness_score()` method. This returns not only the previous TLM trustworthiness score and custom conciseness score, but also an additional calibrated score that optimally combines those two scores to align with human quality ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "444456d6-1dc7-45cb-87c5-99ebab0a5969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Querying TLM... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|\n"
     ]
    }
   ],
   "source": [
    "calibrated_res = tlm_calibrated.get_trustworthiness_score(data[\"improved_prompt\"].tolist(), data[\"answer\"].tolist())\n",
    "calibrated_res_df = pd.DataFrame(calibrated_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8e67547-df9a-4537-89b6-285f1336dfd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trustworthiness_score</th>\n",
       "      <th>log</th>\n",
       "      <th>calibrated_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.651190</td>\n",
       "      <td>{'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.017434491510790216}]}</td>\n",
       "      <td>0.015714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.990492</td>\n",
       "      <td>{'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.997512434971306}]}</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.933385</td>\n",
       "      <td>{'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.2512370425791872}]}</td>\n",
       "      <td>0.577002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.225192</td>\n",
       "      <td>{'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.25034993649245585}]}</td>\n",
       "      <td>0.109214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.941208</td>\n",
       "      <td>{'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.25271347417799767}]}</td>\n",
       "      <td>0.776423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trustworthiness_score  \\\n",
       "0               0.651190   \n",
       "1               0.990492   \n",
       "2               0.933385   \n",
       "3               0.225192   \n",
       "4               0.941208   \n",
       "\n",
       "                                                                                  log  \\\n",
       "0  {'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.017434491510790216}]}   \n",
       "1     {'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.997512434971306}]}   \n",
       "2    {'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.2512370425791872}]}   \n",
       "3   {'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.25034993649245585}]}   \n",
       "4   {'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.25271347417799767}]}   \n",
       "\n",
       "   calibrated_score  \n",
       "0          0.015714  \n",
       "1          1.000000  \n",
       "2          0.577002  \n",
       "3          0.109214  \n",
       "4          0.776423  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calibrated_res_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fddd249-eff5-49be-937f-9187dbb00c94",
   "metadata": {},
   "source": [
    "We again measure alignment between the calibrated scores and our human ratings via their Spearman correlation, which here is the highest among all scoring methods. Thus whenever you have a moderately-sized set of human quality ratings, we recommend considering `TLMCalibrated` to produce a tailored evaluation score for LLM responses in your use-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "118f947a-adb4-4576-a3ac-8d76be2f42de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation: 0.7124205820646288\n"
     ]
    }
   ],
   "source": [
    "print(\"Spearman correlation:\", spearmanr(calibrated_res_df[\"calibrated_score\"], data[\"human_rating\"]).statistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddc060a-9a90-4c2f-a772-66b2ff416cdd",
   "metadata": {},
   "source": [
    "### Using `TLMCalibrated` to get improved scores on new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358f072e-2a69-41ad-bbe5-40b43465afa6",
   "metadata": {},
   "source": [
    "You can use the already-fitted `TLMCalibrated` model to score new responses via the same `get_trustworthiness_score()` method. Here we demonstrate this with some additional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f06d607-72a7-4afd-a48c-043fe7e2a4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_csv(\"triviaqa_without_ratings.csv\")\n",
    "new_data[\"prompt\"] = new_data[\"question\"].apply(create_improved_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec4a5ac-947e-489b-a43b-4bea1651e73e",
   "metadata": {},
   "source": [
    "We call `get_trustworthiness_score()` on the new (prompt, response) pairs, just as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb61511e-a5ec-4586-aad3-01612f527d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Querying TLM... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trustworthiness_score</th>\n",
       "      <th>log</th>\n",
       "      <th>calibrated_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.971024</td>\n",
       "      <td>{'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.9975124352663575}]}</td>\n",
       "      <td>0.923859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.749689</td>\n",
       "      <td>{'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.1843429305003393}]}</td>\n",
       "      <td>0.053251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trustworthiness_score  \\\n",
       "0               0.971024   \n",
       "1               0.749689   \n",
       "\n",
       "                                                                                log  \\\n",
       "0  {'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.9975124352663575}]}   \n",
       "1  {'custom_eval_criteria': [{'name': 'Conciseness', 'score': 0.1843429305003393}]}   \n",
       "\n",
       "   calibrated_score  \n",
       "0          0.923859  \n",
       "1          0.053251  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data_res = tlm_calibrated.get_trustworthiness_score(new_data[\"prompt\"].tolist(), new_data[\"answer\"].tolist())\n",
    "new_data_res_df = pd.DataFrame(new_data_res)\n",
    "new_data_res_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194ec92a-db7a-46a0-bc78-bf7bd7751ac0",
   "metadata": {},
   "source": [
    "The returned results contain the TLM trustworthiness score and `log` (like before), but also the `calibrated_score` to use for evaluating these additional responses (in real-time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "43eafdd9-67f8-459e-abcc-159eac744a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>calibrated_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1976, which gymnast scored 7 maximum scores of 10 as she won three gold medals, one silver and one bronze?</td>\n",
       "      <td>Nadia Comaneci</td>\n",
       "      <td>0.923859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the Milky Way?</td>\n",
       "      <td>The Milky Way is a barred spiral galaxy, one of the billions of galaxies in the observable universe, and it is our home galaxy. As part of the Local Group of galaxies, it exists alongside other notable galaxies like Andromeda and Triangulum, as well as about 54 smaller satellite galaxies. The Milky Way contains between 100 and 400 billion stars, including our Sun, and the dense band of light seen from Earth at night is caused by the multitude of stars and celestial objects aligned along the galactic plane. It is named after the appearance of this bright band of light, which stretches across the night sky.</td>\n",
       "      <td>0.053251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                        question  \\\n",
       "0  In 1976, which gymnast scored 7 maximum scores of 10 as she won three gold medals, one silver and one bronze?   \n",
       "1                                                                                         What is the Milky Way?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 answer  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Nadia Comaneci   \n",
       "1  The Milky Way is a barred spiral galaxy, one of the billions of galaxies in the observable universe, and it is our home galaxy. As part of the Local Group of galaxies, it exists alongside other notable galaxies like Andromeda and Triangulum, as well as about 54 smaller satellite galaxies. The Milky Way contains between 100 and 400 billion stars, including our Sun, and the dense band of light seen from Earth at night is caused by the multitude of stars and celestial objects aligned along the galactic plane. It is named after the appearance of this bright band of light, which stretches across the night sky.   \n",
       "\n",
       "   calibrated_score  \n",
       "0          0.923859  \n",
       "1          0.053251  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data_combined = pd.concat([new_data, new_data_res_df], axis=1)\n",
    "new_data_combined.head(2)[[\"question\", \"answer\", \"calibrated_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b269135c-1fdc-4624-b577-3829663e139d",
   "metadata": {},
   "source": [
    "### Further scoring improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c0ee86-ddf1-49ec-b83e-6eb5a7371b15",
   "metadata": {},
   "source": [
    "At this point, you have specified custom evaluation criteria and combined it with TLM's native trustworthiness score to produce calibrated scores via `TLMCalibrated`. To further align these scores against your human-quality ratings, you can tinker with the prompts used in various arguments of `TLMCalibrated`.\n",
    "\n",
    "Try jointly improving the text used for both the `prompt` argument and the custom evaluation criteria. Iterate through many alternative versions of this text, trying to maximize the resulting Spearman correlation between the resulting calibrated scores from `TLMCalibrated` and human-quality ratings.\n",
    "\n",
    "Tips on tuning both of these include:\n",
    "\n",
    "- The base TLM prompt will perform better in general correctness evaluation, however you can still be specific about certain guidelines that it can follow.\n",
    "- For example in RAG use cases: if the correct response to a question is \"The given context does not contain any relevant information to answer the question\" (because the retrieved context lacks the necessary details), you should explicitly specify this in the prompt. This ensures that the TLM recognizes such responses as correct and does not score them low simply because they do not provide a direct answer.\n",
    "- The custom evaluation criteria is better suited for domain-specific needs beyond a general correctness check. You can specify these custom evaluation criteria to complement the general trustworthiness scoring that the base TLM provides.\n",
    "\n",
    "\n",
    "By systematically adjusting both the prompt and evaluation criteria, you can improve `TLMCalibrated` scores to suite your specific use-case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}