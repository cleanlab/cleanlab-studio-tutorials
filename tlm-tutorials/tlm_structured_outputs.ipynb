{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8f278ba-441a-470a-99c1-54cbd6f8cee5",
   "metadata": {},
   "source": [
    "# Using TLM via the OpenAI library to score the trustworthiness of: structured outputs, function calling, messages, and more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79715fec-e568-407b-b2f6-11aba262c9ed",
   "metadata": {},
   "source": [
    "This tutorial demonstrates how to assess the trustworthiness of OpenAI model responses using Cleanlab's Trustworthy Language Model (TLM), accessible directly through the OpenAI library. Existing OpenAI users: you can obtain real-time trustworthiness scores for every OpenAI response, without changing your code.\n",
    "\n",
    "Using TLM via the OpenAI library enables you to leverage OpenAI's advanced features (structured outputs, function calling, ...), while reliably scoring the trustworthiness of each response to automatically catch errors/hallucinations made by OpenAI.\n",
    "\n",
    "In this tutorial, we use OpenAI's structured outputs feature to perform multi-label classification (i.e. document tagging) with trustworthiness scores from TLM. The same method can be used to score the trustworthiness of any type of output from OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce124e52-f9c4-4110-b844-3fe05a84adb8",
   "metadata": {},
   "source": [
    "## Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c07e776-adf9-4246-8a98-7d19d2fcb422",
   "metadata": {},
   "source": [
    "Using TLM requires a [Cleanlab account](https://app.cleanlab.ai/). Sign up for one [here](https://cleanlab.ai/signup/) if you haven\u2019t yet. \n",
    "\n",
    "The Python package dependencies for this tutorial can be installed using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4c356e4-6b02-4864-a5ff-926d991de162",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade openai tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9bc6a28-1873-4b02-8624-16faffca9108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from enum import Enum\n",
    "from pydantic import BaseModel\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from openai import OpenAI\n",
    "import ast\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e18a67a-8575-43bb-9e53-74385d6fbc6a",
   "metadata": {},
   "source": [
    "## Fetch the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65018bf-464e-4c8c-abe2-7e72c517918d",
   "metadata": {},
   "source": [
    "This tutorial uses a modified version of the [Alexa intent detection dataset](https://huggingface.co/datasets/AmazonScience/massive). \n",
    "\n",
    "Each text sample contains several statements that could correspond to multiple intents (for example controlling devices, asking for information etc). The label corresponding to each example specifies what the intent of that statement is, where there could be more than one intent corresponding to each sample. Let's take a look at the dataset below:\n",
    "\n",
    "In this tutorial, we will only run the LLM inference on 50 randomly sampled examples of this dataset as a demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b43273fc-c518-46fe-b6dd-f34215552fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://cleanlab-public.s3.us-east-1.amazonaws.com/Datasets/massive_multilabel_classification.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a73bec43-dee1-473a-a926-318b1a9c11d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lets have a chat</td>\n",
       "      <td>[general_quirky]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what are meeting scheduled for today</td>\n",
       "      <td>[calendar_query]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>erase all the events. resume my audio book from karl pilkington. tell me the profession of celebrity</td>\n",
       "      <td>[calendar_remove, play_audiobook, qa_factoid]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thirty minute reminder on meeting for tuesday</td>\n",
       "      <td>[calendar_set]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i have a nine am meeting on wednesday send me a reminder</td>\n",
       "      <td>[calendar_set]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                   text  \\\n",
       "0                                                                                      lets have a chat   \n",
       "1                                                                  what are meeting scheduled for today   \n",
       "2  erase all the events. resume my audio book from karl pilkington. tell me the profession of celebrity   \n",
       "3                                                         thirty minute reminder on meeting for tuesday   \n",
       "4                                              i have a nine am meeting on wednesday send me a reminder   \n",
       "\n",
       "                                          labels  \n",
       "0                               [general_quirky]  \n",
       "1                               [calendar_query]  \n",
       "2  [calendar_remove, play_audiobook, qa_factoid]  \n",
       "3                                 [calendar_set]  \n",
       "4                                 [calendar_set]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"massive_multilabel_classification.csv\")\n",
    "data[\"labels\"] = data[\"labels\"].apply(ast.literal_eval)\n",
    "data = data.sample(50, random_state=123).reset_index(drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ba57c0-ea98-4d35-9b6e-41b8313a3f8c",
   "metadata": {},
   "source": [
    "## Obtain LLM Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d8deb5-872e-4866-a6b4-42fddb24a868",
   "metadata": {},
   "source": [
    "### Define Structured Output Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94989f63-ade7-4a01-95bd-bea936f6cfce",
   "metadata": {},
   "source": [
    "First, we need to get a list of all possible classes from the given dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "763c6dcd-6a18-4833-bf94-898f1a000836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['general_quirky', 'calendar_query', 'calendar_remove',\n",
       "       'play_audiobook', 'qa_factoid'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_classes = data[\"labels\"].explode().unique()\n",
    "multilabel_classes[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbe510d-8e60-4544-809f-8d817423e5ca",
   "metadata": {},
   "source": [
    "Then, we can create a object that inherits from pydantic's `BaseModel` to represent the multi-label classification schema, ensuring that each predicted label is validated against the predefined list of possible classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a14ba5ee-44dd-4727-93a2-e547487219c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MultiLabelClassification(BaseModel):\n",
    "    classes: list[Enum(\"MultilabelClasses\", {name: name for name in multilabel_classes})]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f87254-771e-4674-aad9-1a83f08a0343",
   "metadata": {},
   "source": [
    "### Prompt OpenAI "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38fc4d5-f768-430e-94f4-8eba529591ef",
   "metadata": {},
   "source": [
    "Then, we can instantiate the OpenAI client, pointing the `base_url` to TLM, which allows us to also get the trustworthiness score associated with each response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "549220e5-0530-4342-af89-538a7965e577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your API key from https://app.cleanlab.ai/account after creating an account\n",
    "client = OpenAI(\n",
    "    api_key=\"<Cleanlab API key>\",\n",
    "    base_url=\"https://api.cleanlab.ai/api/v1/openai_trustworthy_llm/\"  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1bbaa6-691c-4192-94ce-64c465abfb00",
   "metadata": {},
   "source": [
    "Here is an example of how we can prompt OpenAI with one sample text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5efa2ff4-1b9b-4562-b74d-c431d8db4d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lets have a chat'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = data['text'][0]\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e8792d0-fa34-4824-8dc8-27bda83d9258",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "            {\"role\": \"user\", \"content\": f\"Classify the following text: {sample_text}\"}  \n",
    "        ],\n",
    "    response_format=MultiLabelClassification,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007eeb76-9324-44a4-b08c-a2994d6e9e53",
   "metadata": {},
   "source": [
    "The returned object matches what OpenAI would ordinarily return, except it contains a few additional keys from TLM: trustworthiness_score, tlm_metadata.  This way you can use TLM as a drop-in replacement for OpenAI in any application.  Let's parse the predictions and trustworthiness score from the returned response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6d555f0-a01e-4e9f-9628-3a7d093fca6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Classes: ['general_quirky']\n",
      "Trustworthiness Score: 0.8512080365166845\n"
     ]
    }
   ],
   "source": [
    "parsed_predictions = [prediction.value for prediction in completion.choices[0].message.parsed.classes]\n",
    "trustworthiness_score = completion.tlm_metadata[\"trustworthiness_score\"]\n",
    "\n",
    "print(f\"Predicted Classes: {parsed_predictions}\")\n",
    "print(f\"Trustworthiness Score: {trustworthiness_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcb4ff8-2982-4542-9693-9960583e5c8e",
   "metadata": {},
   "source": [
    "### Batch Prompt on a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453680cb-1bc0-4381-8b43-f405de8366af",
   "metadata": {},
   "source": [
    "Here, we define a quick helper function that allows us to process multiple texts in parallel, which will speed up prompting the LLM on an entire dataset. The helper functions also parses and collects the predictions and trustworthiness score in a DataFrame for easy downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4433f2d-a4db-4909-b523-731ac6227b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text):\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Classify the following text: {text}\"}],\n",
    "        response_format=MultiLabelClassification,\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"predictions\": [pred.value for pred in completion.choices[0].message.parsed.classes],\n",
    "        \"trustworthiness_score\": completion.tlm_metadata[\"trustworthiness_score\"],\n",
    "    }\n",
    "\n",
    "def classify_texts_batch(texts, batch_size=20, max_threads=8, sleep_time=10):\n",
    "    results = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        \n",
    "        with ThreadPoolExecutor(max_threads) as executor:\n",
    "            futures = [executor.submit(classify_text, text) for text in batch]\n",
    "            batch_results = [f.result() for f in futures]\n",
    "        \n",
    "        results.extend(batch_results)\n",
    "\n",
    "        # sleep to prevent hitting rate limits\n",
    "        if i + batch_size < len(texts):\n",
    "            time.sleep(sleep_time)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "786fe8b7-dada-4ec6-81cd-5e1643c12b59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictions</th>\n",
       "      <th>trustworthiness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[general_quirky]</td>\n",
       "      <td>0.851207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[calendar_query]</td>\n",
       "      <td>0.988874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[calendar_remove, play_audiobook, qa_factoid]</td>\n",
       "      <td>0.989885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[alarm_query]</td>\n",
       "      <td>0.338316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[calendar_set, calendar_query]</td>\n",
       "      <td>0.687683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     predictions  trustworthiness_score\n",
       "0                               [general_quirky]               0.851207\n",
       "1                               [calendar_query]               0.988874\n",
       "2  [calendar_remove, play_audiobook, qa_factoid]               0.989885\n",
       "3                                  [alarm_query]               0.338316\n",
       "4                 [calendar_set, calendar_query]               0.687683"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = classify_texts_batch(data[\"text\"])\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b6aacf-2768-45ca-bd10-4eaff37693d8",
   "metadata": {},
   "source": [
    "## Examine Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5ce86f-3085-4c90-aca4-964b1a3fe867",
   "metadata": {},
   "source": [
    "We have now obtained the predictions and trustworthiness score for each given text. Let's examine the results in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0c9198d-d26c-422c-b11f-6adec3fed116",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_results = pd.concat([data, results], axis=1)\n",
    "combined_results = combined_results.rename(columns={\"labels\": \"ground_truth_labels\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3732335f-9bb2-47f0-83da-936b2c66ca09",
   "metadata": {},
   "source": [
    "### High Trustworthiness Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccb3a4f-2c56-427a-88c5-a8345830cfcd",
   "metadata": {},
   "source": [
    "The responses with the highest trustworthiness scores represent texts where TLM is the most confident that it has predicted the correct intents.\n",
    "\n",
    "We can see below that the predictions for the samples below match the ground truth labels and are correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "946e78ca-980d-4078-8564-01496fdfdf06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ground_truth_labels</th>\n",
       "      <th>predictions</th>\n",
       "      <th>trustworthiness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>what alarms did i set</td>\n",
       "      <td>[alarm_query]</td>\n",
       "      <td>[alarm_query]</td>\n",
       "      <td>0.989979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>turn the lights off</td>\n",
       "      <td>[iot_hue_lightoff]</td>\n",
       "      <td>[iot_hue_lightoff]</td>\n",
       "      <td>0.989947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>send an email to margaret. shut down the sound</td>\n",
       "      <td>[email_sendemail, audio_volume_mute]</td>\n",
       "      <td>[email_sendemail, audio_volume_mute]</td>\n",
       "      <td>0.989936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text  \\\n",
       "7                            what alarms did i set   \n",
       "20                             turn the lights off   \n",
       "17  send an email to margaret. shut down the sound   \n",
       "\n",
       "                     ground_truth_labels  \\\n",
       "7                          [alarm_query]   \n",
       "20                    [iot_hue_lightoff]   \n",
       "17  [email_sendemail, audio_volume_mute]   \n",
       "\n",
       "                             predictions  trustworthiness_score  \n",
       "7                          [alarm_query]               0.989979  \n",
       "20                    [iot_hue_lightoff]               0.989947  \n",
       "17  [email_sendemail, audio_volume_mute]               0.989936  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_results.sort_values(\"trustworthiness_score\", ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4639fa71-b7f2-480b-b00b-40fec010338f",
   "metadata": {},
   "source": [
    "### Low Trustworthiness Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25664160-d35f-421b-99b7-ec8e2bd4d200",
   "metadata": {},
   "source": [
    "The responses with the lowest trustworthiness scores indicate outputs we are least confident are good.\n",
    "\n",
    "Results with low trustworthiness scores would benefit most from manual review, especially if we need almost all outputs across the dataset to be correct.\n",
    "\n",
    "For examples with the lowest trustworthiness scores in our dataset shown below, you can see that the predictions tend to be incorrect or could use further review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5f546f4-c2a0-4501-b94f-109c0e0a237c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ground_truth_labels</th>\n",
       "      <th>predictions</th>\n",
       "      <th>trustworthiness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>i will need warm socks in winter in morning</td>\n",
       "      <td>[weather_query]</td>\n",
       "      <td>[general_quirky, datetime_query]</td>\n",
       "      <td>0.264497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thirty minute reminder on meeting for tuesday</td>\n",
       "      <td>[calendar_set]</td>\n",
       "      <td>[alarm_query]</td>\n",
       "      <td>0.338316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>features of google pixel. what is the deepest point on earth</td>\n",
       "      <td>[general_quirky, qa_factoid]</td>\n",
       "      <td>[qa_factoid, recommendation_events]</td>\n",
       "      <td>0.460527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            text  \\\n",
       "42                   i will need warm socks in winter in morning   \n",
       "3                  thirty minute reminder on meeting for tuesday   \n",
       "41  features of google pixel. what is the deepest point on earth   \n",
       "\n",
       "             ground_truth_labels                          predictions  \\\n",
       "42               [weather_query]     [general_quirky, datetime_query]   \n",
       "3                 [calendar_set]                        [alarm_query]   \n",
       "41  [general_quirky, qa_factoid]  [qa_factoid, recommendation_events]   \n",
       "\n",
       "    trustworthiness_score  \n",
       "42               0.264497  \n",
       "3                0.338316  \n",
       "41               0.460527  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_results.sort_values(\"trustworthiness_score\").head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922df4fa-409c-40f3-8b8a-73ba2f5ee313",
   "metadata": {},
   "source": [
    "## Using Different Quality Presets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a396b996-da06-4102-9ef9-8a74f7cf24cb",
   "metadata": {},
   "source": [
    "You can use TLM with different quality presets by specifying the preset after the model name. \n",
    "\n",
    "For example, in this example below we specify `model=\"gpt-4o-low\"` to use TLM on `low` quality preset (for lower cost/latency). If unspecified, the default quality preset used is `medium`.\n",
    "\n",
    "Currently, only `base`, `low`, and `medium` presets are supported when using TLM via the OpenAI library. Read more about quality presets [here](/reference/python/trustworthy_language_model/#class-tlmoptions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "561a4cc9-e835-40e8-adc5-abcecb09db9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = data['text'][0]\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-low\",\n",
    "    messages=[\n",
    "            {\"role\": \"user\", \"content\": f\"Classify the following text: {sample_text}\"}  \n",
    "        ],\n",
    "    response_format=MultiLabelClassification,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}