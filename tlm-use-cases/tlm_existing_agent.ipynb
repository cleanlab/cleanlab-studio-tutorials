{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving LangGraph prebuilt Agents via Trustworthiness Scoring\n",
    "\n",
    "<head>\n",
    "  <meta name=\"title\" content=\"More reliable AI Agents via Trustworthiness Scoring\"/>\n",
    "  <meta property=\"og:title\" content=\"More reliable AI Agents via Trustworthiness Scoring\"/>\n",
    "  <meta name=\"twitter:title\" content=\"More reliable AI Agents via Trustworthiness Scoring\" />\n",
    "  <meta name=\"description\" content=\"How to prevent incorrect responses from your Agent systems.\"  />\n",
    "  <meta property=\"og:description\" content=\"How to prevent incorrect responses from your Agent systems.\" />\n",
    "  <meta name=\"twitter:description\" content=\"How to prevent incorrect responses from your Agent systems.\" />\n",
    "</head>\n",
    "\n",
    "Agentic AI systems coordinate multiple tools and language model interactions to tackle complex user tasks. Tool-calling Agents are a common type of Agent that come prebuilt in libraries like [LangGraph](https://langchain-ai.github.io/langgraph). Despite their capabilities, AI Agents are prone to occasional errors due to LLM hallucinations that make them unreliable. This tutorial demonstrates how to make *any* LangGraph Agent more reliable by scoring LLM response trustworthiness in real time.\n",
    "\n",
    "## Setup\n",
    "\n",
    "You can install the packages needed for this tutorial via pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipython cleanlab-tlm langgraph \"langchain[openai]\" langchain-community langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set API keys\n",
    "import os\n",
    "\n",
    "os.environ[\"CLEANLAB_TLM_API_KEY\"] = \"<YOUR_CLEANLAB_TLM_API_KEY>\"  # Get your free API key from: https://tlm.cleanlab.ai/\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_OPENAI_API_KEY>\"  # Get API key from: https://platform.openai.com/signup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Tool-calling Agent\n",
    "\n",
    "LangGraph's `create_react_agent()` function provides a prebuilt Tool-Calling Agent that can answer user queries and call tools. For demonstration, here we'll give the Agent two tools: `get_weather()` and `get_location()`, which provide information about a city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's currently sunny in {city}!\"  # toy example for demonstration purposes\n",
    "\n",
    "def get_location(city: str) -> str:\n",
    "    \"\"\"Get location for a given city.\"\"\"\n",
    "    return f\"{city} is located in Germany.\"  # toy example for demonstration purposes\n",
    "\n",
    "tools=[get_weather, get_location]\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=\"openai:gpt-4.1-mini\",\n",
    "    tools=tools,\n",
    "    prompt=\"You are a helpful assistant\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the graph implementing our Agent, which can go between LLM and tool calls freely, and return its final response after it's done using tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(agent.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![basic-raw](./assets/tlm-existing-agent/basic-raw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Trust Layer\n",
    "\n",
    "To score the trustworthiness of LLM outputs inside our Agent, we initialize the [Trustworthy Language Model](https://cleanlab.ai/blog/trustworthy-language-model/) (TLM). We log its explanation for *why* certain LLM responses are untrustworthy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab_tlm import TLM\n",
    "\n",
    "tlm = TLM(options={\"log\": [\"explanation\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once trustworthiness scores are added to an Agent, there are many ways to utilize them. When a LLM response is deemed untrustworthy (low score), you could [replace it with a fallback canned response](/tlm/use-cases/tlm_agents) or escalate this Agent interaction to a human employee.\n",
    "\n",
    "Here we instead employ an autonomous strategy to boost Agent accuracy: When a LLM output is deemed untrustworthy, our system automatically produces an internal message to the Agent informing it that its previous LLM output was untrustworthy and should be re-generated to be more trustworthy. This internal re-generation message includes TLM's explanation for *why* the previous LLM output seemed untrustworthy.\n",
    "\n",
    "We'll implement this using a LangGraph [`post_model_hook`](https://langchain-ai.github.io/langgraph/agents/overview/) to interact with the Agent's internal LLM outputs before they are served to the user. The `post_model_hook` is a LangGraph node that gets called after the Agent node, and this is where we will compute TLM trustworthiness scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Callable\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.types import Command\n",
    "from cleanlab_tlm.utils.chat import form_prompt_string\n",
    "from langchain_core.messages.utils import convert_to_openai_messages\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from langchain_core.messages import SystemMessage, ToolMessage\n",
    "\n",
    "SYSTEM_MESSAGE = \"You are a helpful assistant\"  # replace with overall instructions for your Agent\n",
    "\n",
    "def handle_untrustworthy_response(state, score):\n",
    "    \"\"\"You can customize this to differently handle LLM responses deemed untrustworthy, such as logging the response or sending it to a human reviewer.\"\"\"\n",
    "    if state[\"messages\"][-1].tool_calls:\n",
    "        # Cancel tool calls if the response is untrustworthy\n",
    "        for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "            state[\"messages\"].append(ToolMessage(content=f\"Tool Call Canceled\", tool_call_id=tool_call[\"id\"], name=tool_call[\"name\"]))\n",
    "\n",
    "    state[\"messages\"].append(SystemMessage(content=f\"\"\"Your last response was not trustworthy. Rewrite your response to be more trustworthy. The old version will not be shown to the user, so do not reference it.\n",
    "Reason: {score['log']['explanation']}\"\"\"))\n",
    "\n",
    "    return Command(\n",
    "        update=state,\n",
    "        goto=\"agent\"\n",
    "    )\n",
    "\n",
    "def format_response(response: dict) -> str:\n",
    "    \"\"\"Format LLM responses for TLM.\"\"\"\n",
    "    content = response[\"content\"] or ''\n",
    "    if \"tool_calls\" in response:\n",
    "        tool_calls = \"\\n\".join(\n",
    "            [\n",
    "                f\"<tool_call>{json.dumps({'name': call['function']['name'], 'arguments': call['function']['arguments']}, indent=2)}</tool_call>\"\n",
    "                for call in response[\"tool_calls\"]\n",
    "            ]\n",
    "        )\n",
    "        return f\"{content}\\n{tool_calls}\".strip()\n",
    "    return content\n",
    "\n",
    "def build_tlm_verifier(handle_untrustworthy_response: Callable, trustworthiness_threshold: float = 0.8, system_message: str = \"\"):\n",
    "    \"\"\"Creates a step in the Agent to score trustworthiness of LLM outputs using TLM.\n",
    "       If the trusworthiness score falls below a threshold, this step sends a redirection message to the Agent to try again with new feedback.\n",
    "       When building the TLM verifier, we specify: the sender node to redirect back to in case of a low score, the trustworthiness threshold for response re-generation (adjust this to suit your use-case), and the re-generation system message for getting your LLM to generate another response to use in place of the untrustworthy one.\n",
    "    \"\"\"\n",
    "    def review_node(state):\n",
    "        # Give the TLM access to the chat history, including the system message\n",
    "        openai_chat_history = convert_to_openai_messages(([SystemMessage(content=system_message)] if system_message else []) + state[\"messages\"])\n",
    "        openai_chat_tools = [convert_to_openai_tool(tool) for tool in tools]\n",
    "\n",
    "        formatted_prompt = form_prompt_string(openai_chat_history[:-1], openai_chat_tools)\n",
    "        formatted_response = format_response(openai_chat_history[-1])\n",
    "\n",
    "        trustworthiness_score = tlm.get_trustworthiness_score(prompt=formatted_prompt, response=formatted_response)\n",
    "\n",
    "        if trustworthiness_score[\"trustworthiness_score\"] < trustworthiness_threshold:\n",
    "            return handle_untrustworthy_response(state, trustworthiness_score)\n",
    "\n",
    "        return state\n",
    "    return review_node\n",
    "\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=\"openai:gpt-4.1-mini\",\n",
    "    tools=tools,\n",
    "    prompt=\"You are a helpful assistant\",\n",
    "    post_model_hook=build_tlm_verifier(\n",
    "        handle_untrustworthy_response=handle_untrustworthy_response,\n",
    "        system_message=SYSTEM_MESSAGE\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the Agent graph to see its new structure. The Agent now has a post-model hook that can loop back into the Agent if the response is untrustworthy. This allows the Agent to autonomously improve response accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(agent.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![basic-tlm](./assets/tlm-existing-agent/basic-tlm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Agent\n",
    "\n",
    "Let's invoke the Agent with a user query. Here we print all internal LLM outputs as well as the Agent's response to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what is the weather in sf\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_weather (call_Nja4tADZQrHcH0quD2rn5YPV)\n",
      " Call ID: call_Nja4tADZQrHcH0quD2rn5YPV\n",
      "  Args:\n",
      "    city: San Francisco\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_weather\n",
      "\n",
      "It's currently sunny in San Francisco!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The weather in San Francisco is currently sunny.\n"
     ]
    }
   ],
   "source": [
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
    ")\n",
    "\n",
    "for message in response[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the Agent's response is correct. There was no need to fix its LLM outputs since the Agent appropriately used tools and handled their outputs properly (TLM trustworthiness scores were high).\n",
    "\n",
    "Let's run the Agent on another query, again printing all internal LLM calls in addition to the final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Where is sf\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_location (call_doeb8EgChg2RXezG3TnCqog4)\n",
      " Call ID: call_doeb8EgChg2RXezG3TnCqog4\n",
      "  Args:\n",
      "    city: sf\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_location\n",
      "\n",
      "sf is located in Germany.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "SF is located in Germany. If you were asking about \"SF\" as in San Francisco, it is in California, USA. Please let me know if you meant a different \"SF.\"\n",
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Your last response was not trustworthy. Rewrite your response to be more trustworthy. The old version will not be shown to the user, so do not reference it.\n",
      "Reason: The assistant's answer states that \"SF is located in Germany,\" which is based on the function call result from the provided tool. However, this is factually incorrect in the most common and widely recognized context. \"SF\" is a common abbreviation for San Francisco, which is a major city in California, USA, not Germany. The tool's output is likely incorrect or referring to a different, less common place abbreviated as \"sf\" in Germany, but this is not clarified or commonly known.\n",
      "\n",
      "The assistant does hedge by adding \"If you were asking about 'SF' as in San Francisco, it is in California, USA,\" which is the nd most relevant information for the vast majority of users. This part of the answer is accurate and helpful.\n",
      "\n",
      "Given the conflicting information from the tool and the well-known fact about San Francisco, the assistant's answer is partially correct but primarily misleading because it repeats the incorrect tool output without clarifying that it is likely an error or a less common usage. A better answer would have been to clarify that \"SF\" usually refers to San Francisco in California, USA, and that the tool's output might be incorrect or refer to a different place.\n",
      "\n",
      "Overall, the answer contains a factual error but also provides the correct common knowledge. Because the question is about \"Where is sf,\" the most relevant and nswer is San Francisco, California, USA. The incorrect claim that \"SF is located in Germany\" significantly reduces the accuracy of the answer.\n",
      "\n",
      "Considering these points, the answer is not fully correct but not entirely wrong either.\n",
      "\n",
      "Score: Around 40-50 to reflect partial correctness but significant factual inaccuracy. \n",
      "This response is untrustworthy due to lack of consistency in possible responses from the model. Here's one inconsistent alternate response that the model considered (which may not be accurate either): \n",
      "sf\" is located in Germany.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "\"SF\" most commonly refers to San Francisco, which is a major city located in California, USA. If you were referring to something else by \"SF,\" please provide more context so I can assist you better.\n"
     ]
    }
   ],
   "source": [
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Where is sf\"}]}\n",
    ")\n",
    "\n",
    "for message in response[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we purposefully introduced a strange `get_location()` tool, that yielded an abnormal result. With our trusworthiness layer in place, the Agent is able to realize that its response is untrustworthy and generate a more trustworthy response.\n",
    "\n",
    "## Add Trust Scores to another Agent\n",
    "\n",
    "TLM can be used with any LangGraph Agent architecture. For example, let's apply it to LangGraph's [many tools](https://langchain-ai.github.io/langgraph/how-tos/many-tools) Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089b98fd-bd13-4a4d-af70-283ae4692a43",
   "metadata": {},
   "source": [
    "**Optional: Original LangGraph code for the Many Tools Agent.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import uuid\n",
    "\n",
    "from langchain_core.tools import StructuredTool\n",
    "\n",
    "\n",
    "def create_tool(company: str) -> dict:\n",
    "    \"\"\"Create schema for a placeholder tool.\"\"\"\n",
    "    # Remove non-alphanumeric characters and replace spaces with underscores for the tool name\n",
    "    formatted_company = re.sub(r\"[^\\w\\s]\", \"\", company).replace(\" \", \"_\")\n",
    "\n",
    "    def company_tool(year: int) -> str:\n",
    "        # Placeholder function returning static revenue information for the company and year\n",
    "        return f\"{company} had revenues of $100 in {year}.\"\n",
    "\n",
    "    return StructuredTool.from_function(\n",
    "        company_tool,\n",
    "        name=formatted_company,\n",
    "        description=f\"Information about {company}\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Abbreviated list of S&P 500 companies for demonstration\n",
    "s_and_p_500_companies = [\n",
    "    \"3M\",\n",
    "    \"A.O. Smith\",\n",
    "    \"Abbott\",\n",
    "    \"Accenture\",\n",
    "    \"Advanced Micro Devices\",\n",
    "    \"Yum! Brands\",\n",
    "    \"Zebra Technologies\",\n",
    "    \"Zimmer Biomet\",\n",
    "    \"Zoetis\",\n",
    "]\n",
    "\n",
    "# Create a tool for each company and store it in a registry with a unique UUID as the key\n",
    "tool_registry = {\n",
    "    str(uuid.uuid4()): create_tool(company) for company in s_and_p_500_companies\n",
    "}\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "tool_documents = [\n",
    "    Document(\n",
    "        page_content=tool.description,\n",
    "        id=id,\n",
    "        metadata={\"tool_name\": tool.name},\n",
    "    )\n",
    "    for id, tool in tool_registry.items()\n",
    "]\n",
    "\n",
    "vector_store = InMemoryVectorStore(embedding=OpenAIEmbeddings())\n",
    "document_ids = vector_store.add_documents(tool_documents)\n",
    "\n",
    "from typing import Annotated\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "\n",
    "# Define the state structure using TypedDict.\n",
    "# It includes a list of messages (processed by add_messages)\n",
    "# and a list of selected tool IDs.\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    selected_tools: list[str]\n",
    "\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# Retrieve all available tools from the tool registry.\n",
    "tools = list(tool_registry.values())\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "\n",
    "# The agent function processes the current state\n",
    "# by binding selected tools to the LLM.\n",
    "def agent(state: State):\n",
    "    # Map tool IDs to actual tools\n",
    "    # based on the state's selected_tools list.\n",
    "    selected_tools = [tool_registry[id] for id in state[\"selected_tools\"]]\n",
    "    # Bind the selected tools to the LLM for the current interaction.\n",
    "    llm_with_tools = llm.bind_tools(selected_tools)\n",
    "    # Invoke the LLM with the current messages and return the updated message list.\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "# The select_tools function selects tools based on the user's last message content.\n",
    "def select_tools(state: State):\n",
    "    last_user_message = state[\"messages\"][-1]\n",
    "    query = last_user_message.content\n",
    "    tool_documents = vector_store.similarity_search(query)\n",
    "    return {\"selected_tools\": [document.id for document in tool_documents]}\n",
    "\n",
    "\n",
    "builder.add_node(\"agent\", agent)\n",
    "builder.add_node(\"select_tools\", select_tools)\n",
    "\n",
    "tool_node = ToolNode(tools=tools)\n",
    "builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "builder.add_conditional_edges(\"agent\", tools_condition, path_map=[\"tools\", \"__end__\"])\n",
    "builder.add_edge(\"tools\", \"agent\")\n",
    "builder.add_edge(\"select_tools\", \"agent\")\n",
    "builder.add_edge(START, \"select_tools\")\n",
    "graph = builder.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass\n",
    "\n",
    "\n",
    "# Run original LangGraph Agent:\n",
    "user_input = \"Can you give me some information about AMD in 2022?\"\n",
    "\n",
    "result = graph.invoke({\"messages\": [(\"user\", user_input)]})\n",
    "for message in result[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this Agent's graph, we can see that it first selects potential tools it could use based on the user query, and then calls the Agent to process the user's query. The Agent can use any of the selected tools and then loop back onto itself to process the results and return a response to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![implementation-raw](./assets/tlm-existing-agent/implementation-raw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add trust scoring to this Agent. We again create a TLM verifier node (in the form of a post-model hook) and add it to the graph right after the Agent.\n",
    "\n",
    "Note: you can alternatively use a conditional edge to check response trustworthiness and only move onto the handler node if the response is untrustworthy, handling routing in the edge rather than the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(State)\n",
    "builder.add_node(\"agent\", agent)\n",
    "builder.add_node(\"select_tools\", select_tools)\n",
    "\n",
    "tool_node = ToolNode(tools=tools)\n",
    "builder.add_node(\"tools\", tool_node)\n",
    "builder.add_edge(START, \"select_tools\")\n",
    "\n",
    "# Build the TLM reviewer\n",
    "builder.add_node(\"post_model_hook\", build_tlm_verifier(handle_untrustworthy_response=handle_untrustworthy_response))\n",
    "builder.add_edge(\"agent\", \"post_model_hook\")\n",
    "\n",
    "# Connect the TLM reviewer to the tools instead of the agent\n",
    "builder.add_conditional_edges(\"post_model_hook\", tools_condition, path_map=[\"tools\", \"__end__\"])\n",
    "\n",
    "builder.add_edge(\"tools\", \"agent\")\n",
    "builder.add_edge(\"select_tools\", \"agent\")\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the new graph, we see that the post-model hook has been added after the Agent, so all of the Agent's internal LLM responses have their trustworthiness scored by TLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![implementation-tlm](./assets/tlm-existing-agent/implementation-tlm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the Agent with trust scoring in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Can you give me some information about AMD in 2022?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  Advanced_Micro_Devices (call_CASyCvTjhjkDxEyAOIyrsBEM)\n",
      " Call ID: call_CASyCvTjhjkDxEyAOIyrsBEM\n",
      "  Args:\n",
      "    year: 2022\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: Advanced_Micro_Devices\n",
      "\n",
      "Advanced Micro Devices had revenues of $100 in 2022.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "In 2022, Advanced Micro Devices had revenues of $100.\n",
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Your last response was not trustworthy. Rewrite your response to be more trustworthy.\n",
      "Reason: The Assistant's answer states that \"In 2022, Advanced Micro Devices had revenues of $100.\" This information is directly taken from the function call response, which says: \"Advanced Micro Devices had revenues of $100 in 2022.\" However, this figure is suspiciously low for a company like AMD, which is a major semiconductor firm with revenues in the billions of dollars annually. The number \"$100\" is likely a placeholder or an incomplete data point rather than an accurate financial figure.\n",
      "\n",
      "Given the context, the Assistant simply repeated the output from the function call without verifying or contextualizing the data. Since the function response is presumably from a data source or database, it might be incomplete or incorrectly formatted. The Assistant did not clarify the units (e.g., million, billion) or mention that the figure seems unusually low.\n",
      "\n",
      "Therefore, while the Assistant's answer is factually consistent with the provided function response, the response itself is almost certainly incorrect or misleading in a real-world context. The Assistant should have either flagged the data as suspicious or sought clarification.\n",
      "\n",
      "Given the instructions, the Assistant's answer is correct relative to the provided data but factually incorrect in reality. Since the task is to verify factual correctness, the answer is not accurate.\n",
      "\n",
      "Hence, the confidence in the answer's correctness is very low.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I have retrieved some information about AMD for 2022. Advanced Micro Devices had revenues of $100 in 2022. Please note that this figure seems unusually low for a company like AMD, which typically has revenues in the billions of dollars annually.\n",
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Your last response was not trustworthy. Rewrite your response to be more trustworthy.\n",
      "Reason: Cannot verify that this response is correct.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "For detailed information about AMD in 2022, please provide me with more specific details or data points you are interested in knowing.\n"
     ]
    }
   ],
   "source": [
    "user_input = \"Can you give me some information about AMD in 2022?\"\n",
    "\n",
    "result = graph.invoke({\"messages\": [(\"user\", user_input)]})\n",
    "for message in result[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With trust scoring in place, we see that the Agent realizes that $100 revenue for a company like AMD is untrustworthy and will tell the user as such.\n",
    "\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "This tutorial demonstrated how you can score LLM trustworthiness in any pre-built LangGraph Agent. In the Agentic systems we showcased here, low trust scores were handled via an automated internal message to the Agent asking it to re-generate its previous response to be more trustworthy. This accuracy-boosting technique is only one of many possible fallback mechanisms (replace with canned response like *Sorry I don't know*, escalate to human, restart the Agent, etc).\n",
    "\n",
    "To see different fallbacks you can implement when TLM trust scores are low, along with trust scoring in a custom LangGraph Agent built from scratch, check out our [Trustworthy Custom Agents tutorial](/tlm/use-cases/tlm_agents)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}