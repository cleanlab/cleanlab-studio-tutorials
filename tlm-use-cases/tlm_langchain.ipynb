{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add LLM Trustworthiness Scoring to any LangChain or LangGraph application\n",
    "\n",
    "\n",
    "<head>\n",
    "  <meta name=\"title\" content=\"Add LLM Trustworthiness Scoring to any LangChain or LangGraph application\"/>\n",
    "  <meta property=\"og:title\" content=\"Add LLM Trustworthiness Scoring to any LangChain or LangGraph application\"/>\n",
    "  <meta name=\"twitter:title\" content=\"Add LLM Trustworthiness Scoring to any LangChain or LangGraph application\" />\n",
    "  <meta name=\"image\" content=\"/img/langchain.png\" />\n",
    "  <meta property=\"og:image\" content=\"/img/langchain.png\" />\n",
    "  <meta name=\"description\" content=\"How to mitigate unchecked hallucination in your LangChain apps.\"  />\n",
    "  <meta property=\"og:description\" content=\"How to mitigate unchecked hallucination in your LangChain apps.\" />\n",
    "  <meta name=\"twitter:description\" content=\"How to mitigate unchecked hallucination in your LangChain apps.\" />\n",
    "</head>\n",
    "\n",
    "LLMs are prone to unchecked *hallucinations* where they occasionally produce plausible but incorrect responses. \n",
    "The [Trustworthy Language Model](https://cleanlab.ai/blog/trustworthy-language-model/) (TLM) scores the trustworthiness of responses from *any* LLM using state-of-the-art model uncertainty estimation techniques.\n",
    "\n",
    "This tutorial shows how to produce a real-time trustworthiness score for every LLM call in a simple LangChain application.\n",
    "The same method can be applied in any LangChain/LangGraph applications, to migitate unchecked hallucination.\n",
    "\n",
    "![LLM trustworthiness scoring in LangChain](./assets/tlm-langchain-tutorial/langchain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the LangChain, LangGraph, OpenAI, and Cleanlab Studio Python libraries for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain_core langchain_openai langgraph cleanlab_studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "from cleanlab_studio import Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a simple chain\n",
    "\n",
    "This tutorial is based on [LangChain's chatbot tutorial](https://python.langchain.com/docs/how_to/chatbots_memory/). Start there if you aren't familiar with LangChain or LangGraph.\n",
    "\n",
    "Let's create a simple conversational chain using standard concepts like: `prompt_template`, `graph`, `memory`, and `model`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt template with placeholder for messages\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define graph for managing state\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# Function that calls the LLM given current state\n",
    "def call_model(state: MessagesState):\n",
    "    # Given the current state of messages, prompt the model\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "# Define single node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we instantiate the LLM object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o-mini\", api_key=\"<openai_api_key>\")\n",
    "\n",
    "# config ID to capture unique state\n",
    "config = {\"configurable\": {\"thread_id\": \"without-tlm\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple chain offers conversations with the LLM. Let\u2019s ask an example question about lakes in California."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The biggest lake in California is Lake Tahoe. It is renowned for its stunning clarity, beautiful scenery, and recreational opportunities. Lake Tahoe is situated in the Sierra Nevada mountain range, straddling the border between California and Nevada. It is also one of the largest alpine lakes in North America.\n"
     ]
    }
   ],
   "source": [
    "query = \"Which is the biggest lake in California?\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being a stateful chain with memory, we can ask follow-up questions and the LLM will automatically refer back to previous messages as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Lake Tahoe is approximately 1,645 feet (501 meters) deep, making it the second-deepest lake in the United States, after Crater Lake in Oregon. Its significant depth contributes to its striking blue color and clarity.\n"
     ]
    }
   ],
   "source": [
    "query = \"How deep is it?\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far the responses from the LLM looks good. Let's integrate trustworthiness scoring to this chain to see if there's a scope for hallucination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add trustworthiness scoring\n",
    "\n",
    "It's easy to integrate TLM's trustworthiness score in *any* existing chain, irrespective of the LLM model (streaming or batch, async or not). \n",
    "\n",
    "We simply define a [Langchain callback](https://python.langchain.com/docs/concepts/callbacks/) that triggers whenever the LLM generates a response. \n",
    "TLM considers both the prompt (with system, user, context messages) and the LLM response in its trustworthiness scoring.\n",
    "\n",
    "Let's define this custom callback handler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrustworthinessScoreCallback(BaseCallbackHandler):\n",
    "    def __init__(self, tlm, threshold = 0.7, explanation = True):\n",
    "        # Cleanlab's TLM client\n",
    "        self.tlm = tlm\n",
    "        # Keep track of the prompt\n",
    "        self.prompt = \"\"\n",
    "        # Threshold to trigger actions\n",
    "        self.threshold = threshold\n",
    "        # Boolean to enable/disable explanation\n",
    "        self.explanation = explanation\n",
    "    \n",
    "    def on_llm_start(self, serialized, prompts, run_id, **kwargs):\n",
    "        # Store input prompt\n",
    "        self.prompt = prompts[0]\n",
    "    \n",
    "    def on_llm_end(self, response, **kwargs):\n",
    "        # Extract response text from LLMResult object\n",
    "        response_text = response.generations[0][0].text\n",
    "\n",
    "        # Call trustworthiness score method, and extract the score\n",
    "        resp = self.tlm.get_trustworthiness_score(self.prompt, response_text)\n",
    "        score = resp['trustworthiness_score']\n",
    "\n",
    "        # Log score\n",
    "        # This can be replaced with any action that the application requires\n",
    "        # We just print it along with its tag, trustworthy or otherwise\n",
    "        if score < self.threshold:\n",
    "            print(f\"[TLM Score]: {score} (Untrustworthy)\")\n",
    "        else:\n",
    "            print(f\"[TLM Score]: {score} (Trustworthy)\")\n",
    "\n",
    "        # Log explanation\n",
    "        # Reasoning for the predicted trustworthiness score\n",
    "        if self.explanation and resp.get(\"log\", {}).get(\"explanation\"):\n",
    "            print(f\"[TLM Score Explanation]: {resp['log']['explanation']}\")\n",
    "        elif self.explanation:\n",
    "            print(\"[TLM Warning]: Enable `explanation` in TLM client.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then instantiate the TLM object used by this callback. We specify `explanation` in TLM's `options` argument to obtain rationales for why LLM responses are deemed untrustworthy.\n",
    "\n",
    "Note: You can get a free Cleanlab API key [here](https://tlm.cleanlab.ai/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Studio(api_key=\"<cleanlab_api_key>\")  # Get free API key from: https://tlm.cleanlab.ai/account after creating an account\n",
    "tlm = client.TLM(options={'log':['explanation']})  # You can also omit explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we instantiate an instance of the callback with the TLM object. Currently, the callback would print the trustworthiness score and explanation, but you can modify what actions to take based on the trustworthiness score to meet your needs.\n",
    "For instance, you might threshold the scores and revert to some fallback action whenever LLM responses are deemed untrustworthy\n",
    "\n",
    "The callback is attached to the LLM so it triggers whenever the LLM is called in a chain.\n",
    "You can attach the callback to multiple LLMs, agents, and other objects that generate a responses when given a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trustworthiness_callback = TrustworthinessScoreCallback(tlm, explanation=True)\n",
    "model.callbacks = [trustworthiness_callback]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's reset the memory of the assistant and ask the same questions, now with trustworthiness scoring enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = workflow.compile(checkpointer=MemorySaver())\n",
    "config = {\"configurable\": {\"thread_id\": \"with-tlm\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TLM Score]: 0.6246266556912332 (Untrustworthy)\n",
      "[TLM Score Explanation]: The proposed answer states that Lake Tahoe is the biggest lake in California. However, this is incorrect because the largest lake entirely within California is actually the Salton Sea. Lake Tahoe, while it is a significant and well-known lake, is not the largest lake in California when considering the size of lakes that are entirely within the state's borders. Therefore, the answer provided is factually incorrect. \n",
      "This response is untrustworthy due to lack of consistency in possible responses from the model. Here's one inconsistent alternate response that the model considered (which may not be accurate either): \n",
      "The biggest lake in California is the Salton Sea.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The biggest lake in California is Lake Tahoe. It is known for its stunning clear waters and is located in the Sierra Nevada mountain range, straddling the border between California and Nevada. Lake Tahoe is also one of the largest alpine lakes in North America.\n"
     ]
    }
   ],
   "source": [
    "query = \"Which is the biggest lake in California?\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TLM Score]: 0.9875591947901597 (Trustworthy)\n",
      "[TLM Score Explanation]: Did not find a reason to doubt trustworthiness.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Lake Tahoe has a maximum depth of about 1,645 feet (501 meters), making it the second-deepest lake in the United States, after Crater Lake in Oregon. Its depth contributes to its clarity and unique environmental conditions.\n"
     ]
    }
   ],
   "source": [
    "query = \"How deep is it?\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the LLM hallucinated for this simple question. The low trustworthiness score from TLM helps you automatically catch such problems in real-time.\n",
    "\n",
    "TLM optionally provides explanations for its trustworthiness score. You can disable explanations by setting `TrustworthinessScoreCallback(tlm, explanation=False)`.\n",
    "\n",
    "Feel free to modify this callback to meet your application's needs. Mitgating unchecked hallucinations is a key step toward reliable AI applications!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "a0a0263b650d907a3bfe41c0f8d6a63a071b884df3cfdc1579f00cdc1aed6b03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}