{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trustworthiness Scoring for LangChain\n",
    "\n",
    "This tutorial shows how to use TLM's trustworthiness score in LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need LangChain, LangGraph, and OpenAI python libraries for this tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain_core langchain_openai langgraph cleanlab_studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "from cleanlab_studio import Studio\n",
    "\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a simple chain\n",
    "\n",
    "This tutorial builds upon [LangChain's chatbot tutorial](https://python.langchain.com/docs/how_to/chatbots_memory/) as a starting point.\n",
    "\n",
    "Let's first create a simple conversational chain using LangChain's most commonly used objects like `prompt_template`, `graph`, `memory`, and `model`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt template with placeholder for messages\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define graph for managing state\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# Function that calls the LLM given current state\n",
    "def call_model(state: MessagesState):\n",
    "    # Given the current state of messages, prompt the model\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "# Define single node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we instantiate the LLM assitant. We can then ask questions to this assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o-mini\", api_key=\"<openai_api_key>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The biggest lake in California is Lake Tahoe. It is renowned for its stunning clarity, beautiful scenery, and recreational opportunities. Lake Tahoe is situated in the Sierra Nevada mountain range, straddling the border between California and Nevada. It is also one of the largest alpine lakes in North America.\n"
     ]
    }
   ],
   "source": [
    "# config ID to capture unique state\n",
    "config = {\"configurable\": {\"thread_id\": \"without-tlm\"}}\n",
    "\n",
    "query = \"Which is the biggest lake in California?\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being a stateful assistant with memory, we can ask follow-up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Lake Tahoe is approximately 1,645 feet (501 meters) deep, making it the second-deepest lake in the United States, after Crater Lake in Oregon. Its significant depth contributes to its striking blue color and clarity.\n"
     ]
    }
   ],
   "source": [
    "query = \"How deep is it?\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating trustworthiness check\n",
    "\n",
    "It's easy to integrate TLM's trustworthiness score in an existing custom-built chain (using any other LLM generator, streaming or not). \n",
    "\n",
    "In this case, we'd define a [Langchain callback](https://python.langchain.com/docs/concepts/callbacks/) that would trigger when the LLM finishes generating response. \n",
    "Both the prompt (with system, user, context messages) and the response are used to calculate the score.\n",
    "\n",
    "Let's define this custom callback handler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrustworthinessScoreCallback(BaseCallbackHandler):\n",
    "    def __init__(self, tlm, threshold = 0.7, explanation = True):\n",
    "        self.tlm = tlm\n",
    "        # Keep track of the prompt\n",
    "        self.prompt = \"\"\n",
    "        # Threshold for triggering actions\n",
    "        self.threshold = threshold\n",
    "        # Boolean to enable/disable explanation\n",
    "        self.explanation = explanation\n",
    "    \n",
    "    def on_llm_start(self, serialized, prompts, run_id, **kwargs):\n",
    "        # Store input prompt\n",
    "        self.prompt = prompts[0]\n",
    "    \n",
    "    def on_llm_end(self, response, **kwargs):\n",
    "        # Extract response text from LLMResult object which is the return type for most LLMs\n",
    "        response_text = response.generations[0][0].text\n",
    "\n",
    "        # Call trustworthiness score method, and extract the score\n",
    "        resp = self.tlm.get_trustworthiness_score(self.prompt, response_text)\n",
    "        score = resp['trustworthiness_score']\n",
    "\n",
    "        # Log score\n",
    "        # This can be replaced with any action that the application requires\n",
    "        if score < self.threshold:\n",
    "            print(f\"[TLM Score]: {score} (Untrustworthy)\")\n",
    "        else:\n",
    "            print(f\"[TLM Score]: {score} (Trustworthy)\")\n",
    "\n",
    "        # Log explanation\n",
    "        # This can be replaced with any action that the application requires\n",
    "        if self.explanation and resp.get(\"log\", {}).get(\"explanation\"):\n",
    "            print(f\"[TLM Score Explanation]: {resp['log']['explanation']}\")\n",
    "        elif self.explanation:\n",
    "            print(\"[TLM Warning]: Enable `explanation` in TLM client.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then instantiate the Cleanlab Studio client -- you can get your free Cleanlab API key [here](https://app.cleanlab.ai/).\n",
    "\n",
    "We would also instantiate the TLM object that would be used by this callback, and specify that we want explanations in the `options` argument to obtain explanations in the TLM response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Studio(api_key=\"<cleanlab_api_key>\")\n",
    "tlm = client.TLM(options={'log':['explanation']}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will define an instance of the call function with the TLM object, setting `explanation=True`. Currently, the callback would print the trustworthiness score and explanation, but you can modify the action basis your requirements.\n",
    "\n",
    "We'll attach this callback to the LLM so that it triggers when the LLM is called. \n",
    "You can attach the callback to multiple LLMs, agents, and other objects that generates a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trustworthiness_callback = TrustworthinessScoreCallback(tlm, explanation=True)\n",
    "model.callbacks = [trustworthiness_callback]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reset the memory of the assistant and ask the same questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"with-tlm\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TLM Score]: 0.6246266556912332 (Untrustworthy)\n",
      "[TLM Score Explanation]: The proposed answer states that Lake Tahoe is the biggest lake in California. However, this is incorrect because the largest lake entirely within California is actually the Salton Sea. Lake Tahoe, while it is a significant and well-known lake, is not the largest lake in California when considering the size of lakes that are entirely within the state's borders. Therefore, the answer provided is factually incorrect. \n",
      "This response is untrustworthy due to lack of consistency in possible responses from the model. Here's one inconsistent alternate response that the model considered (which may not be accurate either): \n",
      "The biggest lake in California is the Salton Sea.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The biggest lake in California is Lake Tahoe. It is known for its stunning clear waters and is located in the Sierra Nevada mountain range, straddling the border between California and Nevada. Lake Tahoe is also one of the largest alpine lakes in North America.\n"
     ]
    }
   ],
   "source": [
    "query = \"Which is the biggest lake in California?\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TLM Score]: 0.9875591947901597 (Trustworthy)\n",
      "[TLM Score Explanation]: Did not find a reason to doubt trustworthiness.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Lake Tahoe has a maximum depth of about 1,645 feet (501 meters), making it the second-deepest lake in the United States, after Crater Lake in Oregon. Its depth contributes to its clarity and unique environmental conditions.\n"
     ]
    }
   ],
   "source": [
    "query = \"How deep is it?\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed above, hallucinations in LLM outputs can be automatically detected using TLM's trustworthiness score\n",
    "\n",
    "TLM also provides explanations in complement to the trustworthiness score. You can disable it by setting `TrustworthinessScoreCallback(tlm, explanation=False)`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "a0a0263b650d907a3bfe41c0f8d6a63a071b884df3cfdc1579f00cdc1aed6b03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}