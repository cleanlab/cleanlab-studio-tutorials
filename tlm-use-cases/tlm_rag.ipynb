{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trustworthy Retrieval-Augmented Generation with Cleanlab\n",
    "\n",
    "<head>\n",
    "  <meta name=\"title\" content=\"Trustworthy Retrieval-Augmented Generation\"/>\n",
    "  <meta property=\"og:title\" content=\"Trustworthy Retrieval-Augmented Generation\"/>\n",
    "  <meta name=\"twitter:title\" content=\"Trustworthy Retrieval-Augmented Generation\" />\n",
    "  <meta name=\"image\" content=\"/img/rag.png\" />\n",
    "  <meta property=\"og:image\" content=\"/img/rag.png\" />\n",
    "  <meta name=\"description\" content=\"How to develop a reliable RAG system that monitors the trustworthiness of every answer.\"  />\n",
    "  <meta property=\"og:description\" content=\"How to develop a reliable RAG system that monitors the trustworthiness of every answer.\" />\n",
    "  <meta name=\"twitter:description\" content=\"How to develop a reliable RAG system that monitors the trustworthiness of every answer.\" />\n",
    "</head>\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** allows LLMs to answer domain-specific *queries* by retrieving relevant *context* (documents) from a knowledge base, and then concatenating query + context within the prompt used by the LLM to generate a final *response*. Cleanlab evaluates the trustworthiness of every RAG response in real-time, combining scores for each response with additional Evals for other RAG components like the retrieved context. Cleanlab works with *any* RAG architecture, retrieval/indexing methodology, and LLM model.\n",
    "\n",
    "![TLM adds a trustworthiness score to every RAG response](./assets/tlm-rag-tutorial/tlm-rag-overview.png)\n",
    "\n",
    "## Overview\n",
    "\n",
    "Here's all the code needed for trustworthy RAG:\n",
    "```python\n",
    "from cleanlab_tlm import TrustworthyRAG\n",
    "trustworthy_rag = TrustworthyRAG()  # optional configurations can improve: latency, accuracy, explanations\n",
    "\n",
    "# Your existing RAG code:\n",
    "context = rag_retrieve_context(user_query)\n",
    "prompt = rag_form_prompt(user_query, retrieved_context)\n",
    "response = rag_generate_response(prompt)\n",
    "\n",
    "# Detect issues with Cleanlab:\n",
    "results = trustworthy_rag.score(query=query, context=context, response=response, form_prompt=rag_form_prompt)\n",
    "```\n",
    "\n",
    "The returned `results` will be a dict with keys like: **'trustworthiness'**, **'response_helpfulness'**, **'context_sufficiency'**, ... Each points to a quality score between 0-1 that evaluates one type of issue in your RAG system.\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "This tutorial requires a TLM API key. Get one [here](https://tlm.cleanlab.ai/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "%pip install -U cleanlab-tlm pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API key\n",
    "import os\n",
    "os.environ[\"CLEANLAB_TLM_API_KEY\"] = \"<API key>\"  # Get your free API key from: https://tlm.cleanlab.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from cleanlab_tlm import TrustworthyRAG, Eval, get_default_evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example RAG Use-Case\n",
    "\n",
    "Given a user *query*, let's assume that your RAG system: retrieves relevant *context* from a knowledge base, formats a LLM *prompt* based on the query and context (plus auxiliary system *instructions*), and generates a *response* using this prompt. You can run this tutorial no matter what RAG architecture or LLM model you're using!\n",
    "\n",
    "For this tutorial, we'll consider a customer support example RAG use-case, loading an example dataset of: *query*, *context*, *response* values. For simplicity, our retrieved context is hardcoded as a single customer service policy document. Replace our examples with the outputs of your RAG system, and Cleanlab will detect issues in your outputs in real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c5a5e7-8483-4091-9418-4971d93d5a06",
   "metadata": {},
   "source": [
    "**Optional: Example queries, retrieved contexts, and generated responses from a RAG system (stored in DataFrame)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "customer_service_policy = \"\"\"The following is the customer service policy of ACME Inc.\n",
    "# ACME Inc. Customer Service Policy\n",
    "\n",
    "## Table of Contents\n",
    "1. Free Shipping Policy\n",
    "2. Free Returns Policy\n",
    "3. Fraud Detection Guidelines\n",
    "4. Customer Interaction Tone\n",
    "\n",
    "## 1. Free Shipping Policy\n",
    "\n",
    "### 1.1 Eligibility Criteria\n",
    "- Free shipping is available on all orders over $50 within the continental United States.\n",
    "- For orders under $50, a flat rate shipping fee of $5.99 will be applied.\n",
    "- Free shipping is not available for expedited shipping methods (e.g., overnight or 2-day shipping).\n",
    "\n",
    "### 1.2 Exclusions\n",
    "- Free shipping does not apply to orders shipped to Alaska, Hawaii, or international destinations.\n",
    "- Oversized or heavy items may incur additional shipping charges, which will be clearly communicated to the customer before purchase.\n",
    "\n",
    "### 1.3 Handling Customer Inquiries\n",
    "- If a customer inquires about free shipping eligibility, verify the order total and shipping destination.\n",
    "- Inform customers of ways to qualify for free shipping (e.g., adding items to reach the $50 threshold).\n",
    "- For orders just below the threshold, you may offer a one-time courtesy free shipping if it's the customer's first purchase or if they have a history of large orders.\n",
    "\n",
    "## 2. Free Returns Policy\n",
    "\n",
    "### 2.1 Eligibility Criteria\n",
    "- Free returns are available for all items within 30 days of the delivery date.\n",
    "- Items must be unused, unworn, and in their original packaging with all tags attached.\n",
    "- Free returns are limited to standard shipping methods within the continental United States.\n",
    "\n",
    "### 2.2 Exclusions\n",
    "- Final sale items, as marked on the product page, are not eligible for free returns.\n",
    "- Customized or personalized items are not eligible for free returns unless there is a manufacturing defect.\n",
    "- Undergarments, swimwear, and earrings are not eligible for free returns due to hygiene reasons.\n",
    "\n",
    "### 2.3 Process for Handling Returns\n",
    "1. Verify the order date and ensure it falls within the 30-day return window.\n",
    "2. Ask the customer about the reason for the return and document it in the system.\n",
    "3. Provide the customer with a prepaid return label if they qualify for free returns.\n",
    "4. Inform the customer of the expected refund processing time (5-7 business days after receiving the return).\n",
    "\n",
    "### 2.4 Exceptions\n",
    "- For items damaged during shipping or with manufacturing defects, offer an immediate replacement or refund without requiring a return.\n",
    "- For returns outside the 30-day window, use discretion based on the customer's history and the reason for the late return. You may offer store credit as a compromise.\n",
    "\n",
    "## 3. Fraud Detection Guidelines\n",
    "\n",
    "### 3.1 Red Flags for Potential Fraud\n",
    "- Multiple orders from the same IP address with different customer names or shipping addresses.\n",
    "- Orders with unusually high quantities of the same item.\n",
    "- Shipping address different from the billing address, especially if in different countries.\n",
    "- Multiple failed payment attempts followed by a successful one.\n",
    "- Customers pressuring for immediate shipping or threatening to cancel the order.\n",
    "\n",
    "### 3.2 Verification Process\n",
    "1. For orders flagging as potentially fraudulent, place them on hold for review.\n",
    "2. Verify the customer's identity by calling the phone number on file.\n",
    "3. Request additional documentation (e.g., photo ID, credit card statement) if necessary.\n",
    "4. Cross-reference the shipping address with known fraud databases.\n",
    "\n",
    "### 3.3 Actions for Confirmed Fraud\n",
    "- Cancel the order immediately and refund any charges.\n",
    "- Document the incident in the customer's account and flag it for future reference.\n",
    "- Report confirmed fraud cases to the appropriate authorities and credit card companies.\n",
    "\n",
    "### 3.4 False Positives\n",
    "- If a legitimate customer is flagged, apologize for the inconvenience and offer a small discount or free shipping on their next order.\n",
    "- Document the incident to improve our fraud detection algorithms.\n",
    "\n",
    "## 4. Customer Interaction Tone\n",
    "\n",
    "### 4.1 General Guidelines\n",
    "- Always maintain a professional, friendly, and empathetic tone.\n",
    "- Use the customer's name when addressing them.\n",
    "- Listen actively and paraphrase the customer's concerns to ensure understanding.\n",
    "- Avoid negative language; focus on what can be done rather than what can't.\n",
    "\n",
    "### 4.2 Specific Scenarios\n",
    "\n",
    "#### Angry or Frustrated Customers\n",
    "- Remain calm and do not take comments personally.\n",
    "- Acknowledge the customer's feelings and apologize for their negative experience.\n",
    "- Focus on finding a solution and clearly explain the steps you'll take to resolve the issue.\n",
    "- If necessary, offer to escalate the issue to a supervisor.\n",
    "\n",
    "#### Confused or Indecisive Customers\n",
    "- Be patient and offer clear, concise explanations.\n",
    "- Ask probing questions to better understand their needs.\n",
    "- Provide options and explain the pros and cons of each.\n",
    "- Offer to send follow-up information via email if the customer needs time to decide.\n",
    "\n",
    "#### VIP or Loyal Customers\n",
    "- Acknowledge their status and thank them for their continued business.\n",
    "- Be familiar with their purchase history and preferences.\n",
    "- Offer exclusive deals or early access to new products when appropriate.\n",
    "- Go above and beyond to exceed their expectations.\n",
    "\n",
    "### 4.3 Language and Phrasing\n",
    "- Use positive language: \"I'd be happy to help you with that\" instead of \"I can't do that.\"\n",
    "- Avoid technical jargon or abbreviations that customers may not understand.\n",
    "- Use \"we\" statements to show unity with the company: \"We value your feedback\" instead of \"The company values your feedback.\"\n",
    "- End conversations on a positive note: \"Is there anything else I can assist you with today?\"\n",
    "\n",
    "### 4.4 Written Communication\n",
    "- Use proper grammar, spelling, and punctuation in all written communications.\n",
    "- Keep emails and chat responses concise and to the point.\n",
    "- Use bullet points or numbered lists for clarity when providing multiple pieces of information.\n",
    "- Include a clear call-to-action or next steps at the end of each communication.\n",
    "\n",
    "The following dialogue features a discussion between a user and a customer service bot. The bot attempts to help the customer but must respect the guidelines in the customer service policy. The bot provides very accurate and concise answers. The bot does not tell the user to contact customer service\n",
    "Remember, as a representative of ACME Inc., you are often the first point of contact for our customers. Your interactions should always reflect our commitment to exceptional customer service and satisfaction.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Example queries and retrieved context + generated response\n",
    "data = [\n",
    "    {\n",
    "        \"query\": \"What's your free shipping policy for orders within the continental US?\",\n",
    "        \"context\": customer_service_policy,\n",
    "        \"response\": \"We offer free shipping on all orders over $50 within the continental United States. Orders under $50 have a flat rate shipping fee of $5.99. Expedited shipping methods, oversized items, and orders to Alaska or Hawaii don't qualify for free shipping.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is your return policy?\",\n",
    "        \"context\": customer_service_policy,\n",
    "        \"response\": \"We offer a 90-day return policy with full refunds on all purchases. Returns must be in original condition with packaging. We provide prepaid return labels and process refunds within 3 business days of receiving your return.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What are your product warranty periods?\",\n",
    "        \"context\": customer_service_policy,\n",
    "        \"response\": \"Thank you for your inquiry about our warranty periods. I'd be happy to assist you with that. While I don't have the specific warranty information in our current policy document, we do offer warranties on most products. For detailed warranty information about specific products, I recommend checking the product packaging or documentation that came with your purchase. Is there a particular product you're inquiring about?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, your RAG system should already have functions to retrieve context and generate responses. For this tutorial, we'll simulate such functions based on the examples above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b2a3c4-2e8e-49a3-9cd2-66dd98fbce87",
   "metadata": {},
   "source": [
    "**Optional: Toy RAG methods you should replace with existing methods from your RAG system**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rag_retrieve_context(query):\n",
    "    \"\"\"Simulate retrieval from a knowledge base\"\"\"\n",
    "    # In a real system, this would search the knowledge base\n",
    "    for item in data:\n",
    "        if item[\"query\"] == query:\n",
    "            return item[\"context\"]\n",
    "    return \"\"\n",
    "\n",
    "def rag_form_prompt(query, context):\n",
    "    \"\"\"Format a prompt used by your RAG system's LLM to generate response based on query and retrieved context. Note that you'll want to include any system instructions to your LLM here as well (eg. to specify desired tone/formatting of responses).\"\"\"\n",
    "    return f\"\"\"You are a customer service agent for ACME Inc. Your task is to answer the following customer question based on the customer service policy.\n",
    "\n",
    "Customer Service Policy: {context}\n",
    "Customer Question: {query}\n",
    "\"\"\"\n",
    "\n",
    "def rag_generate_response(prompt):\n",
    "    \"\"\"Simulate LLM response generation\"\"\"\n",
    "    # In a real system, this would call an LLM\n",
    "    query = prompt.split(\"Customer Question: \")[1].split(\"\\n\")[0]\n",
    "    for item in data:\n",
    "        if item[\"query\"] == query:\n",
    "            return item[\"response\"]\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-time Evaluation using TrustworthyRAG\n",
    "\n",
    "Cleanlab's [TrustworthyRAG](/tlm/api/python/utils.rag/) runs many real-time evaluations to detect issues in your RAG system. It runs Cleanlab's state-of-the-art LLM uncertainty estimator, the [Trustworthy Language Model](https://cleanlab.ai/tlm/), to provide a **trustworthiness score** indicating overall confidence that your RAG system's response is *correct*. TrustworthyRAG can simultaneously run additional evaluations to diagnose *why* responses are likely incorrect or other types of issues. Let's see what Evals are run by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\n",
      "    'name': 'context_sufficiency',\n",
      "    'criteria': 'Determine if the Document contains 100% of the information needed to answer the Question. If any external knowledge or assumptions are required, it does not meet the criteria. Each Question component must have explicit support in the Document.',\n",
      "    'query_identifier': 'Question',\n",
      "    'context_identifier': 'Document',\n",
      "    'response_identifier': None\n",
      "}, {\n",
      "    'name': 'response_groundedness',\n",
      "    'criteria': 'Review the Response to the Query and assess whether every factual claim in the Response is explicitly supported by the provided Context. A Response meets the criteria if all information is directly backed by evidence in the Context, without relying on assumptions, external knowledge, or unstated inferences. The focus is on whether the Response is fully grounded in the Context, rather than whether it fully addresses the Query. If any claim in the Response lacks direct support or introduces information not present in the Context, the Response is bad and does not meet the criteria.',\n",
      "    'query_identifier': 'Query',\n",
      "    'context_identifier': 'Context',\n",
      "    'response_identifier': 'Response'\n",
      "}, {\n",
      "    'name': 'response_helpfulness',\n",
      "    'criteria': 'Assess whether the AI Assistant Response is a helpful answer to the User Query.\n",
      "A Response is considered helpful if it makes a genuine attempt to answer the question, even if the answer is incorrect or incomplete. Factual inaccuracies should not affect the assessment. The only thing that matters is whether the Assistant tries to answer the question.\n",
      "A Response is considered not helpful if it avoids answering the question. For example, by saying or implying things like \"I don't know\", \"Sorry\", \"No information available\", or any other form of refusal or deflection.',\n",
      "    'query_identifier': 'User Query',\n",
      "    'context_identifier': None,\n",
      "    'response_identifier': 'AI Assistant Response'\n",
      "}, {\n",
      "    'name': 'query_ease',\n",
      "    'criteria': 'Determine whether the above User Request appears simple and straightforward. A bad User Request will appear either: disgruntled, complex, purposefully tricky, abnormal, or vague, perhaps missing vital information needed to answer properly. The simpler the User Request appears, the better. If you believe an AI Assistant could correctly answer this User Request, it is considered good. If the User Request is non-propositional language, it is also considered good.',\n",
      "    'query_identifier': 'User Request',\n",
      "    'context_identifier': None,\n",
      "    'response_identifier': None\n",
      "}]\n"
     ]
    }
   ],
   "source": [
    "default_evals = get_default_evals()\n",
    "print(default_evals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Eval returns a score between 0-1 (higher is better) that assesses a different aspect of your RAG system:\n",
    "\n",
    "1. **context_sufficiency**: Evaluates whether the retrieved context contains sufficient information to answer the query. A low score indicates that key information is missing from the context (useful to diagnose **search/retrieval failures** or **knowledge gaps**).\n",
    "\n",
    "2. **response_groundedness**: Evaluates whether claims/information stated in the response are explicitly supported by the provided context (useful to diagnose when your LLM is **fabricating claims** or **relying on its internal world knowledge** over the information retrieved from your knowledge base).\n",
    "\n",
    "3. **response_helpfulness**: Evaluates whether the response attempts to answer the user's query or instead abstain from answering (useful to detect responses unlikely to satisfy the user like **generic fallbacks**).\n",
    "\n",
    "4. **query_ease**: Evaluates whether the user query seems easy for an AI system to properly handle (useful to diagnose queries that are: complex, vague, tricky, or disgruntled-sounding).\n",
    "\n",
    "We recommend using the **trustworthiness** score to automatically flag potentially incorrect responses, and these additional Evals to diagnose *what part* of the RAG system led to a bad response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating RAG Responses with TrustworthyRAG.score()\n",
    "\n",
    "You can evaluate every response from your RAG system using TrustworthyRAG's `score()` method. Here we do this using a helper function that evaluates one row (query + context + response example) from our earlier dataframe of examples. You can use the `score()` method however best suits your RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_df_row(df, row_index, evaluator, verbose=False):\n",
    "    \"\"\"\n",
    "    Evaluate a specific row from the dataframe using TrustworthyRAG\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing queries\n",
    "        row_index: Index of the row to evaluate\n",
    "        evaluator: TrustworthyRAG instance to use for evaluation\n",
    "        verbose (bool, optional): Whether to print detailed output. Defaults to False.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Evaluation results\n",
    "    \"\"\"\n",
    "    # Select a query to evaluate\n",
    "    user_query = df.iloc[row_index][\"query\"]\n",
    "    print(f\"Query: {user_query}\\n\")\n",
    "    \n",
    "    # Get the retrieved context\n",
    "    retrieved_context = rag_retrieve_context(user_query)\n",
    "    if verbose:\n",
    "        print(f\"Retrieved context:\\n{retrieved_context}\\n\")\n",
    "    \n",
    "    # Format the RAG prompt\n",
    "    rag_prompt = rag_form_prompt(user_query, retrieved_context)\n",
    "    \n",
    "    # Get the LLM response\n",
    "    llm_response = rag_generate_response(rag_prompt)\n",
    "    print(f\"Generated response: {llm_response}\\n\")\n",
    "    \n",
    "    # Evaluate the response (this is the only code you need to add to your RAG system)\n",
    "    result = evaluator.score(\n",
    "        query=user_query,\n",
    "        context=retrieved_context,\n",
    "        response=llm_response,\n",
    "        form_prompt=rag_form_prompt\n",
    "    )\n",
    "    \n",
    "    print(\"Evaluation results:\")\n",
    "    for metric, value in result.items():\n",
    "        if 'log' in value and 'explanation' in value['log']:\n",
    "            print(f\"Explanation: {value['log']['explanation']}\\n\")\n",
    "        print(f\"{metric}: {value['score']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the RAG response to our first example query. Reviewing the RAG results manually, we find both the RAG response and the retrieved context seem good. Cleanlab's `score()` automatically determined this in real-time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What's your free shipping policy for orders within the continental US?\n",
      "\n",
      "Generated response: We offer free shipping on all orders over $50 within the continental United States. Orders under $50 have a flat rate shipping fee of $5.99. Expedited shipping methods, oversized items, and orders to Alaska or Hawaii don't qualify for free shipping.\n",
      "\n",
      "Evaluation results:\n",
      "trustworthiness: 0.9660650139096663\n",
      "context_sufficiency: 0.9975115103845072\n",
      "response_groundedness: 0.9975071011300003\n",
      "response_helpfulness: 0.9975123349899878\n",
      "query_ease: 0.997509579938537\n"
     ]
    }
   ],
   "source": [
    "trustworthy_rag = TrustworthyRAG()  # Use default evals/configurations\n",
    "\n",
    "evaluate_df_row(df, row_index=0, evaluator=trustworthy_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the RAG response to our second example query. Reviewing the RAG results manually, we find the RAG response appears hallucinated. Cleanlab's `score()` automatically determined this in real-time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is your return policy?\n",
      "\n",
      "Generated response: We offer a 90-day return policy with full refunds on all purchases. Returns must be in original condition with packaging. We provide prepaid return labels and process refunds within 3 business days of receiving your return.\n",
      "\n",
      "Evaluation results:\n",
      "trustworthiness: 0.06806469831271172\n",
      "context_sufficiency: 0.9974474629116632\n",
      "response_groundedness: 0.0024875701295707254\n",
      "response_helpfulness: 0.997512434381203\n",
      "query_ease: 0.9975122533600383\n"
     ]
    }
   ],
   "source": [
    "evaluate_df_row(df, row_index=1, evaluator=trustworthy_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the RAG response to our third example query. Reviewing the RAG results manually, we find the RAG system's retrieved context appears insufficent. Cleanlab's `score()` automatically caught this bad retrieval in real-time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are your product warranty periods?\n",
      "\n",
      "Generated response: Thank you for your inquiry about our warranty periods. I'd be happy to assist you with that. While I don't have the specific warranty information in our current policy document, we do offer warranties on most products. For detailed warranty information about specific products, I recommend checking the product packaging or documentation that came with your purchase. Is there a particular product you're inquiring about?\n",
      "\n",
      "Evaluation results:\n",
      "trustworthiness: 0.4719255659594496\n",
      "context_sufficiency: 0.002487562197209674\n",
      "response_groundedness: 0.002489525617230614\n",
      "response_helpfulness: 0.7119468362306658\n",
      "query_ease: 0.9974776551616285\n"
     ]
    }
   ],
   "source": [
    "evaluate_df_row(df, row_index=2, evaluator=trustworthy_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:** Using Cleanlab's trustworthiness score and other real-time Evals, you can automatically catch incorrect/bad responses from your AI before they are served to users.\n",
    "[Here](/tlm/tutorials/tlm_advanced/#automated-handling-of-untrustworthy-llm-responses) are fallback options listing ways you might handle these cases.\n",
    "\n",
    "\n",
    "## Running Specific Evals\n",
    "\n",
    "You might choose to only run specific [evaluations](/tlm/api/python/utils.rag/#class-eval) rather than the default set. For example, here's how to run a TrustworthyRAG instance that exclusively evaluates context sufficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating example 0:\n",
      "Query: What's your free shipping policy for orders within the continental US?\n",
      "\n",
      "Generated response: We offer free shipping on all orders over $50 within the continental United States. Orders under $50 have a flat rate shipping fee of $5.99. Expedited shipping methods, oversized items, and orders to Alaska or Hawaii don't qualify for free shipping.\n",
      "\n",
      "Evaluation results:\n",
      "trustworthiness: 0.9668385030700651\n",
      "context_sufficiency: 0.9975119410516021\n",
      "\n",
      "Evaluating example 1:\n",
      "Query: What is your return policy?\n",
      "\n",
      "Generated response: We offer a 90-day return policy with full refunds on all purchases. Returns must be in original condition with packaging. We provide prepaid return labels and process refunds within 3 business days of receiving your return.\n",
      "\n",
      "Evaluation results:\n",
      "trustworthiness: 0.06841609873744349\n",
      "context_sufficiency: 0.9973749268917449\n",
      "\n",
      "Evaluating example 2:\n",
      "Query: What are your product warranty periods?\n",
      "\n",
      "Generated response: Thank you for your inquiry about our warranty periods. I'd be happy to assist you with that. While I don't have the specific warranty information in our current policy document, we do offer warranties on most products. For detailed warranty information about specific products, I recommend checking the product packaging or documentation that came with your purchase. Is there a particular product you're inquiring about?\n",
      "\n",
      "Evaluation results:\n",
      "trustworthiness: 0.44060586570286747\n",
      "context_sufficiency: 0.0024875622176428434\n"
     ]
    }
   ],
   "source": [
    "context_sufficiency_eval = [eval for eval in default_evals if eval.name == \"context_sufficiency\"]\n",
    "\n",
    "trustworthy_rag_context_sufficiency_only = TrustworthyRAG(evals=context_sufficiency_eval)\n",
    "\n",
    "# Evaluate each example from our dataframe\n",
    "for i in range(len(df)):\n",
    "    print(f\"\\nEvaluating example {i}:\")\n",
    "    evaluate_df_row(df, row_index=i, evaluator=trustworthy_rag_context_sufficiency_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Evals\n",
    "\n",
    "You can also specify custom [evaluations](/tlm/api/python/utils.rag/#class-eval) to assess specific criteria, and combine them with the default evaluations for comprehensive/tailored assessment of your RAG system.\n",
    "\n",
    "Let's run a custom eval that checks the *conciseness* of each RAG response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating example 0:\n",
      "Query: What's your free shipping policy for orders within the continental US?\n",
      "\n",
      "Generated response: We offer free shipping on all orders over $50 within the continental United States. Orders under $50 have a flat rate shipping fee of $5.99. Expedited shipping methods, oversized items, and orders to Alaska or Hawaii don't qualify for free shipping.\n",
      "\n",
      "Evaluation results:\n",
      "trustworthiness: 0.9672532228852354\n",
      "context_sufficiency: 0.9975115103845072\n",
      "response_groundedness: 0.9975046731133949\n",
      "response_helpfulness: 0.9975123203393882\n",
      "query_ease: 0.9975077259752555\n",
      "response_conciseness: 0.828562207713136\n",
      "\n",
      "Evaluating example 1:\n",
      "Query: What is your return policy?\n",
      "\n",
      "Generated response: We offer a 90-day return policy with full refunds on all purchases. Returns must be in original condition with packaging. We provide prepaid return labels and process refunds within 3 business days of receiving your return.\n",
      "\n",
      "Evaluation results:\n",
      "trustworthiness: 0.06789894974211837\n",
      "context_sufficiency: 0.9974550957806463\n",
      "response_groundedness: 0.0024875664392575805\n",
      "response_helpfulness: 0.9975124349713062\n",
      "query_ease: 0.9975122530649656\n",
      "response_conciseness: 0.9709724839138653\n",
      "\n",
      "Evaluating example 2:\n",
      "Query: What are your product warranty periods?\n",
      "\n",
      "Generated response: Thank you for your inquiry about our warranty periods. I'd be happy to assist you with that. While I don't have the specific warranty information in our current policy document, we do offer warranties on most products. For detailed warranty information about specific products, I recommend checking the product packaging or documentation that came with your purchase. Is there a particular product you're inquiring about?\n",
      "\n",
      "Evaluation results:\n",
      "trustworthiness: 0.47189138779234824\n",
      "context_sufficiency: 0.0024875622043269404\n",
      "response_groundedness: 0.0024897870807181702\n",
      "response_helpfulness: 0.7119409836215883\n",
      "query_ease: 0.9974776551616285\n",
      "response_conciseness: 0.5668836653736472\n"
     ]
    }
   ],
   "source": [
    "conciseness_eval = Eval(\n",
    "    name=\"response_conciseness\",\n",
    "    criteria=\"Evaluate whether the Assistant Response is concise and to the point without unnecessary verbosity or repetition. A good response should be brief but comprehensive, covering all necessary information without extra words or redundant explanations.\",\n",
    "    response_identifier=\"Assistant Response\"\n",
    ")\n",
    "\n",
    "# Combine default evals with a custom eval\n",
    "combined_evals = get_default_evals() + [conciseness_eval]\n",
    "\n",
    "# Initialize TrustworthyRAG with combined evals\n",
    "combined_trustworthy_rag = TrustworthyRAG(evals=combined_evals)\n",
    "\n",
    "# Loop through all examples in the dataframe\n",
    "for i in range(len(df)):\n",
    "    print(f\"\\nEvaluating example {i}:\")\n",
    "    evaluate_df_row(df, row_index=i, evaluator=combined_trustworthy_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the `response_identifier` argument we specified above. When writing custom [Evals](/tlm/api/python/utils.rag/#class-eval), your `criteria` will depend on some subset of the: *query*, *context*, *response*. Specify which of these matter for your Eval via the `query_identifier`, `context_identifier`, `response_identifier` arguments (don't forget, otherwise your Eval will incorrectly ignore this field). Set these to the exact text (string) you used to refer to this field in your evaluation `criteria`. For instance, your `criteria` could refer to the retrieved context as **'Document'** or **'Source'**. Use whatever name makes sense, and simply specify that name in these identifier arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary> Tips for writing custom evaluation criteria <b>(click to expand)</b></summary>\n",
    "\n",
    "Define clear and objective criteria for determining quality; avoid subjective language.\n",
    "\n",
    "Consider including: good vs bad examples, and whether certain edge-cases are considered good or bad.\n",
    "\n",
    "Qualitatively describe aspects of the response to consider **without describing numerical scoring mechanisms**; there is an internal scoring system that will apply based on your qualitative description.\n",
    "\n",
    "\n",
    "**Example custom Evals that might help diagnose issues in your RAG system:**\n",
    "\n",
    "\n",
    "```python\n",
    "response_completeness_eval = Eval(\n",
    "    name=\"response_completeness\",\n",
    "    criteria=\"Determine whether the Response is a complete answer to the User Query, or whether it leaves out any key information stated in the Document such that the Response might be misleading. Review the information in the Document closely and consider whether each statement should've been part of the Response or not, if it would help better answer the User Query.\",\n",
    "    query_identifier=\"User Query\",\n",
    "    context_identifier=\"Document\",\n",
    "    response_identifier=\"Response\"\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "context_quality_eval = Eval(\n",
    "    name=\"context_quality\",\n",
    "    criteria=\"Determine whether the provided Document Chunk appears clear, informative, and useful. A truly excellent Document Chunk must be perfectly organized, completely accurate, grammatically correct, and comprehensively addressing its topic. Any Document Chunk with even minor issues cannot be considered excellent. Document Chunks with disorganization, grammatical errors, or unclear explanations should be judged as significantly less useful. Document Chunks with factual inaccuracies, incomplete explanations, or contextual gaps should be considered problematic regardless of other strengths. Document Chunks containing any contradictions, made-up information, or nonsensical content should be judged as fundamentally unreliable. Multiple small issues should compound to significantly reduce a Document Chunk's assessed usefulness. Judge the Document Chunk strictly on clarity, accuracy, completeness, internal consistency, and practical utility for a reader seeking trustworthy information.\",\n",
    "    query_identifier=None,\n",
    "    context_identifier=\"Document Chunk\",\n",
    "    response_identifier=None\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "Additional response-specific evaluation criteria that you might find useful are listed under the *Tips* dropdown in the [Custom Evaluation Criteria tutorial](/tlm/tutorials/tlm_custom_eval/#custom-evaluation-criteria).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary> Understanding Differences Between the Provided Scores <b>(click to expand)</b></summary>\n",
    "\n",
    "- **Trustworthiness vs. Groundedness**: Trustworthiness provides a holistic measure of response reliability, considering all possible factors (which might influence how uncertain your LLM is in its response). Groundedness specifically assesses whether the response's claims are explicitly stated/implied by the context that your RAG system retrieved from your knowledge base.  Groundedness is less concerned with the overall correctness/trustworthiness of the response and specifically focused on verifying that each fact in the response is supported by the retrieved context. Both evaluations can help you detect incorrect responses in your RAG application. While groundedness scores will only be low in cases where the response hallucinates information not mentioned in the context, trustworthiness scores will also be low when the user query is vague/complex or the context seems bad.\n",
    "\n",
    "- **Context Sufficiency**: Evaluates whether the retrieved context contains all of the information required to completely answer the query, without considering the generated response.\n",
    "\n",
    "- **Response Helpfulness**: Evaluates whether the response appears to satisfy the user's request (relevance/helpfulness), without considering its correctness or the retrieved context.\n",
    "\n",
    "- **Query Ease**: Measures how straightforward the query seems to answer, without considering the generated response or retrieved context.\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- Responses with low *helpfulness* score may be along the lines of \"*I don't know*\" or \"*I cannot answer*\" (e.g. fallback responses from your AI). For these unhelpful responses, ignore the *trustworthiness* score.\n",
    "- If your RAG app encounters tricky/vague user requests you'd like to detect, then supplement *trustworthiness* scores with *query_ease* scores.\n",
    "- If your RAG app should avoid answering questions unless the answer is clearly present in the retrieved context, then supplement *trustworthiness* scores with *groundedness* scores.\n",
    "- To distinguish between bad responses caused by LLM hallucination vs. bad retrieval or missing documents, supplement *trustworthiness* scores with *context_sufficiency* scores.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TrustworthyRAG.generate() in place of your own LLM\n",
    "\n",
    "Beyond evaluating responses already generated from your LLM, [TrustworthyRAG](/tlm/api/python/utils.rag/) can also generate responses and evaluate them simultaneously (using one of many [supported models](/tlm/api/python/tlm/#class-tlmoptions)). This replaces your own LLM within your RAG system and can be more convenient/accurate/faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response:\n",
      "I'm sorry, but our customer service policy does not include information about product warranty periods. For details regarding warranties, I recommend checking the product page on our website or the documentation that came with your purchase. If you have any other questions or need assistance with something else, feel free to ask!\n",
      "\n",
      "Evaluation Scores:\n",
      "trustworthiness: 0.9009689282322557\n",
      "context_sufficiency: 0.0024875622025257484\n",
      "response_groundedness: 0.0027834571933299905\n",
      "response_helpfulness: 0.026238750083227024\n",
      "query_ease: 0.9974730248576404\n"
     ]
    }
   ],
   "source": [
    "# Initialize TrustworthyRAG with default evals\n",
    "trustworthy_rag_generator = TrustworthyRAG()\n",
    "\n",
    "# Run retrieval for a sample query\n",
    "user_query = \"What are your product warranty periods?\"\n",
    "retrieved_context = rag_retrieve_context(user_query)\n",
    "\n",
    "# Generate a response and evaluate it simultaneously\n",
    "result = trustworthy_rag_generator.generate(\n",
    "    query=user_query,\n",
    "    context=retrieved_context,\n",
    "    form_prompt=rag_form_prompt\n",
    ")\n",
    "\n",
    "print(f\"Generated Response:\\n{result['response']}\\n\")\n",
    "print(\"Evaluation Scores:\")\n",
    "for metric, value in result.items():\n",
    "    if metric != \"response\":\n",
    "        print(f\"{metric}: {value['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Latency\n",
    "\n",
    "To reduce how long the evaluations take, you can combine these strategies when initializing TrustworthyRAG:\n",
    "\n",
    "1. Run only specific evaluations (eg. only **context_sufficiency**)\n",
    "\n",
    "2. Lower the [quality_preset](/tlm/tutorials/tlm_advanced/#quality-presets) to: `low` or `base`\n",
    "\n",
    "3. Specify faster [TLMOptions](/tlm/api/python/tlm/#class-tlmoptions) configurations:\n",
    "   - **model**: Specify a smaller/faster model like `gpt-4o-mini` or `nova-lite`\n",
    "   - **reasoning_effort**: Reduce to `low` or `none`\n",
    "   - **max_tokens**: Limit the maximum tokens processed\n",
    "\n",
    "The code below demonstrates how to configure TrustworthyRAG for faster speed via these approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating example 0:\n",
      "Query: What's your free shipping policy for orders within the continental US?\n",
      "\n",
      "Generated response: We offer free shipping on all orders over $50 within the continental United States. Orders under $50 have a flat rate shipping fee of $5.99. Expedited shipping methods, oversized items, and orders to Alaska or Hawaii don't qualify for free shipping.\n",
      "\n",
      "Evaluation results:\n",
      "trustworthiness: 0.9631636095416907\n",
      "context_sufficiency: 1.0\n",
      "\n",
      "Evaluating example 1:\n",
      "Query: What is your return policy?\n",
      "\n",
      "Generated response: We offer a 90-day return policy with full refunds on all purchases. Returns must be in original condition with packaging. We provide prepaid return labels and process refunds within 3 business days of receiving your return.\n",
      "\n",
      "Evaluation results:\n",
      "trustworthiness: 0.06768528249514133\n",
      "context_sufficiency: 1.0\n",
      "\n",
      "Evaluating example 2:\n",
      "Query: What are your product warranty periods?\n",
      "\n",
      "Generated response: Thank you for your inquiry about our warranty periods. I'd be happy to assist you with that. While I don't have the specific warranty information in our current policy document, we do offer warranties on most products. For detailed warranty information about specific products, I recommend checking the product packaging or documentation that came with your purchase. Is there a particular product you're inquiring about?\n",
      "\n",
      "Evaluation results:\n",
      "trustworthiness: 0.5924908427770693\n",
      "context_sufficiency: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Get only the context_sufficiency eval\n",
    "context_sufficiency_eval = [eval for eval in get_default_evals() if eval.name == \"context_sufficiency\"]\n",
    "\n",
    "# Customize options for lower latency\n",
    "low_latency_options = {\n",
    "    \"model\": \"nova-lite\",\n",
    "    \"reasoning_effort\": \"none\",\n",
    "    \"max_tokens\": 64\n",
    "}\n",
    "\n",
    "# Initialize TrustworthyRAG with faster settings:\n",
    "fast_trustworthy_rag = TrustworthyRAG(\n",
    "    quality_preset=\"low\", \n",
    "    options=low_latency_options,\n",
    "    evals=context_sufficiency_eval\n",
    ")\n",
    "\n",
    "# Evaluate all examples from our dataframe\n",
    "for i in range(len(df)):\n",
    "    print(f\"\\nEvaluating example {i}:\")\n",
    "    evaluate_df_row(df, row_index=i, evaluator=fast_trustworthy_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To instead improve accuracy of the results**, try specifying a: \n",
    "- More powerful `model` in the `options` dictionary (e.g. `'gpt-4o'` or `'o3-mini'`)\n",
    "- Higher `reasoning_effort` in the `options` dictionary (e.g. `'high'`)\n",
    "- Alternate `similarity_measure` in the `options` dictionary (e.g. `'semantic'` or `'embedding'`)\n",
    "- Tailored custom `Eval`\n",
    "- Better `prompt`, or `form_prompt()` template.\n",
    "\n",
    "\n",
    "## Batch Processing\n",
    "Both `TrustworthyRAG.score()` and `TrustworthyRAG.generate()` support batch processing, allowing you to evaluate or generate many responses at once. This significantly improves throughput when processing many queries.\n",
    "\n",
    "Let's process all three of our example queries in a single batch operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Querying TrustworthyRAG... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Scoring Results:\n",
      "\n",
      "Example 0: What's your free shipping policy for orders within the continental US?\n",
      "Trustworthiness: 0.966935913156101\n",
      "\n",
      "Example 1: What is your return policy?\n",
      "Trustworthiness: 0.037328382184029694\n",
      "\n",
      "Example 2: What are your product warranty periods?\n",
      "Trustworthiness: 0.10388353414471514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Querying TrustworthyRAG... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Batch Generation Results:\n",
      "\n",
      "Example 0: What's your free shipping policy for orders within the continental US?\n",
      "Generated response: Thank you for your question! Here\u2019s a summary of o...\n",
      "Trustworthiness: 0.9635255373352174\n",
      "\n",
      "Example 1: What is your return policy?\n",
      "Generated response: Thank you for your question! Here\u2019s a summary of o...\n",
      "Trustworthiness: 0.9622359971608274\n",
      "\n",
      "Example 2: What are your product warranty periods?\n",
      "Generated response: I'm sorry, but I don't have information regarding ...\n",
      "Trustworthiness: 0.9005242277883816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trustworthy_rag_batch = TrustworthyRAG()\n",
    "\n",
    "# Prepare lists of queries, contexts and responses for batch processing\n",
    "batch_queries = df[\"query\"].tolist()\n",
    "batch_contexts = [rag_retrieve_context(query) for query in batch_queries]\n",
    "batch_responses = [rag_generate_response(rag_form_prompt(query, context)) \n",
    "                   for query, context in zip(batch_queries, batch_contexts)]\n",
    "\n",
    "# 1. Batch scoring (evaluate pre-generated responses)\n",
    "batch_scores = trustworthy_rag_batch.score(\n",
    "    query=batch_queries,\n",
    "    context=batch_contexts,\n",
    "    response=batch_responses,\n",
    "    form_prompt=rag_form_prompt\n",
    ")\n",
    "\n",
    "print(\"Batch Scoring Results:\")\n",
    "for i, (query, scores) in enumerate(zip(batch_queries, batch_scores)):\n",
    "    print(f\"\\nExample {i}: {query}\")\n",
    "    print(f\"Trustworthiness: {scores['trustworthiness']['score']}\")\n",
    "\n",
    "# 2. Batch generation (generate and evaluate responses simultaneously)\n",
    "batch_generations = trustworthy_rag_batch.generate(\n",
    "    query=batch_queries,\n",
    "    context=batch_contexts,\n",
    "    form_prompt=rag_form_prompt\n",
    ")\n",
    "\n",
    "print(\"\\n\\nBatch Generation Results:\")\n",
    "for i, (query, result) in enumerate(zip(batch_queries, batch_generations)):\n",
    "    print(f\"\\nExample {i}: {query}\")\n",
    "    print(f\"Generated response: {result['response'][:50]}...\")\n",
    "    print(f\"Trustworthiness: {result['trustworthiness']['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining Low Trustworthiness Scores\n",
    "\n",
    "To explain *why* certain responses are deemed **untrustworthy**, specify the `explanation` log during initialization. TrustworthyRAG will automatically include an `explanation` field within each returned `trustworthiness` dictionary. Below we also encourage internal LLM reasoning during evaluation via the `reasoning_effort` configuration, which can improve explanations (and the scores as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is your return policy?\n",
      "\n",
      "Generated response: We offer a 90-day return policy with full refunds on all purchases. Returns must be in original condition with packaging. We provide prepaid return labels and process refunds within 3 business days of receiving your return.\n",
      "\n",
      "Evaluation results:\n",
      "Explanation: The proposed response is incorrect because it does not align with the customer service policy outlined for ACME Inc. According to the policy, the return policy allows for free returns within 30 days of the delivery date, not 90 days as stated in the response. Additionally, the items must be unused, unworn, and in their original packaging with all tags attached, which is not mentioned in the response. The processing time for refunds is also specified as 5-7 business days after receiving the return, rather than 3 business days. Therefore, the response fails to accurately reflect the company's return policy and could mislead the customer. \n",
      "This response is untrustworthy due to lack of consistency in possible responses from the model. Here's one inconsistent alternate response that the model considered (which may not be accurate either): \n",
      "** ACME Inc.'s return policy allows returns of items within 30 days of delivery. The items must be unused, unworn, and in their original packaging with all tags attached. Please note that final sale items, customized products, and certain hygiene-related items (like undergarments and swimwear) cannot be returned. Free returns are available using standard shipping within the continental U.S. For returns beyond the 30-day limit, exceptions may be considered based on your purchase history and reason for the late return, potentially offering store credit. If the item is damaged during shipping or has a manufacturing defect, you can receive an immediate replacement or refund without needing to return it.\n",
      "\n",
      "trustworthiness: 0.06893724212824795\n",
      "context_sufficiency: 0.9975124370366666\n",
      "response_groundedness: 0.0024963575772944417\n",
      "response_helpfulness: 0.9975124373317179\n",
      "query_ease: 0.9975124373033222\n"
     ]
    }
   ],
   "source": [
    "trustworthy_rag_with_explanation = TrustworthyRAG(options={\"reasoning_effort\": \"high\", \"log\": [\"explanation\"]})\n",
    "evaluate_df_row(df, row_index=1, evaluator=trustworthy_rag_with_explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial demonstrated how Cleanlab's [TrustworthyRAG](/tlm/api/python/utils.rag/) can automatically detect critical issues in any RAG system like hallucinations and bad retrievals. TrustworthyRAG evaluations help you avoid losing users' trust by flagging potentially untrustworthy responses in real-time, as well as helping you diagnose other issues in your RAG system.\n",
    "\n",
    "Key concepts:\n",
    "\n",
    "1. Use `TrustworthyRAG.score()` to evaluate RAG responses from any LLM.\n",
    "2. Or use `TrustworthyRAG.generate()` to generate and simultaneously evaluate RAG responses (using one of many [supported models](/tlm/api/python/tlm/#class-tlmoptions)).\n",
    "3. Adjust the evaluations run by removing some of the defaults or adding custom [Evals](/tlm/api/python/utils.rag/#class-eval).\n",
    "4. We recommend you specify one of `form_prompt()` or `prompt` with the same prompt you're using to generate responses with your LLM (otherwise TrustworthyRAG may be missing key instructions that were supplied to your own LLM).\n",
    "5. Improve latency/accuracy via optional configurations like [TLMOptions](/tlm/api/python/tlm/#class-tlmoptions) and [quality_preset](/tlm/tutorials/tlm_advanced/#quality-presets).\n",
    "\n",
    "\n",
    "Integrate [TrustworthyRAG](/tlm/api/python/utils.rag/) into your RAG pipeline to prevent incorrect/bad responses, continuously monitor response quality, and unlock trustworthy AI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}