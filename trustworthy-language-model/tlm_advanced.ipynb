{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bb388d8-70ed-4aae-a0dc-0baeee0d9b0d",
   "metadata": {},
   "source": [
    "# Trustworthy Language Model (TLM) - Advanced Usage\n",
    "\n",
    "<head>\n",
    "  <meta name=\"title\" content=\"Trustworthy Language Model (TLM) - Advanced Usage\"/>\n",
    "  <meta property=\"og:title\" content=\"Trustworthy Language Model (TLM) - Advanced Usage\"/>\n",
    "  <meta name=\"twitter:title\" content=\"Trustworthy Language Model (TLM) - Advanced Usage\" />\n",
    "  <meta name=\"image\" content=\"/img/tlm-chat.png\" />\n",
    "  <meta property=\"og:image\" content=\"/img/tlm-chat.png\" />\n",
    "  <meta name=\"description\" content=\"A more reliable LLM that quantifies trustworthiness for every output and can detect bad responses.\"  />\n",
    "  <meta property=\"og:description\" content=\"A more reliable LLM that quantifies trustworthiness for every output and can detect bad responses.\" />\n",
    "  <meta name=\"twitter:description\" content=\"A more reliable LLM that quantifies trustworthiness for every output and can detect bad responses.\" />\n",
    "</head>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3623d9e-491c-4b2b-9dbf-b2e0ef2bbc06",
   "metadata": {},
   "source": [
    ":::info\n",
    "\n",
    "This feature requires a [Cleanlab account](https://app.cleanlab.ai/). Additional instructions on creating your account can be found in the [Python API Guide](/guide/quickstart/api/).\n",
    "\n",
    "Free-tier accounts come with usage limits. To increase your limits, email: [sales@cleanlab.ai](mailto:sales@cleanlab.ai).\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7304fa0-ca0b-4861-839b-8c05ccdc7e19",
   "metadata": {},
   "source": [
    "For an introduction to Cleanlab's Trustworthy Language Model, first check out the [TLM quickstart tutorial](/tutorials/tlm/).\n",
    "This tutorial demonstrates advanced TLM capabilities, including:\n",
    "\n",
    "- Generating explanations of low trustworthiness scores\n",
    "- Using quality presets to control latency/cost vs. response accuracy and trustworthiness score reliability\n",
    "- Running TLM over large datasets\n",
    "- Reducing latency/cost without sacrificing response-quality via a `TLMLite` option that allows different models for producing the response vs. scoring its trustworthiness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9038ef9-743c-48b6-b928-31438aac2c2e",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Using TLM requires a [Cleanlab](https://app.cleanlab.ai/) account. Sign up for one [here](https://cleanlab.ai/signup/) if you haven't yet. If you've already signed up, check your email for a personal login link.\n",
    "\n",
    "Cleanlab's Python client can be installed using pip and a `Studio` object can be instantiated with your API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19a5e57-c7ab-4699-b322-20d68ccecc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade cleanlab-studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "363e7bbe-8a8a-4646-882c-7c87c738172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab_studio import Studio\n",
    "\n",
    "# Get API key from here: https://app.cleanlab.ai/account after creating an account.\n",
    "studio = Studio(\"<API key>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914425e7-336e-4f1d-99fb-5796e2b0530c",
   "metadata": {},
   "source": [
    "## Explaining Low Trustworthiness Scores\n",
    "\n",
    "To understand why the TLM estimated low trustworthiness for each particular prompt/response, specify the `explanation` flag when initializing the TLM. With this flag specified, the `output` dictionary TLM returns for each input will contain an extra field called `explanation`. \n",
    "\n",
    "Explanations will be generated for both `prompt()` and `get_trustworthiness_score()` methods. Reasons why a particular LLM response is untrustworthy include:\n",
    "\n",
    "- an alternative contradictory response was almost instead generated by the LLM\n",
    "- reasoning/factual errors were discovered during self-reflection by the LLM\n",
    "- the given prompt/response is atypical relative to the LLM's training data.\n",
    "\n",
    "Here are examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38801a7c-92b5-4187-9ca1-38aa824e0ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Bobby has 3 sisters, and since Bobby is one of the brothers, he is the only brother that all of his sisters share. Therefore, there is 1 brother (Bobby) in total.\n",
      "Trustworthiness Score: 0.5439078281129455\n",
      "\n",
      "Explanation: This response is untrustworthy due to lack of consistency in possible responses from the model. Here's one inconsistent alternate response that the model considered (which may not be accurate either): \n",
      "2 brothers.\n"
     ]
    }
   ],
   "source": [
    "tlm = studio.TLM(options={\"log\": [\"explanation\"]})\n",
    "\n",
    "output = tlm.prompt(\"Bobby (a boy) has 3 sisters. Each sister has 2 brothers. How many brothers?\")\n",
    "\n",
    "print(f'Response: {output[\"response\"]}')\n",
    "print(f'Trustworthiness Score: {output[\"trustworthiness_score\"]}\\n')\n",
    "print(f'Explanation: {output[\"log\"][\"explanation\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d14e357-ae67-485a-b67d-9ed8e376e296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trustworthiness Score: 0.04714852352701132\n",
      "\n",
      "Explanation: The question \"Do LLMs dream of electric sheep?\" is a playful reference to Philip K. Dick's novel \"Do Androids Dream of Electric Sheep?\" which explores themes of artificial intelligence and consciousness. The proposed answer suggests that LLMs (Large Language Models) do dream, which is anthropomorphizing them, as they do not possess consciousness or the ability to dream in the human sense. LLMs process and generate text based on patterns in data but do not have thoughts, feelings, or dreams. Therefore, the answer is not factually correct, as it implies a level of sentience that LLMs do not have. The humor in the answer does not change the fact that it misrepresents the capabilities of LLMs. Thus,  incorrect. \n",
      "This response is untrustworthy due to lack of consistency in possible responses from the model. Here's one inconsistent alternate response that the model considered (which may not be accurate either): \n",
      "No, LLMs do not dream of electric sheep, as they lack consciousness and the capacity for dreams.\n"
     ]
    }
   ],
   "source": [
    "output = tlm.get_trustworthiness_score(prompt=\"Do LLMs dream of electric sheep?\", response=\"Yes, but they prefer to dream of real sheep.\")\n",
    "print(f'Trustworthiness Score: {output[\"trustworthiness_score\"]}\\n')\n",
    "print(f'Explanation: {output[\"log\"][\"explanation\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f73f33c-1d79-4c51-9d7a-38d6e569d97d",
   "metadata": {},
   "source": [
    "## Quality Presets\n",
    "\n",
    "You can trade-off compute vs. quality via the `quality_presets` argument. Higher quality presets produce better LLM responses and trustworthiness scores, but require more computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b1c65a-b0ad-4f96-a535-f6dc79e4d69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tlm = studio.TLM(\n",
    "    quality_preset=\"best\"  # supported quality presets are: 'best','high','medium','low','base'\n",
    ")\n",
    "\n",
    "# Run a single prompt using the preset parameters:\n",
    "output = tlm.prompt(\"<your prompt>\")\n",
    "\n",
    "# Or run multiple prompts simultaneously in a batch:\n",
    "outputs = tlm.prompt([\"<your first prompt>\", \"<your second prompt>\", \"<your third prompt>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1ea4a3-e992-43b1-8f67-d4f7bbd5b41c",
   "metadata": {},
   "source": [
    "#### Details about the TLM quality presets: \n",
    "\n",
    "Quality Preset | LLM Response Quality | Trustworthiness Score Quality\n",
    " ---|-----|-----\n",
    "Best | Best | Good\n",
    "High | Improved | Good\n",
    "Medium | Standard | Good\n",
    "Low | Standard | Fair\n",
    "Base | Standard | Lowest latency\n",
    "\n",
    "Avoid using `best` or `high` presets if you primarily want to get trustworthiness scores, and are less concerned with improving LLM responses. These presets have higher runtime/cost and are designed to return more accurate LLM outputs, but not more reliable trustworthiness scores than the `medium` preset. More precisely: TLM with `medium`, `low`, or `base` preset returns the same response from the base LLM model that you'd ordinarily get, whereas TLM with `best` or `high` preset calls the base LLM multiple times and returns the response with highest trustworthiness score (hence the TLM response itself can be better under these more expensive presets). So when using `TLM.get_trustworthiness_score()` rather than `TLM.prompt()`: stick with the `medium` or `low` quality preset.\n",
    "\n",
    "Rigorous [benchmarks](https://cleanlab.ai/blog/trustworthy-language-model/) reveal that running TLM with the `best` preset can reduce the error rate (incorrect answers): of GPT-4o by 27%, of GPT-4 by 10%, and of GPT-3.5 by 22%.\n",
    "If you encounter token limit errors, try a lower quality preset.\n",
    "\n",
    "**Note:** The range of the returned trustworthiness scores may slightly differ depending on the preset you select. We recommend not directly comparing the magnitude of TLM scores across different presets (settle on one preset before you fix any thresholds). What remains comparable across different presets is how these TLM scores _rank_ data or LLM responses from most to least confidently good.\n",
    "\n",
    "### Other useful options\n",
    "\n",
    "When constructing a TLM instance, you can optionally specify the `options` argument as a dictionary of advanced configurations beyond the quality preset. These configuration options are enumerated in the `TLMOptions` section of our [documentation](/reference/python/trustworthy_language_model/). Here we list a few useful options:\n",
    "\n",
    "- **model**: Which underlying LLM (neural network model) your TLM should rely on. TLM is a wrapper method that can be combined with *any* LLM API to get trustworthiness scores for that LLM and improve its responses (more details further below).\n",
    "\n",
    "- **max_tokens**: The maximum number of tokens TLM should generate. Decrease this value if you hit token limit errors or to improve TLM runtimes.\n",
    "\n",
    "For instance, here's how to run a more accurate LLM than [GPT-4](https://openai.com/index/gpt-4-research/) and also get trustworthiness scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d950e9-085c-4280-8409-ea4a73162ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tlm = studio.TLM(quality_preset=\"best\", options={\"model\": \"gpt-4\"})\n",
    "\n",
    "output = tlm.prompt(\"<your prompt>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aadf509-d1ba-4dde-a162-88237ada5128",
   "metadata": {},
   "source": [
    "## Running TLM over large datasets\n",
    "\n",
    "To avoid overwhelming our API with requests, there's a maximum number of tokens per minute that you can query the TLM with (*rate limit*). If running multiple prompts simultaneously in batch, you'll need to stay under the rate limit, but you'll also want to optimize for getting all results quickly.\n",
    "\n",
    "If you hit token limit errors, consider playing with TLM's `quality_preset` and `max_tokens` parameters. If you run TLM on individual examples yourself in a for loop, you may hit the rate limit, so we recommend running in batches of many prompts passed in as a list. \n",
    "\n",
    "If you are running TLM on big datasets beyond hundreds of examples, it is important to note that `TLM.prompt()` and `TLM.get_trustworthiness_score()` will fail if *any* of the individual examples within the provided list fail. This may be suboptimal. Instead consider using `TLM.try_prompt()` and `TLM.try_get_trustworthiness_score()` which are analogous methods, except these methods handle failed examples by returning `None` in place of the failures and still return results for the remaining examples in the provided list where TLM ran successfully. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c3b7ad-52b2-49a1-9aaf-e591fe3345fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tlm = studio.TLM()\n",
    "\n",
    "big_dataset_of_prompts = [\"<first prompt>\", \"<second prompt>\", \"<third prompt>\"]  # imagine 1000s instead of 3\n",
    "\n",
    "# Not recommended for dataset with 50+ prompts:\n",
    "outputs_that_may_be_lost = tlm.prompt(big_dataset_of_prompts)\n",
    "\n",
    "# Recommended for moderate-size dataset:\n",
    "outputs_where_some_may_be_none = tlm.try_prompt(big_dataset_of_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b93871-5e1c-4513-b950-354d459427a9",
   "metadata": {},
   "source": [
    "### Mini-batching\n",
    "\n",
    "If your datasets have over several thousand examples, we recommend running TLM in mini-batches to checkpoint intermediate results.\n",
    "\n",
    "This helper function allows you to run TLM in mini-batches. We recommend batch sizes of approximately 1000, but feel free to tinker with this number to best suit your use case. You can re-execute this function in the case of any failures and it will resume from the previous checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": 0,
   "metadata": {},
   "source": [
    "**Optional: TLM batch prompt helper function**\n",
    "\n",
    "",
    "Note that we also use the `tlm.try_prompt()` function here, which will handling any failures (errors or timeouts) by returning `None` in place of the failures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b36674b-477b-4ca1-bcf4-9a676734e924",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "def batch_prompt(tlm: studio.TLM, input_path: str, output_path: str, prompt_col_name: str, batch_size: int = 1000):\n",
    "    if os.path.exists(output_path):\n",
    "        start_idx = len(pd.read_csv(output_path))\n",
    "    else:\n",
    "        start_idx = 0\n",
    "\n",
    "    df_batched = pd.read_csv(input_path, chunksize=batch_size)\n",
    "    curr_idx = 0\n",
    "\n",
    "    for curr_batch in df_batched:\n",
    "        # if results already exist for the entire batch\n",
    "        if curr_idx + len(curr_batch) <= start_idx:\n",
    "            curr_idx += len(curr_batch)\n",
    "            continue\n",
    "\n",
    "        # if results exist for half the batch\n",
    "        elif curr_idx < start_idx:\n",
    "            curr_batch = curr_batch[start_idx - curr_idx:]\n",
    "            curr_idx = start_idx\n",
    "\n",
    "        results = tlm.try_prompt(curr_batch[prompt_col_name].to_list())\n",
    "        results = [\n",
    "            r if r else {\"response\": None, \"trustworthiness_score\": None}\n",
    "            for r in results\n",
    "        ]\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_csv(output_path, mode='a', index=False, header=not os.path.exists(output_path))\n",
    "        \n",
    "        curr_idx += len(curr_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a59af65-4ccc-41c1-9d2e-4b26818166e1",
   "metadata": {},
   "source": [
    "Here we'll demonstrate using the `batch_prompt()` method on a toy dataset of 100 prompts, but this can be run at scale. Just specify: an instantiated TLM object, the input file path to a CSV file containing your prompts and the column name in which they are located, as well as the output file\u00a0path where results should be stored, and your intended batch size (we recommend ~1000 examples per batch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30cba007-157f-4b82-961a-c00c3486cf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create sample prompts\n",
    "sample_prompts = pd.DataFrame({\"prompt\": [f\"What is the sum of 1 and {i}?\" for i in range(1, 101)]})\n",
    "sample_prompts.to_csv(\"sample_tlm_prompts.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fcc5df2-ac2e-4596-9a75-12e7eac7cf91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the sum of 1 and 1?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the sum of 1 and 2?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the sum of 1 and 3?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the sum of 1 and 4?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the sum of 1 and 5?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        prompt\n",
       "0  What is the sum of 1 and 1?\n",
       "1  What is the sum of 1 and 2?\n",
       "2  What is the sum of 1 and 3?\n",
       "3  What is the sum of 1 and 4?\n",
       "4  What is the sum of 1 and 5?"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_path = \"sample_tlm_prompts.csv\"\n",
    "output_path = \"sample_responses.csv\"\n",
    "\n",
    "df = pd.read_csv(input_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e646a7-9f09-45b5-bffe-02179b2a5a9a",
   "metadata": {},
   "source": [
    "We can then call the `batch_prompt` function to run TLM in mini-batches. Note that if this cell fails for any reason, you can just re-execute it and the TLM will resume processing your data from the previous checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1ae1b47-f93f-43c6-9ad2-b0d4ae2a36b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tlm = studio.TLM() \n",
    "\n",
    "batch_prompt(\n",
    "    tlm=tlm,\n",
    "    input_path=input_path, \n",
    "    output_path=output_path, \n",
    "    prompt_col_name=\"prompt\", \n",
    "    batch_size=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e39938-c7ee-4e12-be23-3dea66df14a9",
   "metadata": {},
   "source": [
    "After the cell above is done executing, we can view the saved results in the output file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ddb95fb-b2de-4516-9e16-3f1c9a69409a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>trustworthiness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sum of 1 and 1 is 2.</td>\n",
       "      <td>0.953392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The sum of 1 and 2 is 3.</td>\n",
       "      <td>0.983357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The sum of 1 and 3 is 4.</td>\n",
       "      <td>0.978256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The sum of 1 and 4 is 5.</td>\n",
       "      <td>0.980026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The sum of 1 and 5 is 6.</td>\n",
       "      <td>0.968054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   response  trustworthiness_score\n",
       "0  The sum of 1 and 1 is 2.               0.953392\n",
       "1  The sum of 1 and 2 is 3.               0.983357\n",
       "2  The sum of 1 and 3 is 4.               0.978256\n",
       "3  The sum of 1 and 4 is 5.               0.980026\n",
       "4  The sum of 1 and 5 is 6.               0.968054"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(output_path)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b33f59-371e-40b4-a753-49e06780990a",
   "metadata": {},
   "source": [
    "## Trustworthy Language Model Lite\n",
    "\n",
    "Using a `TLMLite` object in place of a `TLM` enables the use of different LLMs for generating the response vs scoring its trustworthiness. Consider this hybrid approach to get high-quality responses (from a more expensive model), but cheaper trustworthiness score evaluations (via a smaller model).\n",
    "\n",
    "`TLMLite` can be used similarly to `TLM`. The main difference is we can specify a `response_model` when initializing the `TLMLite` object, to specify which model generates responses for given prompts. Other settings specified in the `options` argument apply to the trustworthiness scoring model in `TLMLite`.\n",
    "\n",
    "For example, here we use the larger `gpt-4o` model to generate reponses to our prompts, and the smaller `gpt-4o-mini` model for trustworthiness score evaluations. To further reduce costs, we can also specify `quality_preset=\"low\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fc509d-8d2c-46d8-b934-58737cff8b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "tlm_lite = studio.TLMLite(response_model=\"gpt-4o\", quality_preset=\"low\", options={\"model\": \"gpt-4o-mini\"})\n",
    "\n",
    "output = tlm_lite.prompt(\"<your prompt>\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}